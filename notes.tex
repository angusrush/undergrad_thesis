%        File: notes.tex 
%     Created: Thu May 25 01:00 PM 2017 B 
% Last Change: Thu May 25 01:00 PM 2017 B
%

\documentclass[a4paper,10pt]{scrreprt}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage{fancyhdr} 
\usepackage{amstext}
\usepackage{array} 
\usepackage{amsthm} 
\usepackage{tikz} 
\usepackage{bm}
\usepackage{mathrsfs} 
\usepackage{mathrsfs} 
\usepackage{eucal}
\usepackage{braket} 
\usepackage{pgfplots} 
\usepackage{soul}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{braids}
\usepackage{epigraph}
\usepackage[compat=1.1.0]{tikz-feynman}
\usepackage{xcolor}
\usepackage{appendix}

% Set up hyperref colors
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Good practice to set the version so updates don't break old documents
\pgfplotsset{compat=1.14}

% Load tikz-cd
\usetikzlibrary{cd}

% Declares the font calligra
\usepackage{calligra} 
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n} 
\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[2.2]callig15}{} 

% Makes '\sr' a script r 
\newcommand{\sr}{\ensuremath{\mathcalligra{r}}}

% Makes indentation more note-ish
\setlength{\parindent}{0em} 
\setlength{\parskip}{.5em}
\setlength{\headheight}{14.0pt}

% My commands
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\tder}[2]{\frac{d #1}{d #2}}
\newcommand{\R}{\mathbb{R}} 
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}} 
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}} 
\newcommand{\dd}{\mathrm{d}} 
\newcommand{\Mod}[1]{\(\mathrm{mod}#1\)}
\newcommand{\defn}[1]{\ul{#1}}
\newcommand{\cliff}{\mathrm{C}\ell}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\tAd}{\widetilde{\mathrm{Ad}}}
\newcommand{\Pin}{\mathrm{Pin}}
\newcommand{\Spin}{\mathrm{Spin}}
\newcommand{\Or}{\mathrm{O}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\PP}{\mathrm{P}}
\newcommand{\tP}{\tilde{\mathrm{P}}}
\newcommand{\SP}{\mathrm{SP}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Obj}{\mathrm{Obj}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Nat}{\mathrm{Nat}}
\newcommand{\Line}{\mathrm{Line}}
\newcommand{\ev}{\mathrm{eval}}
\newcommand{\coker}{\mathrm{coker}}
\newcommand{\im}{\mathrm{im}}
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\cod}{\mathrm{cod}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\spec}{\mathrm{Spec}}
\newcommand{\id}{\mathrm{id}}

% Hyphen for names of categories
\def\mhyp{{\hbox{-}}}

% Second level of list uses empty bullets
\renewcommand{\labelitemii}{$\circ$}

% Declare theorem styles
\theoremstyle{definition} 
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{counterexample}{Counterexample}[section]
\newtheorem{joke}{Joke}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{remark}
\newtheorem{claim}{Claim}[section]
\newtheorem{recipe}{Recipe}[section]
\newtheorem{note}{Note}[section]
\newtheorem{notation}{Notation}[section]

\title{Deligne's Theorem on Tensor Categories}
\subtitle{And its Implications for Quantum Mechanics}
\author{Angus Rush}

\begin{document} 
\maketitle
\tableofcontents 

\chapter{Introduction}
%\epigraph{Algebra is the offer made by the devil to the mathematician. The devil says: I will give you this powerful machine, it will answer any questions you like. All you need to do is give me your soul: give up geometry and you will have this marvelous machine.}{M. Atiyah, 2002}

%These are the notes I took while learning about Deligne's theorem on tensor categories and its applications to physics, in particular supersymmetry. 
%
%Almost none of the material is original, including the order of presentation, which follows \cite{nlab-deligne-theorem} \emph{very} closely.
%
%The fact that it's just a collection of notes means that it assumes as background more or less exactly what I knew before I started (although I have made some effort to keep it self-contained). 

\subsection{History of superalgebra and supergeometry}
\subsubsection{Supergeometry and the path integral}
Supergeometry as a mathematical structure came into being out of necessity, as an attempt to extend the path integral approach to quantum field theory to allow the treatment of fermionic fields.

In 1948, Richard Feynman published his so-called \emph{path-integral} approach to quantum mechanics (\cite{feynmanpathintegral}). It was quickly extended to bosonic quantum field theory. However, it took quite some time for it to be applied to fermionic quantum fields. The first textbook account, \cite{berezinsecondquantization}, was published by Felix Berezin in 1965.

This slow uptake was due to some extent to a conceptual difficulty. In the path integral approach to quantum field theory, one treats a quantum field in terms of classical field states. This means that calculations with quantum fields involve corresponding classical fields. For bosonic fields (fields describing particles with integer spin), it is clear how to do this: bosonic quantum fields have natural classical analogs since in the high particle number limit, the quantum dynamics reduce, roughly, to the classical field equations. However, since fermions (particles with half-integer spin) cannot be in the same state due to the Pauli exclusion principle, fermionic fields do not have a high particle humber limit, so they have no obvious classical analog.

It turns out that the way to treat fermionic quantum fields in the path integral formalism is to allow the classical fields which correspond to fermions to anticommute, i.e. to satisfy the relation 
\begin{equation*}
  \psi(x) \psi(y) = -\psi(y) \psi(x).
\end{equation*}
This is puzzling since we do not know how to interpret the order of classical fields. In classical field theory, field histories are given by sections of a bundle over spacetime, and the value of a field at a point can be thought of as given by a collection of numbers of which represent, for example, the components of a vector located at that point. These numbers by definition commute with each other. The anticommutativity of fermionic classical fields means that they cannot be na\"{i}vely interpreted as classical fields in the standard sense.

The correct implementation of anticommuting classical fields turned out to be the following: if one augments the charts on a manifold by adding additional coordinates which anticommute instead of commute, then fields, considered locally as a sort of formal power series in \emph{all} the variables (commuting and anticommuting) can behave non-trivially under comutation.

The mathematical formulation of such an `augmented manifold' is given by what is known as a supermanifold: one takes the standard description of a $C^{\infty}$ manifold as a locally ringed space and allows the sections to be so-called \emph{supercommutative} rings, which contain both commuting and anticommuting elements.

Supermanifolds have a rich mathematical theory, explored in e.g. \cite{susy-for-mathematicians}. In many ways, it runs parallel to the classical theory of $C^{\infty}$ manifolds. For instance, one can study a category whose objects are supermanifolds; the group objects in this category are known as Lie super-groups. The tangent space at the identity of a Lie super-group is known as a Lie super-algebra, an algebra with a bracket which is sometimes symmetric and sometimes antisymmetric.

\subsubsection{Lie superalgebras and the Haag-{\L}opusza{\'n}ski-Sohnius theorem}

In 1967, Sidney Coleman and Jeffrey Mandula published a paper (\cite{coleman-mandula}) in which they analyzed a quantum field theory which was invariant under a connected Lie group $G$ of symmetries. They showed that, under certain physically reasonable assumptions, if $G$ contained the Poincar\'{e} group as a subgroup, then it had to split into a direct product of the Poincare\'{e} group and some internal symmetry group; there could be no mixing.

They accomplished this by studying a representation of the Lie algebra $\mathfrak{g}$ of $G$, and showing that the operators generating spacetime and internal translations could not obey non-trivial commutation relations. That is, $\mathfrak{g}$ had to be decomposable as a direct sum
\begin{equation*}
  \mathfrak{g} = \mathfrak{g}_{\,\text{Poincar\'{e}}} \oplus \mathfrak{g}_{\,\text{internal}}.
\end{equation*}

This was seen as a blow for fundamental physics. Symmetry was viewed as a unifying force in fundamental pysics, and there was a hope that spacetime and internal symmetries could be unified. The so-called Coleman-Mandula theorem showed that this was not possible. 

However, in 1975 Rudolph Haag, Jan {\L}opusza{\'n}ski, and Martin Sohnius showed that the operators representing Lie algebras of the spacetime and internal symmetry groups could mix in a non-trivial way as long as some of they were allowed to obey anti-commutation relations. Mathematically, this meant that the generators really formed a representation of a Lie super-algebra. Thus, Haag, {\L}opusza{\'n}ski, and Sohnius showed that it was possible to unify spacetime and internal symmetries of a relativistic quantum field theory if one assumed that the symmetries were given by a super-group.

This so-called Haag-{\L}opusza{\'n}ski-Sohnius theorem was, and to some extent is, taken as a justification for the belief that spacetime fundamentally has the structure of a supermanifold.

\subsection{The study of superalgebra has proven to be fruitful }

%\chapter{The Mackey machine}
%The \emph{Mackey machine} is another name for the so-called \emph{method of induced representations} developed by George Mackey in \cite{mackey-induced-representations}. 
%
%\begin{note}
%  This definitely won't make it into the final project.
%\end{note}
%
%\textbf{The cast of characters:}
%\begin{itemize}
%  \item $G$: The heroine. A topological group which is Hausdorff, separable, and locally compact.
%
%  \item $S$: $G$'s right-hand man. $S$ is a a topological space. When $S$ is united with his friends $\mathcal{B}$ and $\mu$, he grows stronger, becoming a \emph{measure space}. In this heightened state he admits a measurable right $G$-action, i.e. a map 
%    \begin{equation*}
%      S \times G \to S;\qquad (s, g) \mapsto sg.
%    \end{equation*}
%    which is measurable for any fixed $g \in G$.
%    
%
%  \item $\mathcal{B}$: The Borel $\sigma$-algebra on $S$.
%
%  \item $\mu$: A measure on $S$ which is \emph{$G$-invariant} in the sense that 
%    \begin{equation*}
%      \mu(Eg) = \mu(E)
%    \end{equation*}
%    for any set $E \subseteq S$.
%
%  \item $\mathscr{H}$: The separable Hilbert space of complex-valued, Borel-measurable, square-integrable functions on $S$. $\mathscr{H}$'s full name is $\mathscr{L}^{2}(S, \mu)$.
%\end{itemize}
%
%Any $g \in G$ defines an operator $U_{g}$ on $\mathscr{H}$ as follows:
%\begin{equation*}
%  U_{g}(f)(s) = f(sg).
%\end{equation*}
%
%\begin{theorem}
%  The operators $U_{g}$ defined in this way have the following properties.
%  \begin{enumerate}
%    \item $U(g)$ is unitary for all $g \in G$
%
%    \item $U(g) U(h) = U(gh)$.
%
%    \item The assignment $g \mapsto U_{g}$ is continuous.
%  \end{enumerate}
%\end{theorem}

\chapter{Superalgebra and supergeometry in categories}
Historically, superalgebraic strucutures were defined haphazardly by imposing the Koszul sign rule by hand.\footnote{For a traditional approach to superalgebra, see \hyperref[ch:superalgebra]{Appendix \ref*{ch:superalgebra}}.} Seemingly miraculously, this worked in the sense that if one added the appropriate sign changes to the definitions, they would turn up in the right places in the theorems. In the first part of this chapter, we will see that the appearance of the Koszul sign rule is much more natural: it is a manifestation of the fact that the supercommutativity is really just a different sort of commutativity, one which shows up naturally in the study of $\Z_{2}$-graded vector spaces. We will do this by defining supercommutative structures simply as commutative structures internal to the category of super vector spaces. 

In the second section, we will apply some of the machinery of algebraic geometry to our new understanding of supercommutative algebra, and arrive at a natural definition of a supervariety.

In the third section, we generalize the ideas of the second section, leading to a definition of an affine supergroup.

In what follows, we will us the language of category theory freely. The necessary definitions and theorems have been relegated to \hyperref[ch:categories]{Appendix \ref*{ch:categories}}.

\section{Supercommutativity}
The goal of this section is to explain the ubiquity of supercommutativity (i.e. the Koszul sign rule) in the study of $\Z_{2}$-graded spaces. Stated roughly, the reason is this: it is the only possible `commutative' multiplication law on a $\Z_{2}$-graded algebra which knows about the grading. That is to say, the only other possible commutative multiplication law (up to an appropriate sort of isomorphism) on a $\Z_{2}$-graded algebra is the trivial one, which is equivalent to treating the vector space as ungraded. 

\subsection{The category $\mathsf{SVect}_{k}$}

\begin{definition}[category of $\Z_{2}$-graded vector spaces]
  \label{def:categoryofz2gradedvectorspaces}
  The \defn{category of $\Z_{2}$-graded vector spaces}, i.e. the category whose objects are $\Z_{2}$-graded vector spaces (\hyperref[def:z2gradedvectorspace]{Definition \ref*{def:z2gradedvectorspace}}) and whose morphisms are $\Z_{2}$-graded vector space morphisms, is notated $\mathsf{Vect}_{k}^{\Z_{2}}$.
\end{definition}

\begin{note}
  We call this category the category of $\Z_{2}$-graded vector spaces rather than the category of super vector spaces because we will reserve the latter name for the category with the appropriate symmetric monoidal structure, i.e. that which yields the Koszul sign rule. We will explore this structure now.
\end{note}

\begin{lemma}
  The $\Z_{2}$-graded tensor product (\hyperref[def:tensorproduttofz2gradedvectorspaces]{Definition \ref*{def:tensorproduttofz2gradedvectorspaces}}) can be extended to a bifunctor 
  \begin{equation*}
    \otimes\colon \mathsf{Vect}_k^{\Z_{2}} \times \mathsf{Vect}_k^{\Z_{2}} \rightsquigarrow \mathsf{Vect}_k^{\Z_{2}}.
  \end{equation*}
\end{lemma}
\begin{proof}
  The behavior of the $\Z_{2}$ tensor product on objects and morphisms is inherited from the ungraded tensor product. It is not hard to check that the tensor product of two $\Z_{2}$-graded linear maps defined in this way is $\Z_{2}$-graded.
\end{proof}

\begin{theorem}
  There are only two inequivalent choices for a symmetric braiding $\gamma$ (\hyperref[def:symmetricmonoidalcategory]{Definition \ref*{def:symmetricmonoidalcategory}}) on $\mathsf{Vect}_{k}^{\Z_{2}}$ (that is to say, up to a categorical equivalence \hyperref[def:categoricalequivalence]{Definition \ref*{def:categoricalequivalence}} whose functors are braided monoidal (\hyperref[def:braidedmonoidalfunctor]{Definition \ref*{def:braidedmonoidalfunctor}})),  with components
  \begin{equation*}
    \gamma_{V,W}\colon V \otimes W \to W \otimes V:
  \end{equation*}

  \begin{enumerate}
    \item The \emph{trivial braiding}, which acts on representing tuples (\hyperref[def:tensorproductofvectorspaces]{Definition \ref*{def:tensorproductofvectorspaces}}) by 
      \begin{equation*}
        \gamma_{V, W}\colon (v, w) \mapsto (w, v)
      \end{equation*}

    \item The \emph{super braiding}, which acts on representing tuples of pure degree by
      \begin{equation*}
        \gamma_{V, W}\colon (v, w) \mapsto (-1)^{\tilde{v}\cdot \tilde{w}}(w, v).
      \end{equation*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  See \cite{nlab-deligne-theorem}, Proposition 3.15.
\end{proof}

\begin{definition}[category of super vector spaces]
  \label{def:categoryofsupervectorspaces}
  The \defn{category of super vector spaces}, denoted $\mathsf{SVect}_{k}$, is the category $\mathsf{Vect}_{k}^{\Z_{2}}$ together with the super braiding $\gamma$.
\end{definition}

\begin{lemma}
  The forgetful functor 
  \begin{equation*}
    \mathcal{U}\colon \mathsf{SVect}_{k} \rightsquigarrow \mathsf{Vect}_{k}
  \end{equation*}
  which simply forgets the grading is strong monoidal (\hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}}).
\end{lemma}
\begin{proof}
  We need to find a natural isomorphism $\Phi$ with components 
  \begin{equation*}
    \Phi_{X, Y}\colon \mathcal{U}(X) \otimes \mathcal{U}(Y) \to \mathcal{U}(X \otimes Y)
  \end{equation*}
  and an isomorphism
  \begin{equation*}
    \varphi\colon 1_{\mathsf{Vect}_{k}} \to \mathcal{U}(\mathsf{SVect}_{k})
  \end{equation*}
  which make the necessary diagrams in \hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}} commute.

  But the tensor product on $\mathsf{SVect}_{k}$ is inherited from that on $\mathsf{Vect}_{k}$, so we can take $\Phi_{X, Y}$ to be the identity transformation for all $X$ and $Y$, and since the field $k$ is the identity object in both categories, we can take $\varphi = 1_{k}$.

  The necessary diagrams in \hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}} commute trivially with these definitions.
\end{proof} 

\begin{lemma}
  \label{lemma:monoidalinclusionofvectorspacesintosupervectorspaces}
  The canonical inclusion $\mathcal{I}\colon \mathsf{Vect}_{k} \hookrightarrow \mathsf{SVect}_{k}$, which sends a $k$-vector space $V$ to the super vector space with grading $V \oplus 0$ extends to a strong braided monoidal functor \hyperref[def:braidedmonoidalfunctor]{Definition \ref*{def:braidedmonoidalfunctor}}.
\end{lemma}
\begin{proof}
  First, we need to show that $\mathcal{U}$ is strong monoidal, i.e. that there is a natural isomorphism with components
  \begin{equation*}
    \Phi_{U,V}\colon (V \otimes W) \oplus 0 \to (V \oplus 0) \otimes (W \oplus 0).
  \end{equation*}
  Finding the correct isomorphism is trivial, as is showing that the appropriate diagram commutes.

  Then we need to find a morphism $\varphi\colon 1 \to 1 \oplus 0$. Again, the obvious choice is the correct one.

  With these choices the necessary diagrams commute trivially.
\end{proof}

\subsection{Commutative monoids}

\begin{definition}[internal monoid]
  \label{def:internalmonoid}
  Let $\mathsf{C}$ be a monoidal category (\hyperref[def:monoidalcategory]{Definition \ref*{def:monoidalcategory}}) with monoidal structure $(\otimes, 1, \alpha, \lambda, \rho)$. A \defn{monoid internal to $\mathsf{C}$} is 
  \begin{enumerate}
    \item an object $A \in \Obj(\mathsf{C})$,
    \item a morphism $e\colon 1 \to A$, called the unit, and
    \item a morphism $\mu\colon A \otimes A \to A$, called the product,
  \end{enumerate}
  such that the following diagrams commute.

  \begin{enumerate}
    \item Associativity
      \begin{equation*}
        \begin{tikzcd}
          (A \otimes A) \otimes A
          \arrow[rr, "\alpha"]
          \arrow[d, swap, "\mu"]
          & & A \otimes (A \otimes A)
          \arrow[d, "\mu"]
          \\
          A \otimes A
          \arrow[dr, swap, "\mu"]
          & & A \otimes A
          \arrow[dl, "\mu"]
          \\
          & A
        \end{tikzcd}
      \end{equation*}

    \item Unitality
      \begin{equation*}
        \begin{tikzcd}
          1 \otimes A
          \arrow[r, "e"]
          \arrow[dr, swap, "\lambda"]
          & A \otimes A
          \arrow[d, "\mu"]
          & A \otimes 1
          \arrow[l, swap, "e"]
          \arrow[dl, "\rho"]
          \\
          & A
        \end{tikzcd}
      \end{equation*}
  \end{enumerate}

  Moreover, if $\mathsf{C}$ is a symmetric monoidal category with symmetric braiding $\gamma$, then the monoid $(A, e, \mu)$ is called \defn{commutative} if the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      A \otimes A
      \arrow[rr, "{\gamma_{A, A}}"]
      \arrow[dr, swap, "\mu"]
      & & A \otimes A
      \arrow[dl, "\mu"]
      \\
      & A
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{example}
  \label{eg:tensorunitisinternalmonoid}
  Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category. The tensor unit $1$ is a monoid internal to $\mathsf{C}$, with product $\lambda_{1}\colon 1 \otimes 1 \to 1$ and unit $\mathrm{id}_{1}\colon 1 \to 1$.

  We could equally use $\rho$ instead of $\lambda$ by \hyperref[lemma:leftandrightunitoragreewhenpossible]{Lemma \ref*{lemma:leftandrightunitoragreewhenpossible}}.
\end{example}

\begin{definition}[morphism of monoids]
  \label{def:homomorphismofmonoids}
  A \defn{homomorphism} of monoids $(A_{1}, \mu_{1}, e_{1}) \to (A_{2}, \mu_{2}, e_{2})$ is a morphism $f\colon A_{1} \to A_{2}$ such that the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}
      A_{1} \otimes A_{1}
      \arrow[r, "f \otimes f"]
      \arrow[d, swap, "\mu_{1}"]
      & A_{2} \otimes A_{2}
      \arrow[d, "\mu_{2}"]
      \\
      A_{1} 
      \arrow[r, "f"]
      & A_{2}
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      1
      \arrow[r, "e_{1}"]
      \arrow[rd, swap, "e_{2}"]
      & A_{1}
      \arrow[d, "f"]
      \\
      & A_{2}
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{definition}[category of monoids]
  \label{def:categoryofmonoids}
  Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category. Denote by $\mathsf{Mon}(\mathsf{C}, \otimes, 1)$ the \defn{category of monoids in $\mathsf{C}$}, i.e. the category whose objects are monoids of $\mathsf{C}$ and whose morphism are homomorphisms.

  Denote by $\mathsf{CMon}(\mathsf{C}, \otimes, 1)$ the full subcategory of $\mathsf{Mon}(\mathsf{C}, \otimes, 1)$ whose objects are commutative monoids.
\end{definition}

\begin{lemma}
  \label{lemma:unitobjectisinitialincmon}
  For any monoidal category $(\mathsf{C}, \otimes, 1)$, the category $\mathsf{Mon}(\mathsf{C})$ has as its initial object the tensor unit $1$.
\end{lemma}
\begin{proof}
  By definition, we have for any module object $A \in \mathsf{Mon}(\mathsf{C})$ a morphism 
  \begin{equation*}
    e\colon 1 \to A.
  \end{equation*}
  This is a module homomorphism by

  and it is therefore unique since any other module morphism $\varphi\colon 1 \to A$ would have to make the following diagram commute, forcing $\varphi = e$.
  \begin{equation*}
    \begin{tikzcd}
      1
      \arrow[r, "\sim"]
      \arrow[dr, swap, "e"]
      & 1
      \arrow[d, "\varphi"]
      \\
      & A
    \end{tikzcd}
  \end{equation*}

  This is exactly the property which defines initial objects.
\end{proof}

\begin{corollary}
  For any symmetric monoidal category $(\mathsf{C}, \otimes, 1)$, the category $\mathsf{CMon}(\mathsf{C})$ has initial object $1$.
\end{corollary}

\begin{lemma}
  \label{lemma:monoidalfunctorspreserveinternalmonoids}
  Let $(\mathsf{C}, \otimes_{\mathsf{C}}, 1_{\mathsf{C}})$ and $(\mathsf{D}, \otimes_{\mathsf{D}}, 1_{\mathsf{D}})$ be monoidal categories, and let $(\mathcal{F}, \Phi, \varphi)$ be a lax monoidal functor $\mathsf{C} \rightsquigarrow \mathsf{D}$ (\hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}}). Then for any monoid $(A, \mu_{A}, e_{A})$ in $\mathsf{C}$, its image $\mathcal{F}(A)$ in $\mathsf{D}$ can be made a monoid by setting
  \begin{equation*}
    \mu_{\mathcal{F}(A)}\colon 
    \begin{tikzcd}
      \mathcal{F}(A) \otimes_{\mathsf{D}} \mathcal{F}(A) 
      \arrow[r, "{\Phi_{A, A}}"]
      & \mathcal{F}(A \otimes_{\mathsf{C}} A) 
      \arrow[r, "\mathcal{F}(\mu_{A})"]
      & \mathcal{F}(A)
    \end{tikzcd},
  \end{equation*}
  and
  \begin{equation*}
    e_{\mathcal{F}(A)}\colon
    \begin{tikzcd}
      1_{\mathscr{D}}
      \arrow[r, "\varphi"]
      & \mathcal{F}(1_{\mathsf{C}})
      \arrow[r, "\mathcal{F}(e_{A})"]
      & \mathcal{F}(A)
    \end{tikzcd},
  \end{equation*}
  where $\varphi$ is again the structure morphism of $\mathcal{F}$. This construction extends to a functor 
  \begin{equation*}
    \mathsf{Mon}(\mathcal{F})\colon \mathsf{Mon}(\mathsf{C}, \otimes_{\mathsf{C}}, 1_{\mathsf{C}}) \rightsquigarrow \mathsf{Mon}(\mathsf{D}, \otimes_{\mathsf{D}}, 1_{\mathsf{D}}).
  \end{equation*}

  Furthermore, if $\mathsf{C}$ and $\mathsf{D}$ are symmetric monoidal categories, $\mathcal{F}$ is a braided monoidal functor, and $A$ is a commutative monoid, then so is $\mathcal{F}(A)$, and this construction extends to a functor
  \begin{equation*}
    \mathsf{CMon}(\mathcal{F})\colon \mathsf{CMon}(\mathsf{C}, \otimes_{\mathsf{C}}, 1_{\mathsf{C}}) \rightsquigarrow \mathsf{CMon}(\mathsf{D}, \otimes_{\mathsf{D}}, 1_{\mathsf{D}}). 
  \end{equation*}
\end{lemma} 
\begin{proof}
  All of these are easy to see but hard to write down. The diagrams end up too big to fit comfortably on one page.
%  We prove that $\mu_{\mathcal{F}(A)}$ makes the associativity diagram commute; everything else is similar.
%
%  We want to show that the diagram 
%  \begin{equation*}
%    \begin{tikzcd}
%      (\mathcal{F}(A) \otimes \mathcal{F}(A)) \otimes \mathcal{F}(A)
%      \arrow[rr, "{\alpha_{\mathcal{F}(A), \mathcal{F}(A), \mathcal{F}(A)}}"]
%      \arrow[d, swap, "\mu_{\mathcal{F}(A)}"]
%      & & \mathcal{F}(A) \otimes (\mathcal{F}(A) \otimes \mathcal{F}(A))
%      \arrow[d, "\mu_{\mathcal{F}(A)}"]
%      \\
%      \mathcal{F}(A) \otimes \mathcal{F}(A)
%      \arrow[dr, swap, "\mu_{\mathcal{F}(A)}"]
%      & & \mathcal{F}(A) \otimes \mathcal{F}(A)
%      \arrow[dl, "\mu_{\mathcal{F}(A)}"]
%      \\
%      & \mathcal{F}(A)
%    \end{tikzcd}
%  \end{equation*}
%  commutes. We can do this, as usual, by adding a bunch of rows and then collapsing some.
%  \begin{equation*}
%    \begin{tikzcd}
%      & (\mathcal{F}(A) \otimes \mathcal{F}(A)) \otimes \mathcal{F}(A)
%      \arrow[rr, "{\alpha_{\mathcal{F}(A), \mathcal{F}(A), \mathcal{F}(A)}}"]
%      \arrow[d, swap, "{\Phi_{A, A} \otimes 1_{\mathcal{F}(A)}}"]
%      & & \mathcal{F}(A) \otimes (\mathcal{F}(A) \otimes \mathcal{F}(A))
%      \arrow[d, "{1_{\mathcal{F}(A)} \otimes \Phi_{A, A}}"]
%      \\
%      & \mathcal{F}(A \otimes A) \otimes \mathcal{F}(A)
%      \arrow[d, swap, "{\Phi_{A \otimes A, A}}"]
%      & & \mathcal{F}(A) \otimes \mathcal{F}(A \otimes A)
%      \arrow[d, "{\Phi_{A, A \otimes A}}"]
%      \\
%      & \mathcal{F}((A \otimes A) \otimes A)
%      \arrow[rr, "{\mathcal{F}(\alpha_{A, A, A})}"]
%      \arrow[d, "\mathcal{F}(\mu \otimes 1_{A})"]
%      & & \mathcal{F}(A \otimes (A \otimes A))
%      \arrow[d, swap, "\mathcal{F}(1_{A} \otimes \mu)"]
%      \\
%      & \mathcal{F}(A \otimes A)
%      \arrow[dr, swap, "\mathcal{F}(\mu_{A})"]
%      \arrow[dl, swap, bend right, "{\Phi_{A, A}^{-1}}"]
%      & & \mathcal{F}(A \otimes A)
%      \arrow[dl, "\mathcal{F}(\mu_{A})"]
%      \arrow[dr, bend left, "{\Phi_{A, A}^{-1}}"]
%      \\
%      \mathcal{F}(A) \otimes \mathcal{F}(A)
%      \arrow[ur, bend right, swap, "{\Phi_{A, A}}"]
%      & & \mathcal{F}(A)
%      & & \mathcal{F}(A) \otimes \mathcal{F}(A)
%      \arrow[ul, bend left, "{\Phi_{A, A}}"]
%    \end{tikzcd}
%  \end{equation*}
%  The top rectangle commutes because $\mathcal{F}$ is a monoidal functor, the bottom pentagon is $\mathcal{F}$ applied to the coherence diagram for $A$, and the two legs commute trivially.
%
%  Hopefully it's now obvious why we aren't proving the rest of these.
\end{proof}

\begin{definition}[tensor product of commutative monoids]
  \label{def:tensorproductofcommutativemonoids}
  Let $(\mathsf{C}, \otimes, 1)$ be a symmetric monoidal category and $(E_{1}, \mu_{1}, e_{1})$ and $(E_{2}, \mu_{2}, e_{2})$ monoids internal to $\mathsf{C}$. Then we can construct a new monoid internal to $\mathsf{C}$, the \defn{tensor product} of $E_{1}$ and $E_{2}$ with

  \begin{enumerate}
    \item The object given by $E_{1} \otimes E_{2}$

    \item The multiplication map
      \begin{equation*}
        \mu_{E_{1} \otimes E_{2}}\colon (E_{1} \otimes E_{2}) \otimes (E_{1} \otimes E_{2}) \to (E_{1} \otimes E_{2})
      \end{equation*}
      given by (notationally suppressing the obvious identities and associators)
      \begin{equation*}
        \begin{tikzcd}[column sep=huge]
          E_{1} \otimes E_{2} \otimes E_{1} \otimes E_{2} 
          \arrow[r, "\gamma"]
          & E_{1} \otimes E_{1} \otimes E_{2} \otimes E_{2}  
          \arrow[r, "\mu_{1} \otimes \mu_{2}"]
          & E_{1} \otimes E_{2}
        \end{tikzcd}
      \end{equation*}

    \item and the unit map $e_{E_{1} \otimes E_{2}}$ given by
      \begin{equation*}
        \begin{tikzcd}
          1
          \arrow[r, "\lambda^{-1}_{1}"]
          & 1 \otimes 1
          \arrow[r, "e_{1} \otimes e_{2}"]
          & E_{1} \otimes E_{2}
        \end{tikzcd}
      \end{equation*}
  \end{enumerate}

  It is easy to show that these maps make the necessary diagrams commute.
\end{definition}

\begin{theorem}
  \label{thm:tensorproductiscoproductincategoryofcommutativemonoids}
  Let $\mathsf{C}$ be a symmetric monoidal category. The category $\mathsf{CMon}(\mathsf{C})$ has coproducts, given by the tensor product of monoids.
\end{theorem}
\begin{proof}
  The canonical injections are given by 
  \begin{equation*}
    \iota_{1} = e_{1} \circ \rho_{E_{1}}^{-1},\qquad\text{and}\quad \iota_{2} = \lambda_{E_{2}}^{-1} \circ e_{2}.
  \end{equation*}

  It remains to check that for any commutative monoid $R$ and monoid homomorphisms $f_{i}\colon E_{i} \to R$, there exists a unique map $f\colon E_{1} \otimes E_{2} \to R$ making the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=large, column sep=large]
      E_{1}
      \arrow[r, "e_{2} \circ \rho_{E_{1}}^{-1}"]
      \arrow[rd, swap, "f_{1}"]
      & E_{1} \otimes E_{2}
      \arrow[d, "\exists!f"]
      & E_{2}
      \arrow[l, swap, "\lambda_{E_{2}}^{-1} \circ e_{1}"]
      \arrow[ld, "f_{2}"]
      \\
      & R
    \end{tikzcd}
  \end{equation*}

  The map $f$ is given by the composition $m_{R} \circ (f_{1} \otimes f_{2})$. To see that this indeed makes the diagram commute, consider the composition
  \begin{equation*}
    m_{R} \circ (f_{1} \otimes f_{2}) \circ e_{2} \circ \rho_{E_{1}}^{-1} = m_{R} \circ (f_{1} \otimes f_{2})(f \otimes 1_{C})
  \end{equation*}
\end{proof}

\begin{corollary}
  \label{cor:cmonophasproducts}
  For any symmetric monoidal category $\mathsf{C}$, the category $\mathsf{CMon}(\mathsf{C})^{\mathrm{op}}$ has finite products.
\end{corollary}

\subsection{Algebras} \label{sec:algebras}
\begin{definition}[commutative (super)algebra]
  \label{def:commutativesuperornotalgebra}
  Let $\mathscr{A}$ be either $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$. A \defn{commutative $\mathscr{A}$-algebra} is a commutative monoid internal to $\mathscr{A}$. The category of commutative $\mathscr{A}$-algebras is $\mathsf{CMon}(\mathscr{A})$.

  If $\mathscr{A} = \mathsf{Vect}_{k}$, we call the objects of $\mathsf{CMon}(\mathscr{A})$ \defn{commutative algebras}. If $\mathscr{A} = \mathsf{SVect}_{k}$, we call them \defn{supercommutative superalgebras}.

  That is to say, an commutative $\mathscr{A}$-algebra is a triple $(A, \nabla, \eta)$ where $A \in \Obj(\mathscr{A})$, $\nabla\colon A \otimes A \to A$, and $\eta\colon 1 \to A$ which makes the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=large, column sep=large]
      A \otimes A \otimes A
      \arrow[r, "A \otimes \nabla"]
      \arrow[d, swap, "\nabla \otimes A"]
      & A \otimes A 
      \arrow[d, "\nabla"]
      \\
      A \otimes A
      \arrow[r, "\nabla"]
      & A
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      A 
      \arrow[rr, bend left, "A"]
      \arrow[dr, shift left, "\eta \otimes A"]
      \arrow[dr, shift right, swap, "A \otimes \eta"]
      & & A
      \\
      & A \otimes A
      \arrow[ur, swap, "\nabla"]
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{note}
  Although \emph{supercommutative superalgebra} is the most precise name, it sounds a bit silly so we often call simply \emph{supercommutative algebras}, or just \emph{superalgebras}.
\end{note}

\begin{example}
  \label{eg:algebrasaremonoidsinternaltovect}
  Let us see that a monoid internal to $\mathsf{Vect}_{k}$ really is an associative $k$-algebra with unity. 

  Let $(A, \mu, e)$ be a monoid internal to $\mathsf{Vect}_{k}$. We need to find a product $\cdot\colon A \times A \to A$ and a unit element $1 \in A$; show that the product is bilinear and associative; and that the unit behaves like a unit.

  We have a linear map $\mu\colon V \otimes V \to V$, which gives us a bilinear map $V \times V \to V$ by pre-composition with the tensor product functor:
  \begin{equation*}
    \begin{tikzcd}
      V \times V
      \arrow[r, "\otimes"]
      & V \otimes V
      \arrow[r, "\mu"]
      & V
    \end{tikzcd};
    \qquad
    (v, v') \mapsto v \cdot v' \mu(v \otimes v') \equiv v \cdot v'.
  \end{equation*}
  It is associative since the associativity diagram says that
  \begin{equation*}
    (v_{1} \cdot v_{2}) \cdot v_{3} = \mu((v_{1} \otimes v_{2}) \otimes v_{3}) = \mu(v_{1} \otimes (v_{2} \otimes v_{3})) = v_{1} \cdot (v_{2} \cdot v_{3}).
  \end{equation*}

  For the unit, we take the image of the unit in $1=k$ under the map $e$; call it $1_{V} = e(1_{k})$. To see that this behaves like a unit, we let it act on $v \in V$:
  \begin{equation*}
    1 \cdot v = \mu(1 \otimes v).
  \end{equation*}
  However, the unitality diagram says that this has to equal $\lambda\colon 1 \otimes V \to V$, and we know how that behaves from \hyperref[eg:vectisamonoidalcategory]{Example \ref*{eg:vectisamonoidalcategory}}: it sends $r \otimes v \mapsto rv$. With $r = 1$, it just sends $1 \otimes v \mapsto v$ as we'd like. 

  The story for multiplication on the right is identical, but with $\rho$ instead of $\lambda$.

  Now, let $A$ be a unital, associative $k$-algebra. We need to show that $A$ is a monoid internal to $\mathsf{Vect}_{k}$. The map $A \otimes A \to A$ is exactly that guaranteed by the universal property of the tensor product; the map $e$ sends $r \in k$ to $r\cdot 1_{A} \in A$. It is easy to show that the appropriate diagrams commute.
\end{example}  

\begin{example}
  A commutative monoid internal to $\mathsf{Vect}_{k}$ is exactly a commutative, associative $k$-algebra with unity.
\end{example}

\begin{example}
  A supercommutative superalgebra (\hyperref[def:supercommutativesuperalgebra]{Definition \ref*{def:supercommutativesuperalgebra}}) is nothing else but a commutative monoid in the symmetric monoidal category $\mathsf{SVect}_{k}$.

  Let $V = V_{0} \oplus V_{1}$ be a super vector space. We define, as in \hyperref[eg:algebrasaremonoidsinternaltovect]{Example \ref*{eg:algebrasaremonoidsinternaltovect}}, a bilinear map $V \times V \to V$ by pre-composing $\mu$ with the tensor product on $\mathsf{SVect}_{k}$:
  \begin{equation*}
    \cdot\colon (v, v') \mapsto \mu(v \otimes v').
  \end{equation*}
  The product as defined here inherits the correct grading from the graded tensor product. The unit element is the image of the unit in $k$ under $e$, and the verification that this behaves correctly is identical to that in \hyperref[eg:algebrasaremonoidsinternaltovect]{Example \ref*{eg:algebrasaremonoidsinternaltovect}}.

  All that remains is the verification of the Koszul sign rule: we must have the multiplication law
  \begin{equation*}
    v_{1} \cdot v_{2} = (-1)^{\tilde{v_{1}} \cdot \tilde{v_{2}}} v_{2} \cdot v_{1}.
  \end{equation*}

  But we do by the following:
  \begin{align*}
    v_{1} \cdot v_{2} &= \mu(v_{1} \otimes v_{2}) \\
    &= \mu(\gamma_{V, V}(v_{1} \otimes v_{2})) &\left( \substack{\text{by the commutativity diagram}\\ \text{in \hyperref[def:internalmonoid]{Definition \ref*{def:internalmonoid}}}} \right)\\ 
    &= \mu((-1)^{\tilde{v_{1}} \cdot \tilde{v_{2}}} v_{2} \otimes v_{1}) \\
    &= (-1)^{\tilde{v_{1}} \cdot \tilde{v_{2}}} \mu(v_{2} \otimes v_{1}) \\
    &= (-1)^{\tilde{v_{1}} \cdot \tilde{v_{2}}} v_{2} \cdot v_{1}.
  \end{align*}
\end{example} 

\begin{example}
  For a super vector space concentrated in even degree, i.e. of the form $V \oplus 0$, a supercommutative superalgebra is just a commutative algebra.
\end{example}

The above example is a reflection of the following.

\begin{theorem}
  There is a full subcategory inclusion
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{Alg}_{k}
      \arrow[d, equals]
      \arrow[r, "\iota", hookrightarrow]
      & \mathsf{SAlg}_{k}
      \arrow[d, equals]
      \\
      \mathsf{CMon}(\mathsf{Vect}_{k}) 
      \arrow[r, "\iota", hookrightarrow]
      & \mathsf{CMon}(\mathsf{SVect}_{k})
    \end{tikzcd}
  \end{equation*}
  of commutative algebras into supercommutative algebras induced via \hyperref[lemma:monoidalfunctorspreserveinternalmonoids]{Lemma \ref*{lemma:monoidalfunctorspreserveinternalmonoids}} by the full inclusion $\mathcal{I}\colon \mathsf{Vect}_{k} \hookrightarrow \mathsf{SVect}_{k}$ (which is a strong braided monoidal functor by \hyperref[lemma:monoidalinclusionofvectorspacesintosupervectorspaces]{Lemma \ref*{lemma:monoidalinclusionofvectorspacesintosupervectorspaces}}). The image of a commutative algebra $A$ under $\iota$ is the supercommutative algebra $A \oplus 0$, and the image of a morphism $f\colon A \to B$ is the morphism $f \oplus 0\colon A \oplus 0 \to B \oplus 0$.
\end{theorem}
\begin{proof}
  To show that $\iota$ is a full subcategory inclusion, we need to show that it is fully faithful (\hyperref[def:fullfaithfulfunctor]{Definition \ref*{def:fullfaithfulfunctor}}), i.e. that for all commutative $k$-algebras $A$ and $B$, every morphism $f\in \Hom_{\mathsf{SAlg}_{k}}(\iota(A), \iota(B))$ can be written as the image $\iota(\tilde{f})$ of some morphism $\tilde{f} \in \Hom_{\mathsf{Alg}_{k}}(A, B)$, and the morphism $\tilde{f}$ is unique.

  Any such morphism $f$ is direct sum $f_{0} \oplus f_{1}$, where $f_{0}\colon A_{0} \to B_{0}$ and $f_{1}\colon A_{1} \to B_{1}$. Every supercommutative superalgebra in the image of $\iota$ is of the form $A \oplus 0$, so $f_{0}\colon A \to B$ and $f_{1}\colon 0 \to 0$. Thus $f$ can be written $\iota(f_{0})$, where $f_{0}$ is clearly unique.
\end{proof}



\section{Supervarieties}
%\begin{definition}
%  \label{def:functorcinfinity}
%  Let $M$ be a $C^{\infty}$ manifold (that is, recalling \hyperref[eg:moreexamplesofcategories]{Example \ref*{eg:moreexamplesofcategories}}, an object in the category $\mathsf{SmoothMfd}$.) The functor 
%  \begin{equation*}
%    C^{\infty}\colon \mathsf{SmoothMfd}^{\mathrm{op}} \rightsquigarrow \mathsf{Alg}_{\R}
%  \end{equation*}
%  is the defined by sending $M$ to the $\R$-algebra of smooth functions $M \to \R$, and morphisms $\varphi\colon M \to N$ to homomorphisms of $\R$-algebras $C^{\infty}(N) \to C^{\infty}(M)$ via the pullback. 
%\end{definition}
%
%\begin{theorem}
%  The functor $C^{\infty}\colon \mathsf{SmoothMfd}^{\mathrm{op}} \rightsquigarrow \mathsf{Alg}_{\R}$ is fully faithful (\hyperref[def:fullfaithfulfunctor]{Definition \ref*{def:fullfaithfulfunctor}}).
%\end{theorem}
%\begin{proof}
%  See \cite{KMS-natural-operations-differential-geometry}, lemma 35.8, corollaries 35.9 and 35.10.
%\end{proof}
%
%As we saw in \hyperref[lemma:fullyfaithfulfunctorinjectiveuptoisomorphism]{Lemma \ref*{lemma:fullyfaithfulfunctorinjectiveuptoisomorphism}}, the meaning of this theorem is that the functor $C^{^\infty}$ is injective on objects up to isomorphism. Thus, we can identify any smooth manifold with (the formal dual of) its algebra of functions, and if we are given any $\R$-algebra in the range of the functor $C^{\infty}$, we can always find a smooth manifold, unique up to isomorphism, which gives rise to it.
%
%We might take this idea even further: given \emph{any} associative algebra $A$, we can think of its formal dual as some sort of generalized manifold. 
%
%\begin{note}
%  My plan is to replace the above introduction with an exposition of why defining algebraic groups as group objects in $\mathsf{CMon}(\mathsf{SVect}_{k})$ is a good idea from an algebro-geometric perspective (without leaning on smooth manifolds, which are too poorly behaved to furnish a good example), as well as explaining exactly what is meant by `formal duality'. I'll leave it in for now, but what follows will be the \emph{real} introduction.
%\end{note}

This section expands on ideas taken from \cite{milne-affine-group-schemes}.

The objects central to our interests will be so-called \emph{affine super-groups}. These are a generalization of the notion of an \emph{affine algebraic group}, which is, roughly speaking, a group object in the category of affine algebraic varieties.

First, we must generalize the notion of an algebraic variety. Roughly speaking, an algebraic variety is a surface cut out as the zero set of a set of polynomials. 

\subsection{Varieties as formal duals to algebras}

Let $k$ be an algebraically closed field of characteristic zero and let 
\begin{equation*}
  S \subset k[x_{1}, x_{2}, \ldots, x_{n}] 
\end{equation*}
be a set of polynomials. For any $k$-algebra $R$, we can define the \emph{zero set} of $S$ over $R$ as follows:
\begin{equation*}
  Z_{R}(S) = \left\{ (a_{1}, a_{2}, \ldots, a_{n}) \in R^{n} \,\big|\, f(a_{1}, a_{2}, \ldots, a_{n}) = 0\text{ for all }f \in S \right\}.
\end{equation*}

We can see that algebraic varieties over $R$ are precisely sets of the type $Z_{R}(S)$.

This is a perfectly good definition of an algebraic variety, but it is poorly suited to the task at hand. In this section we will find a better one. First, we must make a few observations.

\begin{itemize}
  \item Clearly, if $\mathfrak{a} = (S)$ is the ideal generated by $S$, then $Z_{R}(S) = Z_{R}(\mathfrak{a})$ for any $k$-algebra $R$. 

  \item Any homomorphism of $k$-algebras $\varphi\colon R \to R'$ induces a map $Z_{R}(\mathfrak{a}) \to Z_{R'}(\mathfrak{a})$. Therefore, any ideal $\mathfrak{a}$ defines a functor 
    \begin{equation*}
      \mathcal{V}_{\mathfrak{a}}\colon k\mhyp\mathsf{Alg} \to \mathsf{Set};\qquad R \mapsto Z_{R}(\mathfrak{a})
    \end{equation*}
    which takes a $k$-algebra to its zero set.
\end{itemize}

\begin{theorem}
  \label{thm:zerosetisrepresentablefunctor}
  Let $\mathfrak{a} \subseteq k[x_{1}, x_{2}, \ldots, x_{n}]$ be an ideal, and let
  \begin{equation*}
    A = k[x_{1}, x_{2}, \ldots, x_{n}]/\mathfrak{a}.
  \end{equation*}

  The functor $\mathcal{V}_{\mathfrak{a}}\colon R \mapsto Z_{R}(\mathfrak{a})$ is representable (\hyperref[def:representablefunctor]{Definition \ref*{def:representablefunctor}}). It is represented by the $k$-algebra 
  \begin{equation*}
    A = k[x_{1}, x_{2}, \ldots, x_{n}]/\mathfrak{a}.
  \end{equation*}
\end{theorem}
\begin{proof}
  We need to show that $\mathcal{V}_{\mathfrak{a}}$ is naturally isomorphic to the functor
  \begin{equation*}
    h^{A}\colon R \mapsto \Hom_{k\mhyp\mathsf{Alg}}(A, R),
  \end{equation*}
  i.e. that there is a bijection
  \begin{equation*}
    \Phi_{R}\colon \Hom_{k\mhyp\mathsf{Alg}}(A, R) \to Z_{R}(\mathfrak{a})
  \end{equation*}
  which is natural in $R$.

  We define $\Phi_{R}$ as follows. Let $\eta\in \Hom_{k\mhyp\mathsf{Alg}}(A, R)$. Then $\eta$ extends uniquely to a $k$-algebra homomorphism 
  \begin{equation*}
    \bar{\eta}\colon k[x_{1}, x_{2}, \ldots, x_{n}] \to R
  \end{equation*}
  which vanishes on $\mathfrak{a}$. That is, we have the following commutative diagram.
  \begin{equation*}
    \begin{tikzcd}
      k[x_{1}, x_{2}, \ldots, x_{n}]
      \arrow[d, twoheadrightarrow, swap, "\pi"]
      \arrow[dr, "\bar{\eta}"]
      \\
      A
      \arrow[r, "\eta"]
      & R
    \end{tikzcd}
  \end{equation*}

  We then define
  \begin{equation*}
    \Phi_{R}\colon \eta \mapsto (\bar{\eta}(x_{1}), \bar{\eta}(x_{2}), \ldots, \bar{\eta}(x_{n})).
  \end{equation*}

  To simplify notation, let $\bar{\eta}(x_{i}) = a_{i}$.

  Of course, a priori the codomain of $\Phi_{R}$ is just $R^{n}$. Our first job is to show that our map is well-defined, i.e. that for any $f \in \mathfrak{a}$, $f(a_{1}, a_{2}, \ldots a_{n}) = 0$.

  Any $f \in k[x_{1}, x_{2}, \ldots, x_{n}]$ can be written
  \begin{equation*}
    f(x_{1}, \ldots, x_{n}) = \sum c_{D} x_{1}^{d_{1}}\cdots x_{n}^{d_{n}},
  \end{equation*}
  where $D$ is a multi-index. Now,
  \begin{align*}
    \bar{\eta}(f(x_{1}, \ldots, x_{n})) &= \bar{\eta}\left( \sum c_{D} x_{1}^{d_{1}}\cdots x_{n}^{d_{n}} \right) \\
    &= \sum c_{D} \bar{\eta}(x_{1})^{d_{1}}\cdots \bar{\eta}(x_{n})^{d_{n}} &\left( \text{since $\bar{\eta}$ is a homomorphism} \right) \\
    &= \sum c_{D} a_{1}^{d_{1}}\cdots a_{i}^{d_{n}} \\
    &= f(a_{1}, \ldots, a_{n}).
  \end{align*}

  But since the above diagram commutes, $\bar{\eta}(f) = \eta(\pi(f)) = \eta(0) = 0$, since $\eta \in \mathfrak{a}$.

  We still must show that $\Phi_{R}$ is bijective. It is clearly injective since any homomorphism 
  \begin{equation*}
    k[x_{1}, x_{2}, \ldots, x_{n}] \to R
  \end{equation*}
  is completely determined by how it behaves on the $x_{i}$, and the homomorphism $\bar{\eta}$ is uniquely determined by $\eta$. It is surjective since any tuple $(a_{1}, a_{2}, \ldots, a_{n})$ is the image of the unique homomorphism which sends $x_{i} \mapsto a_{i}$, $i = 1$, $2$, \dots, $n$.

  It remains only to show that $\Phi_{R}$ is natural, which can be seen from the commutativity of the following diagram (with $\varphi\colon R \to R'$ a homomorphism).
  \begin{equation*}
    \begin{tikzcd}
      \Hom_{k\mhyp\mathsf{Alg}}(A, R)
      \arrow[rrr, "\varphi \circ (-)"]
      \arrow[ddd, swap, "\Phi_{R}"]
      & & & \Hom_{k\mhyp\mathsf{Alg}}(A, R')
      \arrow[ddd, "\Phi_{R'}"]
      \\
      & \eta
      \arrow[r, mapsto]
      \arrow[d, mapsto]
      & \varphi \circ \eta
      \arrow[d, mapsto]
      \\
      & (\bar{\eta}(x_1), \ldots, \bar{\eta}(x_{n}))
      \arrow[r, mapsto]
      & ((\phi \circ \bar{\eta})(x_1), \ldots, (\phi \circ \bar{\eta})(x_{n}))
      \\
      Z_{R}(\mathfrak{a})
      \arrow[rrr, "\mathcal{V}_{\mathfrak{a}}(\varphi)"]
      & & & Z_{R'}(\mathfrak{a})
    \end{tikzcd}
  \end{equation*}
\end{proof}

The meaning of the above theorem is that we can study the variety cut out by the zeroes of the ideal $\mathfrak{a}$ in $R^{n}$ by studying the hom-set $\Hom_{k\mhyp\mathsf{Alg}}(A = k[x_{i}]/\mathfrak{a}, R)$. There is a lot of benefit to this new perspective: we have studied hom functors in some detail, and we can apply results about them to the study of varieties.

However, we can take this abstraction further. Since by definition any variety over $R$ is the zero-set $Z_{R}(\mathfrak{a})$ of some ideal $\mathfrak{a}$, we can view the study of varieties over $R$ as equivalent to the study of those functors $k\mhyp\mathsf{Alg} \to \mathsf{Set}$ which are represented by some finitely-presented $k$-algebra $A = k[x_{i}]/\mathfrak{a}$. The image of $R$ under such a functor would be of the form $\Hom_{k\mhyp\mathsf{Alg}}(A, R)$, hence naturally isomorphic to $Z_{R}(\mathfrak{a})$.

This new perspective allows us to make a new definition, which will be more useful to us.

\begin{definition}[affine (algebraic) variety 2]
  \label{def:affinealgebraicvariety2}
  An \defn{affine variety} is a representable functor $k\mhyp\mathsf{Alg} \to \mathsf{Set}$. An affine variety is \defn{algebraic} if it is represented by a finitely presented $k$-algebra.

  The category of affine algebraic varieties is thus given by the category of functors $k\mhyp\mathsf{Alg} \to \mathsf{Set}$ which are representable by a finitely presented $k$-algebra.
\end{definition}

By the covariant Yoneda lemma (see \hyperref[note:covariantyonedaembedding]{Note \ref*{note:covariantyonedaembedding}}), the category of representable functors $k\mhyp\mathsf{Alg} \to \mathsf{Set}$ is equivalent to the opposite of the category of $k$-algebras, and the subcategory of such functors which are represented by a finitely presented $k$-algebra is equivalent to the opposite of the category of finitely presented $k$-algebras. This gives us the following equivalent definition.

\begin{definition}[affine (algebraic) variety 3]
  \label{def:affinealgebraicvariety3}
  An \defn{affine variety} is the `formal dual' of a $k$-algebra; that is, it is an object in the category $k\mhyp\mathsf{Alg}^{\mathrm{op}}$. An affine variety is \defn{algebraic} if it is dual to a finitely presented $k$-algebra.

  By the `formal dual' of a finitely-presented $k$-algebra $A$, we mean that there is a covariant equivalence of categories between the category of finitely-presented $k$-algebras and the category of affine varieties.

\end{definition}

To summarize, the recipe for turning a finitely presented $k$-algebra into a variety is as follows.
\begin{enumerate}
  \item We start with a finitely-presented $k$-algebra A, which by definition can always be expressed as a quotient $k[x_{1}, x_{2}, \ldots, x_{n}]/\mathfrak{a}$ for some ideal $\mathfrak{a}$.

  \item We dualize, viewing $A$ as an object in the opposite category $k\mhyp\mathsf{Alg}^{\mathrm{op}}$.

  \item We apply the covariant Yoneda embedding to get the functor 
    \begin{equation*}
      h^{A} = \Hom_{k\mhyp\mathsf{Alg}}(A, -)\colon k\mhyp\mathsf{Alg} \to \mathsf{Set}.
    \end{equation*}

    By the covariant Yoneda lemma, the assignment $A \mapsto h^{A}$ is an equivalence of categories.

  \item Evaluating this functor on any $k$-algebra $R$ yields the set $\Hom_{k\mhyp\mathsf{Alg}}(A, R)$. For each $f \in \Hom_{k\mhyp\mathsf{Alg}}(A, R)$, the images $a_{i} = f(x_{i})$ of the generators $x_{i}$ give a point $(a_{1}, a_{2}, \ldots, a_{n}) \in R^{n}$ which lies on the variety in question.
\end{enumerate}

\begin{note}
  The condition of finite presentability is necessary from a geometric point of view to preserve the interpretation of $h^{A}$ as being the functor which assigns to a group its sets of `$R$-points', i.e. its points in affine $R$-space $\mathbb{A}^{n}_{R}$. However, as long as we vow to view these algebras only algebraically, we can just as easily work with algebras that are not finitely presented. It is still possible to view these algebras as formal duals to geometrical structures, namely schemes, but we will not take this approach.
\end{note}

\subsection{Supervarieties}
The generality introduced in the last section allows us to give a slick definition of a (super)variety, which we will use from now on.
%
%\begin{definition}[algebra modelled on a tensor category]
%  \label{def:algebramodelledonatensorcategory}
%  Let $\mathscr{A}$ be a $k$-tensor category. The \defn{category of $k$-algebras modelled on $\mathscr{A}$} is the category $\mathsf{CMon}(\mathscr{A})$.
%\end{definition}

\begin{definition}[affine variety]
  \label{def:supervariety}
  Let $\mathscr{A}$ be $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$. A \defn{affine $\mathscr{A}$-variety} is a group object in the category of representable functors $\mathscr{A} \to \mathsf{Set}$.

  If $\mathscr{A} = \mathsf{Vect}_{k}$, we call the affine $\mathscr{A}$-varieties \defn{affine varieties}. If $\mathscr{A} = \mathsf{SVect}_{k}$, we call them \defn{affine supervarieties}.
\end{definition}

We could equivalently say that an affine $\mathscr{A}$-variety was the formal dual to a commutative $\mathscr{A}$-algebra since by the Yoneda lemma the category of representable functors $\mathscr{A} \to \mathsf{Set}$ is equivalent to $\mathscr{A}^{\mathrm{op}}$.

\begin{notation}
  If we are given a supervariety in the form of a functor $G\colon \mathsf{SAlg}_{k} \to \mathsf{Set}$, we call the associated superalgebra $\mathscr{O}(G)$. If we are given a supercommutative superalgebra $A$, we call the associated functor $\spec(A)$. For our purposes, both $\mathscr{O}$ and $\spec$ are simply different notations for the opposite functor.
\end{notation}

Since we will be dealing only with affine supervarieties, we will often refer to them as simply supervarieties.

\begin{note}
  We do not give any definition of an affine \emph{algebraic} supervariety as we did for affine algebraic varieties. The reason for this is that the motivation for the definition of an affine algebraic variety is purely geometrical, and imagining supervarieties as embedded in $\mathbb{A}_{A}^{n}$ for $A$ a supercommutative algebra turns out not to be the correct approach. For the correct geometric interpretation of supervarieties, one needs to the theory of super schemes, which are beyond the scope of these notes. For us, it will suffice to consider supervarieties purely algebraically.
\end{note}

\section{Supergroups as supercommutative Hopf algebras}
We have seen that a variety $A$ can be viewed as the formal dual of commutative algebra $\mathscr{O}(A)$, and that a supervariety $S$ as the formal dual of a supercommutative superalgebra $\mathscr{O}(S)$. We will soon define affine (super)groups to be (super)varieties which are also groups, i.e. group objects $(A, m, e)$ in the category of affine (super)varieties. This extra structure on $A$ will correspond to extra structure on the algebra $\mathscr{O}(A)$ to which it is formally dual, giving it the structure of a \emph{Hopf algebra}. Roughly speaking, a Hopf algebra is both an algebra and a coalgebra, hence a \emph{bialgebra}, together with some extra structure. We have already seen the definition of an algebra (\hyperref[sec:algebras]{Subsection \ref*{sec:algebras}}); now we need some definitions leading up to that of a Hopf algebra.

\subsection{Coalgebras}
Coalgebras are dual to algebras. In other words, to get the definition of a coalgebra, we take the definition of an algebra and turn the arrows around. More formally, we have the following.

\begin{definition}[coalgebra]
  \label{def:coalgebra}
  Let $\mathscr{A}$ be either $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$ An $\mathscr{A}$-coalgebra is dual to a monoid internal to $\mathscr{A}^{\mathrm{op}}$. Thus the category of coalgebras is 
  \begin{equation*}
    \mathsf{CMon}(\mathscr{A}^{\mathrm{op}})^{\mathrm{op}}.
  \end{equation*}

  That is, a coalgebra is a triple $(C, \Delta, \varepsilon)$ where 
  \begin{itemize}
    \item $C \in \Obj(\mathsf{C})$;

    \item $\Delta\colon C \to C \otimes C$; and

    \item $\varepsilon\colon C \to k$;
  \end{itemize}
  which make the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=large, column sep=large]
      C
      \arrow[r, "\Delta"]
      \arrow[d, swap, "\Delta"]
      & C \otimes C
      \arrow[d, "C \otimes \Delta"]
      \\
      C \otimes C
      \arrow[r, swap, "\Delta \otimes C"]
      & C \otimes C \otimes C
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      C 
      \arrow[rr, bend left, "C"]
      \arrow[rd, swap, "\Delta"]
      && C
      \\
      & C \otimes C
      \arrow[ru, shift left, "C \otimes \varepsilon"]
      \arrow[ru, shift right, swap, "\varepsilon \otimes C"]
    \end{tikzcd}
  \end{equation*}

  A homomorphism of coalgebras $(C, \Delta_{C}, \varepsilon_{C}) \to (D, \Delta_{D}, \varepsilon_{D})$ is a morphism $f \in \Hom_{\mathscr{A}}(C, D)$ which makes the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      C \otimes C
      \arrow[r, "f \otimes f"]
      & D \otimes D
      \\
      C 
      \arrow[r, "f"]
      \arrow[u, "\Delta_{C}"]
      & D
      \arrow[u, swap, "\Delta_{D}"]
    \end{tikzcd}
  \end{equation*}
\end{definition}

\subsection{Bialgebras and Hopf algebras}
Bialgebras are objects $A$ which have both an algebra and coalgebra structure. 
\begin{definition}[bialgebra]
  \label{def:bialgebra}
  Let $\mathscr{A}$ be either $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$. An \defn{$\mathscr{A}$-bialgebra} is the formal dual of a monoid internal to $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$. 
\end{definition}

What exactly is the dual to a monoid in $\mathsf{CMon}(\mathscr{A})^{\text{op}}$? First, let's answer the question: what is a monoid in $\mathsf{CMon}(\mathscr{A})^{\text{op}}$. Then we can turn the arrows around.

A monoid in the category $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$ consists of object $\spec(H) \in \Obj(\mathsf{CMon}(\mathscr{A})^{\text{op}})$, together with the following.
\begin{itemize}
  \item A \emph{multiplication} map $\mu\colon \spec(H) \times \spec(H) \to \spec(H)$; and
  \item An \emph{identity} map $\iota\colon \spec(*) \to \spec(H)$, where $*$ is the terminal object in $\mathsf{CMon}(\mathscr{A})$.
\end{itemize}
We can organize this information like so.
\begin{equation*}
  \begin{tikzcd}
    \spec(H) \times \spec(H)
    \arrow[d, "\mu"]
    \\
    \spec(H) 
    \\
    \spec(*)
    \arrow[u, swap, "\iota"]
  \end{tikzcd}
\end{equation*}

The commutative monoid $H$ itself comes with some structure. In particular, it has its own unit map $\eta\colon * \to H$, and its own multiplication $\nabla\colon H \otimes H \to H$. We can write this like.

\begin{equation*}
  \begin{tikzcd}
    H \otimes H
    \arrow[d, "\nabla"]
    \\
    H
    \\
    *
    \arrow[u, swap, "\eta"]
  \end{tikzcd}
\end{equation*}

We can take the information from the first diagram and put it on the second as long as we dualize. Remember: by \hyperref[thm:tensorproductiscoproductincategoryofcommutativemonoids]{Theorem \ref*{thm:tensorproductiscoproductincategoryofcommutativemonoids}}, $\spec(H) \times \spec(H) = \spec(H \otimes H)$.

\begin{equation*}
  \begin{tikzcd}
    H \otimes H
    \arrow[d, bend left, "\nabla"]
    \\
    H
    \arrow[u, bend left, "\Delta"]
    \arrow[d, swap, bend right, "\varepsilon"]
    \\
    *
    \arrow[u, swap, bend right, "\eta"]
  \end{tikzcd}
\end{equation*}

Thus we will often say that a bialgebra is a quintuple $(A, \nabla, \eta, \Delta, \varepsilon)$.

The maps $\Delta$ and $\varepsilon$ are both algebra homomorphisms and coalgebra homomorphisms by definiton. The maps $\nabla$ and $\eta$ are algebra homomorphisms, but not a priori coalgebra homomorphisms. 

\begin{lemma}
  Let $(a, \nabla, \eta, \Delta, \varepsilon)$ be a bialgebra. Then $\nabla$ and $\eta$ are coalgebra homomorphisms.
\end{lemma}

\begin{proof}
  In order for $\Delta$ to be an algebra homomorphism it needs to make the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=large, column sep=large]
      A \otimes A
      \arrow[r, "\Delta \otimes \Delta"]
      \arrow[d, swap, "\nabla"]
      & A \otimes A \otimes A \otimes A
      \arrow[d, "\nabla \otimes \nabla"]
      \\
      A
      \arrow[r, "\Delta"]
      & A \otimes A
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      1
      \arrow[r, "\eta"]
      \arrow[rd, swap, "\eta \otimes \eta"]
      & A
      \arrow[d, "\Delta"]
      \\
      & A \otimes A
    \end{tikzcd}
  \end{equation*}

  The first diagram is the condition for $\Delta$ to be a coalgebra homomorphism, which it is by definition. Since $\Delta\colon 1 \to 1 \otimes 1$ is an isomorphism, we can modify the second slightly.
  \begin{equation*}
    \begin{tikzcd}
      1
      \arrow[r, "\eta"]
      \arrow[d, swap, "\Delta"]
      & A
      \arrow[d, "\Delta"]
      \\
      1 \otimes 1
      \arrow[r, swap, "\eta \otimes \eta"]
      & A \otimes A
    \end{tikzcd}
  \end{equation*}

  In this form it is clear that it is the condition for $\Delta$ to be a 
\end{proof}

\begin{definition}[bialgebra homomorphism]
  \label{def:bialgebrahomomorphism}
  Let $(A, \nabla, \eta, \Delta, \varepsilon)$ and $(A', \nabla', \eta', \Delta', \varepsilon')$ be bialgebras. A \defn{bialgebra homomorphism} is a morphism $A \to A'$ which is both an algebra homomorphism and a coalgebra homomorphism.
\end{definition}

\begin{lemma}
  \label{lemma:multiplicationisdualtodiagonalmap}
  Let $\spec(H)$ be a monoid internal to $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$. Then the diagonal map $\delta\colon \spec(H) \to \spec(H) \times \spec(H)$ is dual to the multiplication $\nabla\colon H \otimes H \to H$.
\end{lemma}
\begin{proof}
  The diagonal morphism is defined via the universal property of the product: it is the unique map which makes the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      & \spec(H)
      \arrow[dl, swap, "\mathrm{id}"]
      \arrow[dr, "\mathrm{id}"]
      \arrow[d, "\delta"]
      \\
      \spec(H)
      & \spec(H) \times \spec(H)
      \arrow[l, "\pi_{1}"]
      \arrow[r, swap, "\pi_{2}"]
      & \spec(H)
    \end{tikzcd}
  \end{equation*}

  Dualizing, we find that equivalently, the morphism dual to $\delta$ is defined uniquely by making the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      H
      \arrow[r, "e \circ \rho_{H}^{-1}"]
      \arrow[rd, swap, "H"]
      & H \otimes H
      \arrow[d, "\tilde{\delta}"]
      & H
      \arrow[ld, "H"]
      \arrow[l, swap, "\lambda_{H}^{-1} \circ e"]
      \\
      & H
    \end{tikzcd}
  \end{equation*}

  But the multiplication $\nabla$ makes the above diagram commute, so we are done.
\end{proof}

\begin{definition}[commutative Hopf algebra]
  \label{def:superornotcommutativehopfalgebra}
  Let $\mathscr{A} = \mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$. A \defn{commutative $\mathscr{A}$-Hopf algebra} is the formal dual to a group internal to the category $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$.

  If $\mathscr{A} = \mathsf{Vect}_{k}$, then we call the objects of $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$ \defn{commutative Hopf algebras}. If $\mathscr{A} = \mathsf{SVect}_{k}$, we call them \defn{supercommutative Hopf algebras}.
\end{definition}

\begin{note}
  By \hyperref[cor:cmonophasproducts]{Corollary \ref*{cor:cmonophasproducts}}, the product $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$ has all products, and by \hyperref[lemma:unitobjectisinitialincmon]{Lemma \ref*{lemma:unitobjectisinitialincmon}} it has terminal objects. This means that it makes sense to talk about group objects.
\end{note}

Since a group is just a monoid with inverses, a Hopf algebra is just a special case of the notion of a bialgebra. In particular, a group object in $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$ is a monoid $(\spec(H), m, e)$ together with an extra morphism $i\colon \spec(H) \to \spec(H)$ as shown below.
\begin{equation*}
  \begin{tikzcd}
    \spec(H) \times \spec(H)
    \arrow[d, "\mu"]
    \\
    \spec(H) 
    \arrow[loop right, "i"]
    \\
    \spec(*)
    \arrow[u, swap, "\iota"]
  \end{tikzcd}
\end{equation*}

The morphism $i$ must make the following diagram commute.

We can dualize this to get a map $S\colon H \to H$ and add it to our diagram summarizing the morphisms for a bialgebra.
\begin{equation*}
  \begin{tikzcd}
    H \otimes H
    \arrow[d, bend left, "\nabla"]
    \\
    H
    \arrow[u, bend left, "\Delta"]
    \arrow[d, swap, bend right, "\varepsilon"]
    \arrow[loop right, "S"]
    \\
    *
    \arrow[u, swap, bend right, "\eta"]
  \end{tikzcd}
\end{equation*}

From \hyperref[def:groupobject]{Definition \ref*{def:groupobject}} together with \hyperref[lemma:multiplicationisdualtodiagonalmap]{Lemma \ref*{lemma:multiplicationisdualtodiagonalmap}} we can see that the map $S$ is required to make the following diagram commute.
\begin{equation*}
  \begin{tikzcd}[row sep=3.6em,column sep=1em]
    & H\times H 
    \arrow[rr,"S\times H"] 
    & & H\times H 
    \arrow[dr,"\nabla"] 
    \\
    H 
    \arrow[ur,"\Delta"] 
    \arrow[rr, "\varepsilon"] 
    \arrow[dr, swap, "\Delta"] 
    & & * 
    \arrow[rr,"\eta"] 
    & & H 
    \\
    & H\times H 
    \arrow[rr," H\times S"] 
    & & H\times H 
    \arrow[ur, swap, "\nabla"]
  \end{tikzcd}
\end{equation*}

\begin{definition}[Hopf algebra homomorphism]
  \label{def:hopfalgebrahomomorphism}
  A homomorphism of Hopf algebras is simply a homomorphism of the underlying bialgebras.
\end{definition}

\begin{definition}[category of Hopf algebras]
  \label{def:categoryofhopfalgebras}
  Let $\mathscr{A}$ be either $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$. The category $\mathsf{Hopf}(\mathscr{A})$ is the category whose objects are $\mathscr{A}$-hopf algebras and whose morphisms are Hopf algebra homomorphisms.
\end{definition}

\subsection{Affine groups as the formal duals of Hopf algebras}
\begin{definition}[affine group]
  \label{def:affinesuperornotgroup}
  An \defn{affine $\mathscr{A}$-group} is an affine $\mathscr{A}$-variety that is also a group, i.e. a group object in the category of representable functors $\mathsf{CMon}(\mathscr{A}) \to \mathsf{Set}$. If $\mathscr{A} = \mathsf{Vect}_{k}$, then we say that affine $\mathscr{A}$-groups are called \defn{affine groups}. If $\mathscr{A} = \mathsf{SVect}_{k}$, we say that they are called \defn{affine supergroups}.
\end{definition}

By the Yoneda lemma, we could equivalently say that an affine $\mathscr{A}$-group $G$ is a group internal to the category $\mathsf{CMon}(\mathscr{A})^{\mathrm{op}}$, and this means that it is formally dual to a commutative Hopf algebra. Or to put it another way, there is a contravariant equivalence of categories between the category of affine (super)groups and the category of (super)commutative Hopf algebras.

\begin{note}
  We can talk about the `points' of an $\mathscr{A}$-group $G$ in terms of morphisms from the terminal object $k \in \Obj(\mathsf{Hopf}(\mathscr{A}))$.
\end{note}

\begin{example}
  \label{eg:slnisahopfalgebra}
  The prototypical example of an affine group is $\SL_{n}$. Here we will explain how $\SL_{n}$ is both a representable functor and a commutative Hopf algebra.

  As we have seen (\hyperref[eg:twofunctorsgrptocring]{Example \ref*{eg:twofunctorsgrptocring}}), $\GL_{n}$ is a functor $\mathsf{CRing} \to \mathsf{Grp}$. The same is certainly true of $\SL_{n}$, which will be more convenient to study. 

  We can also view $\SL_{n}$ as a functor to $\mathsf{Set}$ by composing it with the forgetful functor $\mathsf{Grp} \to \mathsf{Set}$. Since all $k$-algebras are in particular commutative rings, we can also view $\SL_{n}$ as a functor $k\mhyp\mathsf{Alg} \to \mathsf{Set}$. This is the viewpoint we will take from now on. 

  For each commutative algebra $R$, the group structure on $\SL_{n}(R)$ gives us the following.
  \begin{itemize}
    \item Matrix multiplication on $\SL_{n}(R)$ gives us a natural transformation
      \begin{equation*}
        m\colon \SL_{n} \times \SL_{n} \to \SL_{n}.
      \end{equation*} 

    \item We get a natural transformation $e\colon \mathcal{T}_{e} \to \SL_{n}$, where $\mathcal{T}_{e}\colon k\mhyp\mathsf{Alg} \to \mathsf{Grp}$ is the constant functor which sends everything to the trivial group. This picks out the identity element of $\SL_{n}$.

    \item We get a natural transformation $i\colon \SL_{n} \to \SL_{n}$ which sends matrices to their inverses.
  \end{itemize}

  These natural transformations make $\SL_{n}$ a group object (\hyperref[def:groupobject]{Definition \ref*{def:groupobject}}) in the functor category (\hyperref[def:functorcategory]{Definition \ref*{def:functorcategory}}) $\mathsf{Func}(k\mhyp\mathsf{Alg}, \mathsf{Set})$.

  Thanks to the proof of \hyperref[thm:zerosetisrepresentablefunctor]{Theorem \ref*{thm:zerosetisrepresentablefunctor}}, it is hopefully no surprise that the functor $\SL_{n}$ is represented by the $k$-algebra 
  \begin{equation*}
    A = k[X_{11}, X_{12}, \ldots, X_{nn}]/(\det(X_{ij}) - 1).
  \end{equation*}

  That is, there is a natural isomorphism 
  \begin{equation*}
    \SL_{n} \simeq \Hom_{k\mhyp\mathsf{Alg}}(A, -) = h^{A}.
  \end{equation*}

  By the Yoneda lemma, we can learn everything we need to know about $h^{A}$ by studying its opposite object under the embedding $h^{A} \mapsto A$, where $A$ is viewed as an object in the opposite category $\mathsf{Alg}_{k} = \mathsf{CMon}(\mathsf{Vect}_{k})$. In particular: 
  \begin{itemize}
    \item The identity $e\colon h^{*} \to h^{A}$ is the image of a map $\varepsilon\colon A \to *$, where $* = k$ is the initial object in the category $k\mhyp\mathsf{Alg}$.

    \item The inverse $i\colon h^{A} \to h^{A}$ is the image of a map $S\colon A \to A$

    \item By combining \hyperref[thm:tensorproductiscoproductincategoryofcommutativemonoids]{Theorem \ref*{thm:tensorproductiscoproductincategoryofcommutativemonoids}} and the coproduct section of \hyperref[eg:naturaltransformationsforcccs]{Example \ref*{eg:naturaltransformationsforcccs}}, $h^{A} \times h^{A} \simeq h^{A \otimes A}$. Therefore the multiplication $m$ can be viewed as a natural transformation $h^{A \otimes A} \to h^{A}$, and its counterpart on $k\mhyp\mathsf{Alg}$ is a map $A \to A \otimes A$, which we'll call $\Delta$.
  \end{itemize}

  Now let's consider how these behave on elements. First, we need some notation. Let 
  \begin{equation*}
    \pi\colon k[X_{11}, X_{12}, \ldots, X_{nn}] \to k[X_{11}, X_{12}, \ldots, X_{nn}]/(\det(X_{ij}) - 1)
  \end{equation*}
  be the canonical projection, and let $x_{ij} = \pi(X_{ij})$.

  \begin{itemize}
    \item The counit $\varepsilon$ maps 
      \begin{equation*}
        x_{ij} \mapsto 
        \begin{cases}
          1, &i = j\\
          0, &i \neq j
        \end{cases}.
      \end{equation*}

    \item The map $S\colon A \to A$ sends each $x_{ij}$ to the $(i, j)$th element in the formal inverse matrix $(x_{ij})^{-1}$.

    \item The map $\Delta\colon A \mapsto A \otimes A$ maps
      \begin{equation*}
        x_{ij} \mapsto \sum_{k=1}^{n} x_{ik} \otimes x_{kj}.
      \end{equation*}
  \end{itemize}
\end{example}

The algebraic group $\SL_{n}$ is easier to study than $\GL_{n}$ because its defining equation, $\det(X_{ij}) = 1$, defines it in terms of the zeroes of a polynomial. However, we can treat the case $G = \GL_{n}$ in a similar way; we just have to be a bit tricky.

\begin{example}
  \label{eg:glnisanalgebraicgroup}
  We now show that $\GL_{n}$ is an algebraic group. Unfortunately, the set
  \begin{equation*}
    \left\{ (x_{11}, x_{12} \ldots, x_{nn}) \in R^{n \times n}\,\big|\, \det(x_{ij}) \neq 0 \right\}
  \end{equation*}
  is not given by the zeroes of any sets of polynomials, so we cannot mimic exactly the construction of $\SL_{n}$. The trick is to add an extra coordinate and then ignore it. Define
  \begin{equation*}
    A = k[X_{11}, X_{12}, \ldots, X_{nn}, Y] / (\det(X_{ij})\cdot Y - 1),
  \end{equation*}
  let $\pi$ be the canonical projection as before, and let
  \begin{equation*}
    x_{ij} = \pi(X_{ij}),\qquad y = \pi(Y).
  \end{equation*}

  Then $\GL_{n}$, as a functor $k\mhyp\mathsf{Alg} \to \mathsf{Set}$, is represented by $A$. We have the same maps as before, but this time slightly extended:
  \begin{itemize}
    \item The counit $\varepsilon$ maps 
      \begin{equation*}
        x_{ij} \mapsto 
        \begin{cases}
          1, &i = j\\
          0, &i \neq j
        \end{cases},
        \qquad\text{and}\quad y \mapsto 1.
      \end{equation*}

    \item The map $S\colon A \to A$ sends each $x_{ij}$ to $y\cdot(x)_{ij}^{-1}$ (where $(x)_{ij}^{-1}$ is the $(i,j)$th element of the formal `inverse matrix' to $x_{ij}$), and $y$ to 1.

    \item The map $\Delta\colon A \mapsto A \otimes A$ maps
      \begin{equation*}
        x_{ij} \mapsto \sum_{k=1}^{n} x_{ik} \otimes x_{kj},\qquad\text{and}\quad y \mapsto y \otimes y.
      \end{equation*}
  \end{itemize}
\end{example}

\begin{definition}[inner parity]
  \label{def:innerparity}
  Let $G$ be an affine supergroup. An \defn{inner parity} on $G$ is an homomorphism $\varepsilon^{*}\colon \mathscr{O}(G) \to k$ such that
  \begin{itemize}
    \item the result of the composition
      \begin{equation*}
        \begin{tikzcd}
          \mathscr{O}(G) 
          \arrow[r, "\Psi"]
          & \mathscr{O}(G) \otimes \mathscr{O}(G)
          \arrow[r, "\varepsilon^{*} \otimes \varepsilon^{*}"]
          & k \otimes k \simeq k
        \end{tikzcd},
      \end{equation*}
      agrees with the counit $\varepsilon\colon \mathscr{O}(G) \to k$.
      Morally, this means that it behaves like the multiplication by $\pm 1$; this will become clear when we study the representation theory of affine supergroups.

    \item The result of the composition
      \begin{equation*}
        \begin{tikzcd}[column sep=huge]
          \mathscr{O}(G) 
          \arrow[r, "(\mathrm{id} \otimes \Psi) \circ \Psi"]
          & \mathscr{O}(G) \otimes \mathscr{O}(G) \otimes \mathscr{O}(G)
          \arrow[r, "\varepsilon^{*} \otimes \mathrm{id} \otimes (c \circ \varepsilon^{*})"]
          & k \otimes \mathscr{O}(G) \otimes k
        \end{tikzcd}
      \end{equation*}
      is equal to the parity involution (\hyperref[def:parityinvolution]{Definition \ref*{def:parityinvolution}}).
  \end{itemize}
\end{definition}

\section{Modules and comodules in symmetric monoidal categories}
For the remainder of this section, $\mathscr{A}$ will stand for either $\mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$.

\subsection{Modules}
\begin{definition}[module object]
  \label{def:moduleobject}
  Let $(A, \mu, e)$ a monoid internal to $\mathscr{A}$. A \defn{[left] module object} in $\mathscr{A}$ over $A$ is
  \begin{enumerate}
    \item An object $N \in \Obj(\mathscr{A})$ and

    \item A morphism $\rho\colon A \otimes N \to N$ (called the \emph{action})
  \end{enumerate}
  such that the following diagrams commute.
  \begin{enumerate}
    \item (unitality)
      \begin{equation*}
        \begin{tikzcd}
          1 \otimes N
          \arrow[r, "e \otimes 1_{N}"]
          \arrow[dr, swap, "\lambda_{N}"]
          & A \otimes N
          \arrow[d, "\rho"]
          \\
          & N
        \end{tikzcd}
      \end{equation*}

    \item (action property)
      \begin{equation*}
        \begin{tikzcd}
          A \otimes A \otimes N
          \arrow[d, swap, "\mu \otimes N"]
          \arrow[r, "A \otimes \rho"]
          & A \otimes N
          \arrow[d, "\rho"]
          \\
          A \otimes N
          \arrow[r, "\rho"]
          & N
        \end{tikzcd}
      \end{equation*}
  \end{enumerate}
\end{definition}

\begin{definition}[homomorphism of module objects]
  \label{def:homomorphismofmoduleobjects}
  A homomorphism of [left] $A$-module objects $(N_{1}, \rho_{1})$, $(N_{2}, \rho_{2})$ is a morphism
  \begin{equation*}
    f\colon N_{1} \to N_{2}
  \end{equation*}
  such that the diagram
  \begin{equation*}
    \begin{tikzcd}
      A \otimes N_{1}
      \arrow[r, "1_{A} \otimes f"]
      \arrow[d, swap, "\rho_{1}"]
      & A \otimes N_{2}
      \arrow[d, "\rho_{2}"]
      \\
      N_{1}
      \arrow[r, swap, "f"]
      & N_{2}
    \end{tikzcd}
  \end{equation*}
  commutes.
\end{definition}

\begin{definition}[category of internal $A$-modules]
  \label{def:categoryofinternalmodules}
  Write $A\mhyp\mathsf{Mod}(\mathscr{A})$ for the category whose objects are $A$-modules internal to $\mathscr{A}$ and whose morphisms are homomorphisms of module objects.
\end{definition}

\begin{example}
  \label{eg:categoryof1modulesisequivalenttocategoryitself}
  As we saw in \hyperref[eg:tensorunitisinternalmonoid]{Example \ref*{eg:tensorunitisinternalmonoid}}, we can view the tensor unit $1 \in \Obj(\mathscr{A})$ as a monoid internal to $\mathscr{A}$. Then for every object $C \in \Obj(\mathscr{A})$, the left unitor $\lambda_{C}\colon 1 \otimes C \to C$ makes $C$ into a a left module.

  This holds for any $C \in \Obj(\mathscr{A})$, and in fact gives an equivalence of categories
  \begin{equation*}
    \mathscr{A} \simeq 1\mhyp\mathsf{Mod}(\mathscr{A}).
  \end{equation*}
\end{example}

\begin{example}
  \label{eg:anyringisamoduleoveritself}
  Any monoid $(A, \mu, e)$ is a left-module over itself with $\rho = \mu$.
\end{example}

\begin{example}
  \label{eg:freemodule}
  Let $C \in \Obj(\mathscr{A})$ and $(A, \mu, e)$ a monoid internal to $\mathscr{A}$. Then $A \otimes C$ has a natural left-module structure with
  \begin{equation*}
    \rho\colon A \otimes (A \otimes C) \to A \otimes C
  \end{equation*}
  given by the composition.
  \begin{equation*}
    \begin{tikzcd}
      A \otimes (A \otimes C)
      \arrow[r, "{\alpha_{A, A ,C}^{-1}}"]
      & (A \otimes A) \otimes C
      \arrow[r, "\rho \otimes 1_{C}"]
      & A \otimes C
    \end{tikzcd}.
  \end{equation*}

  Modules of this form are called \defn{free modules} because the the functor 
  \begin{equation*}
    \mathcal{F}_{A}\colon \mathscr{A} \to A\mhyp\mathsf{Mod}(\mathscr{A});\qquad C \mapsto (C \otimes A, \rho)
  \end{equation*}
  which sends an object to its free $A$-module is left-adjoint to the functor 
  \begin{equation*}
    \mathcal{U}\colon A\mhyp\mathsf{Mod}(\mathscr{A}) \to \mathscr{A};\qquad (N, \rho) \mapsto N.
  \end{equation*}
\end{example}

\begin{definition}[tensor product of module objects]
  \label{def:tensorproductofmoduleobjects}
  For any commutative monoid $(A, \mu, e)$ and two left $A$-module objects $(N_{1}, \rho_{1})$ and $(N_{2}, \rho_{2})$, the \defn{tensor product of module objects}, denoted $N_{1} \otimes_{A} N_{2}$, is the coequalizer
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      N_{1} \otimes A \otimes N_{2}
      \arrow[r, shift left=1.5, "1_{N_{1}} \otimes \rho_{2}"]
      \arrow[r, shift right=1.5, swap, "{\rho_{1} \circ (\gamma_{N_{1}, A} \otimes N_{2})}"]
      & N_{1} \otimes N_{2}
      \arrow[r, "\mathrm{coeq}"]
      & N_{1} \otimes_{A} N_{2}
    \end{tikzcd}
  \end{equation*}

  That is to say, roughly speaking, it is the quotient of $N_{1} \otimes A \otimes N_{2}$ by the smallest equivalence relation so that left-multiplication agrees with right-multiplication.
\end{definition}

\begin{definition}[function module]
  \label{def:functionmodule}
  For $(N_{1}, \rho_{1})$ and $(N_{2}, \rho_{2})$ two $A$-modules internal to a closed symmetric $\mathscr{A}$, the \defn{function module} $\hom_{A}(N_{1}, N_{1})$ is, as an object, the equalizer 
  \begin{equation*}
    \begin{tikzcd}
      \hom_{A}(N_{1}, N_{2})
      \arrow[rr, "\mathrm{equ}"]
      & & {[N_{1}, N_{2}]_{\mathscr{A}}}
      \arrow[rr, "{[\rho_{1}, 1_{N_{2}}]_{\mathscr{A}}}"]
      \arrow[dr, swap, "A \otimes (-)"]
      & & {[A \otimes N_{1}, N_{2}]_{\mathscr{A}}}
      \\
      & & & {[A \otimes N_{1}, A \otimes N_{2}]_{\mathscr{A}}}
      \arrow[ur, swap, "{[1_{A \otimes N_{1}}, \rho_2]_{\mathscr{A}}}"]
    \end{tikzcd},
  \end{equation*}
  where $[-,-]_{\mathscr{A}}$ denotes the hom functor internal to $\mathscr{A}$, and $A \otimes (-)$ is the natural transformation from \hyperref[lemma:cantensorbothsidesofinternalhom]{Lemma \ref*{lemma:cantensorbothsidesofinternalhom}}.

  The left action $A \otimes \hom_{A}(N_{1}, N_{2}) \to \hom_{A}(N_{1}, N_{2})$ is given by 
\end{definition}

\begin{theorem}
  Let $(A, \mu, e)$ be a commutative monoid internal to $\mathscr{A}$. Then if $\mathscr{A}$ has all coequalizers, the category $A\mhyp\mathsf{Mod}(\mathscr{A})$ of $A$-modules internal to $\mathscr{A}$ becomes a symmetric monoidal category with
  \begin{itemize}
    \item The tensor product $\otimes$ given by $\otimes_{A}$ (see \hyperref[def:tensorproductofmoduleobjects]{Definition \ref*{def:tensorproductofmoduleobjects}}).

    \item The tensor unit 1 given by $A$, with the $A$-module structure as in \hyperref[eg:anyringisamoduleoveritself]{Example \ref*{eg:anyringisamoduleoveritself}}.
  \end{itemize}

  Additionally, if all equalizers exist the $\mathscr{A}$ is a closed monoidal category, with the function module $\hom_{A}$ from \hyperref[def:functionmodule]{Definition \ref*{def:functionmodule}} acting as the internal hom.
\end{theorem}
\begin{proof}
  See \cite{nlab-deligne-theorem}, Proposition 3.65.
\end{proof}

\subsection{Comodules}
\begin{definition}[comodule object]
  \label{def:comoduleobject}
  Let $(A, \Delta, \varepsilon)$ be an $\mathscr{A}$-coalgebra.

  An \defn{$A$-comodule} is a pair $(V, \rho)$, where $V \in \Obj(\mathscr{A})$ and
  \begin{equation*}
    \rho\colon V \to A \otimes V
  \end{equation*}
  is a morphism which makes the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}
      V
      \arrow[r, "\rho"]
      & V \otimes A
      \arrow[r, shift left, "\rho \otimes V"]
      \arrow[r, shift right, swap, "V \otimes \Delta"]
      & V \otimes A \otimes A
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      V
      \arrow[rr, bend left, "V"]
      \arrow[r, swap, "\rho"]
      & V \otimes A
      \arrow[r, swap, "V \otimes \varepsilon"]
      & V
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{definition}[comodule homomorphism]
  \label{def:comodulehomomorphism}
  Let $V$ and $V'$ be $A$-comodule objects. A \defn{$A$-comodule homomorphism} $\varphi\colon V \to V'$ is a homomorphism of commutative monoids which makes the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      V 
      \arrow[r, "\varphi"]
      \arrow[d, swap, "\rho"]
      & V'
      \arrow[d, "\rho'"]
      \\
      V \otimes A
      \arrow[r, "\varphi \otimes A"]
      & V' \otimes A
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{definition}[category of comodules]
  \label{def:categoryofcomodules}
  Denote by $A\mhyp\mathsf{Comod}$ the category whose objects are comodules over $A$ (\hyperref[def:comoduleobject]{Definition \ref*{def:comoduleobject}}) and whose morphisms are comodule homomorphisms (\hyperref[def:comodulehomomorphism]{Definition \ref*{def:comodulehomomorphism}}).
\end{definition}

\begin{definition}[tensor product of comodules]
  \label{def:tensorproductofcomodules}
  Let $A$ be a commutative Hopf algebra (\hyperref[def:superornotcommutativehopfalgebra]{Definition \ref*{def:superornotcommutativehopfalgebra}}) over $k$, and let $V$ and $V'$ be $A$-comodules (\hyperref[def:comoduleobject]{Definition \ref*{def:comoduleobject}}).

  The \defn{comodule tensor product} is the object $V \otimes V'$ together with the coaction $V \otimes V' \to A \otimes V \otimes V'$ given by the composition
  \begin{equation*}
    \begin{tikzcd}
      V \otimes V'
      \arrow[r, "\rho \otimes \rho'"]
      & A \otimes V \otimes A \otimes V'
      \arrow[r, "\gamma"]
      & A \otimes A \otimes V \otimes V'
      \arrow[r, "\nabla"]
      & A \otimes V \otimes V'
    \end{tikzcd}.
  \end{equation*}
\end{definition}

\section{Representations as comodules}
Let $\mathscr{A} = \mathsf{Vect}_{k}$ or $\mathsf{SVect}_{k}$.
\begin{definition}[representation of a group]
  \label{def:representationofgroup}
  Let $G$ be an affine $\mathscr{A}$-group, and let $H = \mathscr{O}(G)$ be its dual $\mathscr{A}$-Hopf algebra. A \defn{representation} of $G$ on $V \in \Obj(\mathscr{A})$ is a natural transformation $r\colon G \to \mathrm{Aut}_{(-)\mhyp\mathsf{Mod}}(V \otimes -)$, where for any commutative $\mathscr{A}$-algebra $R$, $V \otimes R$ has the right module structure from \hyperref[eg:freemodule]{Example \ref*{eg:freemodule}}.

  That is, for every commutative $\mathscr{A}$-algebra $R$, it is a group homomorphism $G(R) \to \mathrm{Aut}_{R\mhyp\mathsf{Mod}}(V \otimes R)$ such that the following diagram commutes for every $f\colon R \to S$.
  \begin{equation*}
    \begin{tikzcd}
      G(R)
      \arrow[r, "G(f)"]
      \arrow[d, swap, "\eta_{R}"]
      & G(S)
      \arrow[d, "\eta_{S}"]
      \\
      \mathrm{Aut}_{R\mhyp\mathsf{Mod}}(V \otimes R)
      \arrow[r, swap]
      & \mathrm{Aut}_{S\mhyp\mathsf{Mod}}(V \otimes S)
    \end{tikzcd}
  \end{equation*}

  A representation is called \defn{finite-dimensional} if $V$ is finite-dimensional.
\end{definition}

\begin{theorem}
  Let $G$ be an affine $\mathscr{A}$-group. There is a bijection between
  \begin{itemize}
    \item The set of of representations of $G$, i.e. the set of natural transformations
      \begin{equation*}
        \eta\colon G \to \mathrm{Aut}_{(-)\mhyp\mathsf{Mod}}(V \otimes -)
      \end{equation*}

    \item The set of $\mathscr{O}(G)$-comodules.
  \end{itemize}
\end{theorem}
\begin{proof}
  Let $A = \mathscr{O}(G)$.

  Pick some $V \in \Obj(\mathscr{A})$. Then the natural transformation $\eta$ has a component 
  \begin{equation*}
    \eta_{A}\colon G(A) = \Hom_{\mathsf{CMon}(\mathscr{A})}(A, A) \to \Aut_{A\mhyp \mathsf{Mod}}(V \otimes A). 
  \end{equation*}
  The image of the element $\mathrm{id}_{A}$ is an isomorphism $\tilde{\rho}\colon V \otimes A \to V \otimes A$. This restricts to a map $\rho\colon V \to V \otimes A$ which is a comodule structure.

  See \cite{milne-affine-group-schemes}, Proposition 6.1, page 118.
\end{proof}

\begin{example}
  Consider an affine algebraic group $G$ with Hopf algebra $\mathscr{O}(G) = A$.

  Consider the simple case $V = k^{n}$. Let $\hat{e}_{i}$ be the canonical basis for $V$. A representation of $G$ on $V$ is a natural transformation $r\colon G \to \Aut_{(-)\mhyp\mathsf{Mod}}$ whose components are group homomorphisms $r_{R}\colon G(R) \to \Aut_{R\mhyp\mathsf{Mod}}(V \otimes R)$. Any such group homomorphism maps each element $g \in G(R)$ to an automorphism of $V \otimes R$, which is given by an $n \times n$ matrix $r_{ij}$ with coefficients in $R$. This acts on elements of $V \otimes R$ via 
  \begin{equation*}
    \sum_{i} \hat{e}_{i} \otimes r_{i} \mapsto \sum_{ij} \hat{e}_{j} \otimes (r_{j}\cdot r_{ji}).
  \end{equation*}

  It also restricts to a map $V \to V \otimes R$ which acts on basis vectors via
  \begin{equation*}
    \hat{e}_{i} \mapsto \sum_{j = 1}^{n} \hat{e}_{j} \otimes r_{ji}.
  \end{equation*}

  The fact that the assignment of $g$ to $r_{ij}(g)$ is a group homomorphism implies that $gg'$ will be assigned to 
  \begin{equation*}
    \sum_{k = 1}^{n} r_{jk}(g) \cdot r_{ki}(g').
  \end{equation*}

  But how does one ensure that the components $r_{R}$ taken together are indeed natural? Since $A$ is an $\mathscr{A}$-algebra, the representation $r$ must have a component $r_{A}\colon G(A) \to \mathrm{Aut}_{A\mhyp\mathsf{Mod}}(V \otimes A)$. Since $G(A) = \Hom_{\mathscr{A}\mhyp\mathsf{Alg}}(A, A)$, it contains a distinguished element $\mathrm{id}_{A}$. For any other $\mathscr{A}$-algebra $S$, consider the commuting square
\end{example}

\begin{definition}[category of representations]
  \label{def:categoryofrepresentations}
  Let $G$ be an affine $\mathscr{A}$-group. We \emph{define} the category $\mathscr{O}(G)\mhyp\mathsf{CoMod}$ of representations $\mathsf{Rep}(G)$ to be the category of $\mathscr{O}(G)$-comodules.
\end{definition}


%\section{Super fiber functors and their automorphism groups}
%We would like to view tensor categories $\mathscr{A}$ as categories of representations. Thus, we would like to be able to view the objects of $\mathscr{A}$ as being the objects of some other category $\mathscr{V}$, plus some extra structure. The extra structure should be extra in the sense that it is forgettable, so there should be a forgetful functor
%\begin{equation*}
%  \omega\colon \mathscr{A} \rightarrow \mathscr{V}.
%\end{equation*}
%For example, the category $\mathsf{FinRep}_{k}(G) = \mathsf{Func}(\mathbf{B}G, \mathsf{FinVect}_{k})$ of finite-dimensional $k$-linear representations of $G$ has as its objects pairs $(\rho, V)$, where $V$ is a vector space and $\rho$ is a homomorphism $G \to \mathrm{Aut}(V)$. There is an obvious forgetful functor $\mathsf{FinRep}_{k}(G) \to \mathsf{FinVect}_{k}$ which sends $(\rho, V) \to V$. Note that this functor is not `maximally forgetful' in the sense that it retains some of its structure. Thus it is wise to demand that such a functor $\omega$ retain a reasonable amount of structure. In our case, it should be a tensor functor (\hyperref[def:tensorfunctor]{Definition \ref*{def:tensorfunctor}}).
%
%\begin{definition}[fiber functor]
%  \label{def:fiberfunctor}
%  Let $\mathscr{A}$ and $\mathscr{T}$ be two $k$-tensor categories. Suppose that both $\mathscr{A}$ and $\mathscr{T}$ satisfy the following criteria.
%  \begin{enumerate}
%    \item All objects are of finite length.
%
%    \item All of the hom-spaces are finite-dimensional as $k$-vector spaces.
%  \end{enumerate}
%
%  Let $R \in \mathsf{CMon}(\mathsf{Ind}(\mathscr{T}))$, where $\mathsf{Ind}(\mathscr{T})$ is the \emph{category of inductive objects over $\mathscr{T}$}, see \hyperref[def:categoryofindobjects]{Definition \ref*{def:categoryofindobjects}}.
%
%  A tensor functor (\hyperref[def:tensorfunctor]{Definition \ref*{def:tensorfunctor}})
%  \begin{equation*}
%    \omega\colon \mathscr{A} \to R\mhyp\mathsf{Mod}(\mathsf{Ind}(\mathscr{T}))
%  \end{equation*}
%  is called a \defn{fiber functor} on $\mathscr{A}$ over $R$.
%\end{definition}
%
%\begin{definition}[super fiber functor, neutral super tannakian category]
%  \label{def:suberfiberfunctorneutralsupertannakiancategory}
%  A fiber functor over $\mathscr{T} = \mathsf{FinSVect}$ is called a \defn{super fiber functor}. A tensor category $\mathscr{A}$ which admits a super fiber functor is called a \defn{neutral super tannakian category}.
%
%  Note that in this case a fiber functor over $\mathsf{FinSVect}_{k}$ is a fiber functor to
%  \begin{equation*}
%    \mathsf{CMon}(\mathsf{Ind}(\mathsf{FinSVect}_{k})) = \mathsf{CMon}(\mathsf{SVect}_{k}).
%  \end{equation*}
%\end{definition}
%
%\section{Super-exterior powers and Schur functors}
%One of the core ideas of category theory is that the intrinsic structure of an object can often be best understood by studying its relationships to other objects. For $V$ a vector space, one defines its dimension $\dim(V)$ as the cardinality of any linearly independent spanning subset of $V$. When moving to the category $\mathsf{Vect}_{k}$, one does not have the option of defining things in terms of elements. However, there is an alternate way of defining $\dim(V)$ which does not reference the elements: it is the smallest natural number $n$ such 
%\begin{equation*}
%  \bigwedge^{n} V = 0.
%\end{equation*}
%
%This definition is very appealing because the notion of an exterior product makes sense in any tensor category. 
%
%\begin{definition}[exterior power]
%  \label{def:exteriorpowerintensorcategory}
%  Let $(\mathscr{A}, \otimes, 1)$ be a $k$-tensor category with braiding $\gamma$. Let $X \in \Obj(\mathscr{A})$. For any $n \in \N$, we can use $\gamma$ to build an action of $S_{n}$ on $X^{\otimes n}$ (by Mac Lane's coherence theorem, the specifics of how we do this don't matter).
%
%  We define an \defn{antisymmetrization map} $A\colon X^{\otimes n} \to X^{\otimes n}$ by
%  \begin{equation*}
%    A_{n} = \frac{1}{n!}\sum_{\sigma \in S_{n}} \mathrm{sgn}(\sigma) \sigma.
%  \end{equation*}
%
%  The \defn{$n$th exterior power} of $X$, denoted $\bigwedge^{n}X$, is then defined to be the cokernel (\hyperref[def:cokernalofmorphism]{Definition \ref*{def:cokernalofmorphism}}) of $A_{n}$.
%\end{definition}
%
%However, this notion is not general enough to completely capture the dimension in a tensor category. Suppose that $\mathscr{A} = \mathsf{SFinVect}_{k}$, and $X$ is a super vector space concentrated in odd degree. Then the action of $\sigma$ on $X$ comes alrcady with a factor of $\mathrm{sgn}(\sigma)$ from the super braiding, and $A_{n}$ reduces to the symmetrization of $X^{\otimes n}$, which never vanishes. What is needed is an operation which is general enough to include both symmetrization and antisymmetrization.
%
%The correct generalization turns out to be that of a \emph{Schur functor}. But before we define Schur functors, we need to recall some representation theory.
%
%\subsection{Interlude: representations of the symmetric group}
%Much of this section is from \cite{jpmds-representation-theory-symmetric-groups}.
%
%Let $G$ be a group. We say that two elements $g$ and $g'$ are \emph{conjugate} if there exists $h \in G$ such that $g' = hgh^{-1}$. Conjugacy is an equivalence relation, and partitions a group into \emph{conjugacy classes}.
%
%The conjugacy classes of $S_{n}$ are in one-to-one correspondence with partitions of $n$, notated by so-called \emph{Young tableaux}. 
%
%\begin{definition}[partition]
%  \label{def:partition}
%  Denote by $\lambda$ is a sequence of integers $\lambda_{1} \geq \lambda_{2} \geq \cdots \geq \lambda_{s} > 0$. For simplicity, we will define $\lambda_{r} = 0$ for $r > s$. We denote
%  \begin{equation*}
%    \left|\lambda\right| = \sum_{i = 1}^{r} \lambda_{r},
%  \end{equation*}
%  and say that $\lambda$ is a \defn{partition of $\left|\lambda\right|$}.
%\end{definition}
%
%The importance of the conjugacy classes is that they correspond precisely to irreducible representations of $S_{n}$. Thus for any partition $\lambda$ of $n$ we have an irreducible representation $\rho_{\lambda}$ of $S_{n}$ on a vector space $V_{\lambda}$.
%
%\begin{definition}[Schur functor]
%  \label{def:schurfunctor}
%  Any
%\end{definition}
%
\chapter{Deligne's theorem on tensor categories}
\section{Statement}
Deligne's theorem tells us that every $k$-tensor category, for $k$ an algebraically closed field of characteristic zero, is the category of representations of some algebraic super group.

The following statement of Deligne's theorem was taken, mutatis mutandis, from \cite{nlab-deligne-theorem}.

\begin{theorem}[Deligne's theorem on tensor categories]
  \label{thm:delignestheorem}
  Let $\mathscr{A}$ be a $k$-tensor category (\hyperref[def:tensorcategory]{Definition \ref*{def:tensorcategory}}) such that
  \begin{enumerate}
    \item $k$ is an algebraically closed field of characteristic zero, and

    \item $\mathscr{A}$ is of subexponential growth (\hyperref[def:subexponentialgrowth]{Definition \ref*{def:subexponentialgrowth}}).
  \end{enumerate}

  Then $\mathscr{A}$ is a neutral super Tannakian category (REF), and there exists 
  \begin{enumerate}
    \item an affine algebraic supergroup $G$ (REF) whose algebra of functions $\mathscr{O}(G)$ is a finitely-generated $k$-algebra, and 

    \item a tensor equivalence of categories
      \begin{equation*}
        \mathscr{A} \simeq \mathsf{Rep}(G, \varepsilon)
      \end{equation*}
      between $\mathscr{A}$ and the category of representations of $G$ of finite-dimension, according to REF.
  \end{enumerate}
\end{theorem}
Deligne's proof of \hyperref[thm:delignestheorem]{Theorem \ref*{thm:delignestheorem}} (\cite{deligne-categories-tensorielle}) can be broken down into the following steps.

\begin{enumerate}
  \item \label{item:proofsketch1} First, he shows that in any $k$-tensor category $\mathscr{A}$, an object $X$ is of subexponential growth if and only if there is a Schur functor that annihilates it.

  \item \label{item:proofsketch2} Then, he shows that if every object of $\mathscr{A}$ is annihilated by some Schur functor, then $\mathscr{A}$ admits a super fiber functor over some over some supercommutative superalgebra $R$, so every object of $\mathscr{A}$ has underlying it a super vector space with some extra structure.

  \item \label{item:proofsketch3} Finally, he shows that every $k$-tensor category which admits such a fiber functor 
    \begin{equation*}
      \omega\colon \mathscr{A} \to \mathsf{SVect}_{k}
    \end{equation*}
    is equivalent to the category of representations of the automorphism supergroup of $\omega$.
\end{enumerate}

\section{What does it mean?}
The meaning of \hyperref[thm:delignestheorem]{Theorem \ref*{thm:delignestheorem}} is the following: given any tensor category $\mathscr{A}$ of subexponential growth, there exists an affine super group $G$ with some inner parity $\varepsilon$ such that $\mathscr{A}$ is categorically equivalent to $\mathsf{Rep}(G, \varepsilon)$, where the categorical equivalence is given by tensor functors (that is, braided monoidal functors).

\chapter{Relevance to physics}
\section{Symmetries in quantum mechanics}

%, but a summary of the aspects which will be essential in what follows is given for completeness.

%In this set-up of quantum mechanics, the space of states of a physical system are represented mathematically as normalized vectors $\psi$ belonging to some complex separable Hilbert space $\mathscr{H}$. Observable quantities (such as angular momentum) correspond to self-adjoint operators whose eigenvalues correspond to the possible values of a measurement of the corresponding observable. There is a distinguished operator $H$, the so-called \emph{Hamiltonian} operator, whose corresponding observable quantity is energy. The Hamiltonian defines how the state $\psi$ system evolves through time via the time-dependent Schr\"{o}dinger equation
%\begin{equation*}
%  i\tder{\psi}{t} = H\psi.
%\end{equation*}

Quantum mechanics is usually introduced 
\begin{itemize}
  \item in the non-relativistic arena, and

  \item in the Schr\"{o}dinger picture.
\end{itemize}
This can obscure the vital importance that symmetries play in quantum mechanics. In the next section, we give an example of the standard treatment of symmetry. 
\subsection{Symmetries in the Schr\"{o}dinger picture}

Consider the non-relativistic free particle in one dimension. For simplicity we set $\hbar = 1$ and consider our particle to have mass $1$. Our Hilbert space is $\mathscr{H} = L^{2}(\R)$, and the Hamiltonian is
\begin{equation*}
  H = -\frac{1}{2} \tder{^{2}}{x^{2}}.
\end{equation*}

According to the standard treatment (e.g. section 4.2 of \cite{binneyphysicsofqm}), an operator on $L^{2}(\R)$ is a symmetry if it commutes with the Hamiltonian. Taking this definition, the free particle propagating in one-dimensional has two important classes of symmetries.
\begin{enumerate}
  \item Translation symmetry, i.e. for each $a \in \R$ an operator 
    \begin{equation*}
      M_{a}\colon \psi(x) \mapsto \psi(x + a).
    \end{equation*}

  \item Parity symmetry, i.e. an operator 
    \begin{equation*}
      P\colon \psi(x) \mapsto \psi(-x).
    \end{equation*}
\end{enumerate}

These symmetries are both derived from the symmetries of the underlying space $\R$: the isometry group of $\R$ is generated by translations and parity transformations.

However, in a sense we are missing some symmetries. We are describing a non-relativistic quantum particle. Recall that even in non-relativistic mechanics one can introduce spacetime, and the symmetries of our theory should include those of the underlying spacetime. The symmetries of the non-relativistic $(1+1)$-dimensional spacetime underlying our example are given by the $(1+1)$-dimensional Galilean group. These include the translation and parity symmetries that we have seen, but also three more classes of symmetries:
\begin{enumerate}
  \item Galilean boosts, i.e. for each $v \in \R$ an operator
    \begin{equation*}
      B_{v}\colon \psi(x, t) \mapsto \psi(x + vt, t);
    \end{equation*}

  \item Time inversion, i.e. an operator
    \begin{equation*}
      T\colon \psi(x, t) \mapsto \psi(x, -t);
    \end{equation*}

  \item Time translation, i.e. for each $s \in \R$ an operator 
    \begin{equation*}
      U_{s}\colon \psi(x, t) \mapsto \psi(x, t + s).
    \end{equation*}
\end{enumerate}

Note that the first two of these are not symmetries in the traditional sense; Galilean boosts and time inversion do not commute with the Hamiltonian. However, these are certainly \emph{are} symmetries of non-relativistic mechanics, so they should be symmetries of non-relativistic quantum mechanics. The third is a symmetry in the traditional sense, but is not just any old symmetry; it is exactly the evolution defined by the Schr\"{o}dinger equation.

The fact that these symmetries do not manifest themselves in the traditional approach stems from a conceptually ugliness in the Schr\"{o}dinger picture: one is implicitly forced to choose a preferred frame of reference. This is convenient for calculations, but in general masks symmetries of the underlying theory which do not leave the frame fixed.

By considering the symmetries of spacetime to be fundamental, one avoids the conceptually displeasing step of choosing a reference frame. Moreover, with the above expanded notion of symmetry we do not need to take the Schr\"{o}dinger equation as an axiom: the existence of time evolution is taken care of by the time translation operators guaranteed us by postulating Galilean symmetry. We will see how this works in more detail in the next section.

\subsection{Symmetric quantum mechanics}
As we saw in the last section, the postulate that the state vector $\psi$ evolves via the Schr\"{o}dinger equation is superfluous if we use the correct notion of symmetry. This suggests that we can do away with the postulate that our system evolves in time via the Schr\"{o}dinger equation if we think about a quantum theory as being defined by its symmetries.

Thus, we should think of a quantum system as consisting of the following.
\begin{enumerate}
  \item A separable complex Hilbert space $\mathscr{H}$ in which our state vectors live.\footnote{Note that standard quantum mechanics, the Hilbert space is infinite-dimensional. In this case we do not even really have to think of the Hilbert space as included in the data, since there is only one infinite-dimensional separable Hilbert space up to isomorphism.}

  \item A collection of operators which implement symmetry transformations.

\end{enumerate}

We are being intentionally vague about what sort of operators are allowed, and what sort of symmetry transformations we want to include. This is a complicated question to which we hope to present a partial answer. However, the following example gives the most common way of specifying such a collection of symmetry transformations.

\begin{example}
  Suppose we decide that we want to study a quantum mechanical system which is invariant under some group $G$ of symmetries. Fix some state $\psi$. Each element $g \in G$ will map $\psi$ to some other state $\psi' = \hat{g}\psi$; in our example, the time translation $U_{s}$ maps $\psi(t)$ to $\psi(t + s)$. Furthermore, we want the states $\hat{g}\psi$ to obey the reasonable condition that 
  \begin{equation*}
    h(g\psi) = (hg)\psi.
  \end{equation*}
  This means that the action of $G$ on $\mathscr{H}$ is given by a representation 
  \begin{equation*}
    \rho\colon G \to \mathrm{Aut}(\mathscr{H});\qquad g \mapsto \hat{g}.
  \end{equation*}
  If one further assumes
  \begin{enumerate}
    \item that $G$ is a simply connected Lie group; and

    \item that the representation of $G$ is unitary (which is sensible because then its action will preserve probability) and strongly continuous (i.e. that $\lim_{g \to g_{0}}\hat{g} = \hat{g_{0}}$)
  \end{enumerate}
  then we can choose a representation of the Lie algebra $\mathfrak{g}$ of $G$ and express any $\hat{g}$ as the exponential of a Lie algebra element
  \begin{equation*}
    \hat{g} = e^{i X}
  \end{equation*}
  where $X$ is self-adjoint. 

  In particular, we can parametrize the operators representing any one-dimensional subgroup $\left\{ g(t) \right\}_{t \in \R}$ as
  \begin{equation*}
    \hat{g}(t) = e^{-iXt}.
  \end{equation*}
  In our example, $\hat{g}(t) = U(t)$, and we can write
  \begin{equation*}
    U(t) = e^{-i H t}
  \end{equation*}
  for some self-adjoint operator $H$. Letting this operator act on a state $\psi$ and differentiating with respect to $t$, we find
  \begin{equation*}
    i \tder{}{t} \psi = H \psi,
  \end{equation*}
  which is precisely the Schr\"{o}dinger equation.
\end{example}

In the preceeding section, we showed that expanding our definition of symmetry to allow not only operators which commute with the Hamiltonian. This allowed us to uncover hidden symmetries which did not leave our frame of reference fixed. However, we have have exchanged one conceptual difficulty for another: we no longer have any definition of what it means for an operator to be a symmetry. Before we had a concrete definition, if a defective one; now that we have demoted the Hamiltonian, we will need some way of deciding which operators we want to call symmetries.

We propose the following radical definition: the operators which represent symmetries are those which commute with all the other operators. Or rather, the only other operators one is allowed to consider are those that commute with the symmetry operators. Note that this is a restriction not of the symmetry operators themselves, but the other operators in the theory.

This is a rather draconian definition, but it has a reasonable interpretation: we wish to consider only quantum systems which are \emph{defined} by their symmetries. That is, we view our quantum system as completely determined by the symmetry transformations, and the only other operators we allow are those which do not interfere with them in precisely the sense that they commute. 

This excludes a great deal of physically interesting quantum mechanical systems from the discussion: for example, the only symmetries of the simple harmonic oscillator in one dimension are time translation and parity, so one would not be justified in talking about the position operator. In practice, when dealing with the harmonic oscillator, we borrow operators from the free theory which has full Galilean invariance. 

This provides another interpretation: we are investigating only the possible sorts of kinematics our quantum theories can exhibit, without restricting the kinetics. That is, we are looking at the possible symmetries that quantum spacetime can have without paying attention to interactions.

We can formalize this in a definition.

\begin{definition}
  \label{def:symmetricquantumsystem}
  For any set $\mathcal{S}$, an \defn{$\mathcal{S}$-symmetric quantum system} is a tuple $(\mathscr{H}, \varphi)$, where
  \begin{itemize}
    \item $\mathscr{H}$ is a separable Hilbert space, and 

    \item $\varphi\colon \mathcal{S} \to \mathcal{L}(\mathscr{H})$ maps the elements of $\mathcal{S}$ to linear operators on $\mathscr{H}$. The elements in the range of $\varphi$ are called the \emph{symmetry operators}.
  \end{itemize}
\end{definition}

So far we have made no mention of the form that the symmetry operators take: they are just the images of the elements of some set. Now we need to make some assumptions. 

\begin{itemize}
  \item First, we make a \emph{huge} simplifying assumption: we restrict our attention to finite-dimensional quantum systems, i.e. quantum systems whose underlying Hilbert space is finite-dimensional. 

  \item Next, we say that two systems have the same underlying symmetry if the symmetry operators are the images of the same set $\mathcal{S}$. 
    \begin{itemize}
      \item Since for any set $\mathcal{S}$ and every Hilbert space $\mathscr{H}$ there is a function which maps every element of $\mathcal{S}$ to $\mathrm{id}_{\mathscr{H}}$, we can view any Hilbert space as possessing any symmetry trivially.
    \end{itemize}

  \item We demand that given two $\mathcal{S}$-symmetric quantum systems $(\mathscr{H}, \varphi)$ and $(\mathscr{H}', \varphi')$, we define the $\mathcal{S}$-symmetric composite system $(\mathscr{H}'', \varphi'')$ as follows.
    \begin{itemize}
      \item As with regular old quantum mechanics, the Hilbert space $\mathscr{H}''$ underlying the composite system is given by the tensor product $\mathscr{H} \otimes \mathscr{H}'$ of the Hilbert spaces of the component systems. 

      \item The function $\varphi''\colon \mathcal{S} \to \mathscr{H}_{1} \otimes \mathscr{H}_{2}$ is given by
        \begin{equation*}
          \varphi''(s) = \varphi_{1}(s) \otimes \varphi_{2}(s).
        \end{equation*}
    \end{itemize}

  \item We can swap the order in which we consider taking the composite system, i.e. for any two  $\mathcal{S}$-symmetric quantum systems $(\mathscr{H}, \varphi)$ and $(\mathscr{H}', \varphi')$ there is a natural isomorphism 
    \begin{equation*}
      \gamma\colon \mathscr{H} \otimes \mathscr{H}' \to \mathscr{H}' \otimes \mathscr{H}.
    \end{equation*}
    We further demand that $\gamma$ preserve the symmetries in the sense that for any $s \in \mathcal{S}$,
    \begin{equation*}
      \gamma \circ (\varphi(s) \otimes \varphi'(s)) = (\varphi'(s) \otimes \varphi(s)) \circ \gamma.
    \end{equation*}

    Note that we do \emph{not} assume that this is given by the standard braiding of the tensor product which sends $v \otimes w \mapsto w \otimes v$; we allow for more general isomorphisms.

  \item However, we demand that if we swap our systems twice, we get back the system we started with.\footnote{It is not entirely clear that this assumption is justified; for example, 2-$d$ QFTs can exhibit braid group symmetry; see for example \cite{braidstatisticsinlocalqft}.}

  \item Lastly, for any $\mathcal{S}$-symmetric quantum system $\mathscr{H}$ with symmetry operators $\varphi(s)$, we define the dual space to be the space $(\mathscr{H}^{*}, \varphi^{*})$ with symmetry operators $\varphi(s)^{*}$.
\end{itemize}

The only other maps we consider on $\mathscr{H}$ are those which commute with the symmetries operators. That is, we allow only linear maps $L\colon \mathscr{H} \to \mathscr{H}$ such that 
\begin{equation*}
  \varphi(s)L = L \varphi(s)
\end{equation*}
for all $s \in \mathcal{S}$. 

For two $\mathcal{S}$-symmetric quantum systems $(\mathscr{H}, \varphi)$ and $(\mathscr{H}', \varphi')$ we also consider maps $L\colon \mathscr{H} \to \mathscr{H}'$ such that
\begin{equation*}
  L \varphi(s) = \varphi'(s) L
\end{equation*}
for all $s \in \mathcal{S}$. These have the following interpretation.

INTERTWINERS AS AN APPROXIMATION TO DYNAMICS.

\subsection{The category of $\mathcal{S}$-symmetric quantum systems}
In this section, we will study the mathematical structure of $\mathcal{S}$-symmetric quantum systems.

\begin{definition}
  \label{def:categoryofsymmetricquantumsystems}
  For any set $\mathcal{S}$, let $\mathsf{QSys}(\mathcal{S})$ be the category whose objects are finite-dimensional $\mathcal{S}$-symmetric quantum systems and whose morphisms are linear maps which commute with the symmetry operators in the sends that for two $\mathcal{S}$-symmetric quantum systems $(\mathscr{H}, \varphi)$ and $(\mathscr{H}', \varphi') \in \Obj(\mathsf{QSys}(\mathcal{S}))$, 
  \begin{equation*}
    \Hom_{\mathsf{QSys}(\mathcal{S})}(\mathscr{H}_{1}, \mathscr{H}_{2}) = \left\{ L\colon \mathscr{H}_{1} \to \mathscr{H}_{2}:\ L\varphi(s) = \varphi'(s)L\text{ for all }s \in \mathcal{S} \right\}.
  \end{equation*}

  The unit morphism is the identity map, and we can compose morphisms because for symmetric quantum systems and morphisms as follows
  \begin{equation*}
    \begin{tikzcd}
      (A, \varphi)
      \arrow[r, "L"]
      & (B, \varphi')
      \arrow[r, "L'"]
      & (C, \varphi'')
    \end{tikzcd}
  \end{equation*}
  and any $s \in \mathcal{S}$, we have 
  \begin{equation*}
    (\varphi(s) \circ L') \circ L = (L' \circ \varphi'(s)) \circ L = L' \circ (\varphi'(s) \circ L) = L' \circ (L \circ \varphi''(s)).
  \end{equation*}
\end{definition}

\begin{theorem}
  The category $\mathsf{QSys}(\mathcal{S})$ is a tensor category (\hyperref[def:tensorcategory]{Definition \ref*{def:tensorcategory}}) of subexponential growth. That is, it is an
  \begin{enumerate}
    \item essentially small (\hyperref[def:essentiallysmall]{Definition \ref*{def:essentiallysmall}})

    \item $k$-linear\footnote{Hence abelian.} (\hyperref[def:linearcategory]{Definition \ref*{def:linearcategory}})

    \item rigid (\hyperref[def:rigidmonoidalcategory]{Definition \ref*{def:rigidmonoidalcategory}})

    \item symmetric (\hyperref[def:symmetricmonoidalcategory]{Definition \ref*{def:symmetricmonoidalcategory}})

    \item monoidal category (\hyperref[def:monoidalcategory]{Definition \ref*{def:monoidalcategory}})
  \end{enumerate}
  such that
  \begin{enumerate}
      \setcounter{enumi}{5}
    \item the tensor product functor $\otimes\colon \mathscr{A} \times \mathscr{A} \rightsquigarrow \mathscr{A}$ is, in both arguments separately,
      \begin{enumerate}
        \item $k$-linear (\hyperref[def:linearfunctor]{Definition \ref*{def:linearfunctor}})

        \item exact (\hyperref[def:exactfunctor]{Definition \ref*{def:exactfunctor}})
      \end{enumerate}

    \item $\mathrm{End}(1) \simeq k$, where $\mathrm{End}$ denotes the endomorphism ring (\hyperref[def:endomorphismring]{Definition \ref*{def:endomorphismring}}).
  \end{enumerate}
\end{theorem}
\begin{proof}
  We check each condition separately.
  \begin{enumerate}
    \item 

    \item 
      \begin{itemize}
        \item For any $L$, $M\colon (\mathscr{H}, \varphi) \to (\mathscr{H}', \varphi')$, 
          \begin{equation*}
            (L+M)\varphi = L\varphi + M\varphi = \varphi'L + \varphi'M = \varphi'(L+M)
          \end{equation*}
          and
          \begin{equation*}
            (\alpha L)\varphi = \alpha (L\varphi) \alpha(\varphi'L) = \varphi' (\alpha L),
          \end{equation*}
          so $\mathsf{QSys}(\mathcal{S})$ is enriched over abelian groups with a compatible $k$-linear structure.

        \item The biproduct of $\mathcal{S}$-symmetric quantum spaces is given by the direct sum of vector spaces:
          \begin{equation*}
            (\mathscr{H}_{1}, \varphi_{1}) \amalg (\mathscr{H}_{2}, \varphi_{2}) = (\mathscr{H}_{1} \oplus \mathscr{H}_{2}, \varphi_{1} \oplus \varphi_{2}),
          \end{equation*}
          where $\varphi_{1} \oplus \varphi_{2}$ is defined by
          \begin{equation*}
            (\varphi_{1} \oplus \varphi_{2})(s) = \varphi_{1}(s) \oplus \varphi_{2}(s).
          \end{equation*}

          The zero object is $(0, \varphi)$, where $0$ is the zero $\C$-vector space.

        \item The kernel of a map $L\colon (\mathscr{H}_{1}, \varphi_{1}) \to (\mathscr{H}_{2}, \varphi_{2})$ is a subspace $\ker(L) \subset\mathscr{H}_{1}$. Each $\varphi(s)$ fixes the kernel since for any $v \in \ker(L)$
          \begin{equation*}
            L \varphi_{1}(s) v = \varphi_{2}(s) L v = 0,
          \end{equation*}
          so $\varphi_{1}(s) v$ is also in $\ker(L)$. Thus for each $s \in \mathcal{S}$, the map $\varphi(s)$ restricts to a map $\ker(L) \to \ker(L)$, and $(\ker(L), \varphi_{1}|_{\ker(L)})$ is an $\mathcal{S}$-symmetric quantum space.

          Similarly, if $w \in \im(L)$, then there exists $v \in \mathscr{H}_{1}$ such that $w = Lv$. But then 
          \begin{equation*}
            \varphi_{2}(s)w = \varphi_{2}(s) Lv = L(\varphi_{1}(s) v),
          \end{equation*}
          so $\varphi_{2}(s) v \in \im(L)$. Thus, $\varphi_{2}$ fixes $\im(L)$.
      \end{itemize}

    \item For any $\mathcal{S}$-symmetric quantum space $(\mathscr{H}, \varphi)$, the dual object is the 

    \item[4-5.] The bifunctor is given by the creation of composite systems:
      \begin{equation*}
        (\mathscr{H}_{1}, \mathscr{H}_{2}) \mapsto \mathscr{H}_{1} \otimes \mathscr{H}_{2}.
      \end{equation*}
      Its functoriality follows from that of the monoidal structure on $\mathsf{FinVect}_{\C}$.

      The associator is also inherited from the tensor product on $\mathsf{FinVect}_{\C}$. The unit element is the trivial quantum system $(\C, \varphi)$, where $\varphi(s) = \mathrm{id}_{\C}$ for all $s$.

      It is symmetric by assumption.

      \setcounter{enumi}{5}
    \item 
      \begin{enumerate}
        \item The linearity of the tensor product follows from that on $\mathsf{FinVect}_{k}$.

        \item The exactness of the tensor product follows from that on $\mathsf{FinVect}_{k}$.
      \end{enumerate}

    \item Since the tensor unit $1$ is the $\mathcal{S}$-symmetric space $(\C, s \mapsto \mathrm{id}_{\C})$, $\End_{\mathsf{QSys}(\mathcal{S})}(1) = \End_{\mathsf{FinVect}_{\C}}(\C) \simeq \C$.
  \end{enumerate}
\end{proof}

Under these conditions, we have constructed a category whose objects are symmetric quantum spaces and whose morphisms are intertwiners, and shown that it is a tensor category. Deligne's theorem tells us that this category is equivalent to the category $\mathsf{Rep}(G)$ for some affine algebraic super group $G$. That is, 
\begin{equation*}
  \mathsf{QSys}(\mathcal{S}) \simeq \mathsf{Rep}(G)\qquad\text{for some affine super group }G.
\end{equation*}

But what does this mean? It means we have a fully faithful, essentially surjective functor 
\begin{equation*}
  \mathcal{J}\colon \mathsf{Rep}(G) \to \mathsf{QSys}(\mathcal{S}).
\end{equation*}


%\section{Practicalities of supergeometry}
%\subsection{The super Lie algebra of an algebraic super-group}
%
%\subsection{Super Harish-Chandra pairs}
%
%
%\section{Wigner's classification of fundamental particle species}
%This section follows \cite{haag-local-quantum-physics} closely. Some stuff was taken from \cite{sexl-urbantke-relativity-groups-particles}.
%
%The axioms of quantum mechanics tell us that the states of a quantum system correspond to vectors $\psi$ belonging to some separable Hilbert space $\mathscr{H}$. However, this is not the end of the story. The observable quantities built from the states, i.e. the conditional probabilities
%\begin{equation*}
%  P(\varphi | \psi) = \frac{\left|\left\langle \varphi | \psi \right\rangle\right|^{2}}{\norm{\varphi}^{2} \cdot \norm{\psi}^{2}}
%\end{equation*}
%are invariant under a rescaling of either $\varphi$ or $\psi$ by any nonzero complex number: that is,
%\begin{equation*}
%  \psi \sim \psi' \qquad\text{if}\qquad \psi' = \lambda \psi\qquad\text{for some }\lambda \in \C\setminus\left\{ 0 \right\},
%\end{equation*}
%where $\sim$ should be read as `is physically indistinguishable from.'
%
%One often `fixes' this problem by working only with normalized state vectors, but this does not completely solve the problem. Instead we should think about two states which differ only by a nonzero complex factor as being physically equivalent. That is, the true space of states of our quantum system is not $\mathscr{H}$, but $\mathscr{H} / \sim$, \emph{projective Hilbert space}. Physically inequivalent quantum states are in one-to-one correspondence to the equivalence classes in $\mathscr{H}/\sim$.
%
%Now suppose we want our quantum theory to have Poincar{\'e} symmetry. For us, the Poincar{\'e} group will mean the proper orthochromous Poincar{\'e} group. Its elements are ordered pairs $(a, \Lambda)$, with $a \in \R^{4}$ and $\Lambda \in \mathcal{L}^{\uparrow}_{+}$.
%
%Pick any ray $[\psi_{0}] \in \mathscr{H}/\sim$. For any Poincar{\'e} transformation $g \in \mathcal{P}$, there should be another state (i.e. ray) $[\psi]_{g}$, which corresponds to the result of acting on the system with $g$. Furthermore, we want the result of acting on $[\psi_{0}]$ with a sequence of Poincar{\'e} transformations to be the same no matter how we imagine them to be bracketed. Mathematically, this means that if we are to consider a quantum theory which is Poincar{\'e}-invariant, we want a Hilbert space $\mathscr{H}$ which admits an injective homomorphism
%\begin{equation*}
%  \rho\colon \R^{4} \rtimes \mathcal{L}^{\uparrow}_{+} \hookrightarrow \mathrm{Aut}(\mathscr{H}/\sim).
%\end{equation*}
%
%Of course, having such a $\rho$ is nice, but quantum mechanics does not deal with projective spaces and ray transformations. If we are to use these results as-is, we need to translate them to results about $\mathscr{H}$ itself.
%
%Pick one specific element $g \in \mathcal{P}$. To this element there is associated a bijective ray map on $\mathscr{H}$. Obviously, we can mimic this bijective ray map with a function $\bar{U}\colon \mathscr{H} \to \mathscr{H}$; we need only ensure that the vectors making up individual rays are mapped to the correct target rays. Equally obviously however, this does not specify $\bar{U}(g)$ uniquely; there are many possible ways of mixing the vectors composing the rays. We have some freedom to choose this mixing in as nice as possible a way.
%
%By a theorem of Wigner, we can get away with demanding that $\bar{U}(g)$ be additive, length-preserving, and either
%\begin{itemize}
%  \item linear and unitary or
%  \item antilinear and antiunitary.
%\end{itemize}
%In doing so, we specify $\bar{U}(g)$ up to an arbitrary phase.
%
%But we have many $\bar{U}(g)$---one for each Poincar{\'e} transformation. The arbitrarity in the phases translates to a change in representation law for multiplication of the $\bar{U}(g)$; we now have 
%\begin{equation*}
%  \bar{U}(g) \bar{U}(g') = \eta(g, g')\bar{U}(gg')
%\end{equation*}
%where the phases $\eta$ are arbitrary.
%
%Recall that we have restricted our attention to the component of $\mathcal{P}$ connected to the identity, the \emph{proper orthochronous Poincar{\'e} group} $\mathcal{P}_{+}^{\uparrow}$. In this setting, we can use the fact that on a connected Lie group, every element has a square root, so
%\begin{equation*}
%  \bar{U}(g) = \eta(g',g')(\bar{U}(g'))^{2}\qquad\text{for some }g' \in \mathcal{P}.
%\end{equation*}
%But even if $\bar{U}(g')$ is antilinear and antiunitary, its square will be linear and unitary. Since this is true for any $g$, we must have that for all $g \in \mathcal{P}_{+}^{\uparrow}$, $\bar{U}(g)$ is linear and unitary.
%
%In fact, we can do even more. By another theorem of Wigner, we can use the phase factor $\eta$ to replace our unitary ray representation of $\mathcal{P}_{+}^{\uparrow}$ by a bona fide unitary representation of the universal covering group $\overline{\mathcal{P}_{+}^{\uparrow}}$. To save on symbols, we'll call this $\widetilde{\mathcal{P}}$.
%
%The moral of the story so far: the Hilbert space of any quantum mechanical theory which is invariant under the proper orthochronous Poincar{\'e} group must admit a representation of the covering group of the proper orthochronous Poincar{\'e} group, $\widetilde{\mathcal{P}}$.
%
%Denote the representation of $(a, \alpha)$ by $U(a, \alpha)$.
%
%Consider some vector in $\R^{4}$, say
%\begin{equation*}
%  \hat{e}_{0} = (1,0,0,0).
%\end{equation*}
%The one-dimensional subspace of $\R^{4}$ generated by $\hat{e}_{0}$ is a one-dimensional Lie subgroup of $\widetilde{\mathcal{P}}$, so by Stone's theorem, we can write its action on $\mathscr{H}$ as follows:
%\begin{equation*}
%  U(\hat{e}_{0} t, I) = e^{i P_{0} t},\qquad P_{0} = -i \tder{}{t}\bigg|_{t=0} U(\hat{e}_{0} t, I).
%\end{equation*}
%For each of the other canonical basis vectors $\hat{e}_{i}$ we get similar $P_{i}$. Since $\R^4$ is abelian, these commute, so by the BCH formula,
%\begin{equation*}
%  U(a^{\mu} \hat{e}_{\mu}, I) = e^{i P_{\mu} a^{\mu}}.
%\end{equation*}
%
%Each of the $P_{\mu}$ are Hermitian, and they commute. Thus, they have a common eigenbasis. A priori, all we know is that their spectrum must inhabit some subspace $S \subseteq \R^{4}$. For each spectral component $p_{\mu} \in S$, we have an associated degeneracy space $\mathscr{H}_{p} \subseteq \mathscr{H}$, and if we wave our hands a bit, we can decompose $\mathscr{H}$ into these subspaces.
%\begin{equation*}
%  \mathscr{H} = \bigoplus_{p \in S} \mathscr{H}_{p}
%\end{equation*}
%That is to say, for any $\psi \in \mathscr{H}_{p}$, we have $P_{\mu}\psi = p_{\mu}\psi$.
%
%Under the action of $\alpha \in \SL(2, \C)$, we have
%\begin{multline*}
%  P_\mu \mapsto P_{\mu}' = U^{-1}(\alpha,0) P_\mu\, U(\alpha,0) = U^{-1}(\alpha,0)\left[-i\tder{}{t}\biggr|_{t=0} U(I, \hat{e}_\mu t)\right]U(\alpha,0)\\ = -i\tder{}{t}\biggr|_{t=0}U^{-1}(\alpha,0) U(I, \hat{e}_\mu t)U(\alpha,0) = -i\tder{}{t}\biggr|_{t=0}U\left(I,\Lambda^{-1}(\alpha)\hat{e}_\mu t\right) = \Lambda^{-1}(\alpha)^\nu_\mu P_\nu.
%\end{multline*}
%
%Thus, if $p_{\mu}$ is in $S$, so must be the entire orbit of $p_{\mu}$ under the Lorentz group, i.e. the invariant hyperboloid on which $p_{\mu}$ lives. Furthermore, these orbits are never mixed by the action of $\widetilde{\mathcal{P}}$.
%
%Perhaps now is a good time to make the following remark. Henceforth, everything we have said applies to \emph{any} quantum system which admits a Poincar{\'e} symmetry. As is well know, if we wish to view two quantum systems with Hilbert spaces $\mathscr{H}_{1}$ and $\mathscr{H}_{2}$ as a single composite system, the Hilbert space we consider is $\mathscr{H}_{1} \otimes \mathscr{H}_{2}$. That is to say, composite systems are the tensor product of their constituents. And the composite system's representation of $\widetilde{\mathcal{P}}$ is given by the tensor product of the representations of its constituents.
%
%This would seem to imply that given a Poincar{\'e}-invariant quantum system and its associated representation of $\widetilde{\mathcal{P}}$, we could break it and its representation down into smaller and smaller subsystems, until we hit `atomic' subsystems whose representations were irreducible, i.e. could not be broken down any further. The resulting atomic representations would be the building blocks of our original system. 
%
%But we can pull this trick with \emph{any} system! This means that cataloging the irreducible representations of $\widetilde{\mathcal{P}}$ will give us a list of the building blocks of any Poincar{\'e}-invariant system---the fundamental particles.
%
%Now let us concentrate on our previous analysis again. Recall: the orbit of any point $p_{\mu}$ in the spectrum of $P_{\mu}$ is the invariant hyperboloid on which $p_{\mu}$ lives. Different hyperboloids are not mixed by $U(a, \alpha)$. Thus, the spectrum of an \emph{irreducible representation} must be concentrated on one hyperboloid, or else the Hilbert space could be decomposed into invariant subspaces. This gives us our first breakdown of the irreducible representations of $\widetilde{\mathcal{P}}$. The following table was taken, mutatis mutandis, from \cite{haag-local-quantum-physics}.
%
%\begin{center}
%  \begin{tabular}{l l l l l}
%    class & orbit &  \\
%    \hline
%    $m_{+}$ & Hyperboloid in forward cone; & $p^{2} = m^{2}$ & and & $p_{0} > 0$. \\
%    $0_{+}$ & Surface of forward cone; & $p^{2} = 0$ & and & $p_{0} \geq 0$. \\
%    $0_{0}$ & The single point $p_{\mu} = 0$. \\
%    $\kappa$ & Space-like hyperboloid; & $p^{2} = -\kappa^{2}$ & ($\kappa$ real). \\
%    $m_{-}$ & Hyperboloid in backward cone; & $p^{2} = m^{2}$ & and & $p_{0} < 0$. \\
%    $0_{-}$ & Surface of backward cone; & $p^{2} = 0$ & and & $p_{0} \leq 0$. \\
%    \hline
%  \end{tabular}
%\end{center}
%Only the first two of these are physically interesting.
%
%\subsubsection{Case 1: positive mass}
%Let us focus first on the case $m_{+}$.
%
%If we can understand how $U$ acts on each of the $\mathscr{H}_{p}$, and we understand how it maps the $\mathscr{H}_{p}$ amongst each other, we understand everything about it. Pick some $\bar{p}$, and for every other $p$ on its orbit, find some $\beta(\alpha) \in \SL(2, \C)$ such that $\Lambda(\beta(\alpha))\bar{p} = p$. If we pick a basis in $\mathscr{H}_{\bar{p}}$, we can use $U(\beta(\alpha))$ to map it to a basis for $\mathscr{H}_{p}$; in this way we arrive at a basis for our entire Hilbert space 
%\begin{equation*}
%  \mathscr{H} = L^{2}(\R^{3}) \otimes \mathscr{H}_{\bar{p}}.
%\end{equation*}
%
%Now here is the claim: we can understand the action of \emph{any} $\alpha \in \bar{\mathcal{L}}$ if we understand its action on $\mathscr{H}_{\bar{p}}$. For any $\alpha \in \bar{\mathcal{L}}$, we can make the decomposition 
%\begin{equation*}
%  \alpha = \beta(\Lambda(\alpha)p) \gamma(\alpha, p) \beta^{-1}(p):
%\end{equation*}
%we map $\mathscr{H}_{p}$ to our proto-little-Hilbert-space $\mathscr{H}_{\bar{p}}$, act on it with some $\gamma \in \bar{\mathcal{L}}$ which fixes $\mathscr{H}_{\bar{p}}$, then map it back to where it needs to be. Since we can pull this trick for any $\alpha$ and $p$, to get an irreducible representation on $\mathscr{H}$, we only need an irreducible representation of the stabilizer of $\bar{p}$ in $\mathscr{H}_{\bar{p}}$. 
%
%There is a particularly easy choice: we can choose 
%\begin{equation*}
%  \bar{p}_{\mu} = (m,0,0,0).
%\end{equation*}
%With this choice, it becomes manifest that the subgroup of $\mathcal{P}$ which leaves $\bar{p}_{\mu}$ invariant is exactly the covering group of $\SO(3)$, i.e. $\mathrm{Spin}(3) = \SU(2)$.
%
%Thus, to obtain an irreducible representation of $\widetilde{\mathcal{P}}$ on $\mathscr{H}$, we need only find an irreducible representation of $\SU(2)$ on $\mathscr{H}_{\bar{p}_{\mu}}$. These are classified by half-integers.
%
%\subsubsection{Case 2: zero mass}
%Next, let us focus on the class $0_{+}$. In this case, we do not have the `canonical' choice of $\bar{p}_{\mu} = (m,0,0,0)$. We can make a different choice, however:
%\begin{equation*}
%  \bar{p}_{\mu} = (1/2,0,0,1/2).
%\end{equation*}
%
%As far as the action of $\widetilde{\mathcal{P}}$ is concerned, this vector is really the matrix
%\begin{equation*}
%  \hat{p} = 
%  \begin{pmatrix}
%    1 & 0 \\
%    0 & 0
%  \end{pmatrix}
%\end{equation*}
%via the correspondence between $2 \times 2$ matrices $\hat{V}$ and vectors $V^{\mu} \in \R^{4}$
%\begin{equation*}
%  \hat{V} = V^{\mu}\sigma_{\mu};\qquad V^{\mu} = \frac{1}{2} \mathrm{tr}(\hat{V} \sigma_{\mu}).
%\end{equation*}
%
%The action of $\SL(2, \C)$ is via conjugation:
%\begin{equation*}
%  \hat{V} \mapsto \alpha \hat{V} \alpha^{\dagger}
%\end{equation*}
%
%The subgroup of $\SL(2, \C)$ which fixes $\hat{p}$ is generated by the following subgroups of $\SL(2, \C)$:
%\begin{equation*}
%  \gamma_{\varphi} = 
%  \begin{pmatrix}
%    e^{i \varphi} & 0 \\
%    0 & e^{-i \varphi}
%  \end{pmatrix}
%  \qquad\text{and}\qquad \gamma_{\eta} = 
%  \begin{pmatrix}
%    1 & \eta \\
%    0 & 1
%  \end{pmatrix},
%\end{equation*}
%where $\varphi \in [0, 2\pi)$ and $\eta \in \C$.
%
%The non-trivial representations of the group $\gamma_{\eta}$ are infinite-dimensional, and appear not to be realized in experiments. The irreducible representations of $\gamma_{\varphi}$ are all one-dimensional, and are classified by integers:
%\begin{equation*}
%  U(\gamma_{\varphi}) = e^{i n \varphi},\qquad n \in \Z.
%\end{equation*}
%
%Since
%\begin{equation*}
%  \Lambda(\gamma_{\varphi}) = 
%  \begin{pmatrix}
%    1 & 0 & 0 & 0 \\
%    0 & \cos(2\varphi) & -\sin(2\varphi) & 0 \\
%    0 & \sin(2\varphi) & \cos(2\varphi) & 0 \\
%    0 & 0 & 0 & 1
%  \end{pmatrix},
%\end{equation*}
%the matrix representing rotation by $2\varphi$ in the $x^{1}$-$x^{2}$ plane, it makes sense to interpret $n/2$ as angular momentum along the axis of $\bar{p}_{\mu}$, i.e. the \emph{helicity}.
%
%
%\subsection{A caveat or two}
%This is mostly from \cite{neumaier}
%
%The eagle-eyed reader has probably noticed that not all fundamental particles fit into this classification scheme: both quarks and neutrinos can exist in a superposition of states of different mass, so they cannot be classified by mass-squared. This is because quarks and neutrinos are not irreducible representations of the Poincar\'{e} group, but rather of the full symmetry group
%\begin{equation*}
%  G = \overbrace{\mathrm{ISO}(1, 3)}^{\text{Poincar\'{e}}} \times \overbrace{\mathrm{SU}(3) \times \mathrm{SU}(2) \times \mathrm{U}(1)}^{\text{gauge}} \times \mathrm{U}(3),
%\end{equation*}
%where the final $\mathrm{U}(3)$ factor interchanges the three generations of particles which couple to the weak interaction.
%
%Thus it is not exactly correct to say that fundamental particles are exactly irreducible representations of the symmetry group of spacetime: the more correct statement is that particles are irreducible representations of the total symmetry group. Of course, for a given particle species, it may be that the representation of the full group decomposes into a direct sum of representations of each individual part, in which case one can consider the representation of the Poincar\'{e} group separately. Then one recovers the classification by mass-squared and spin/helicity.
%
%\section{Feynman diagrams: representations and intertwiners}
%The ideas in this section were taken from \cite{baez-lauda-prehistory}.
%
%In high-energy physics, one is generally interested in computing quantities known as scattering amplitudes. One imagines a collection of particles headed for each other, some time before a collision, and asks the question: long after the collision has occured, when the particles created in the collision are flying away from each other, what is the probability that one has ended up with some given collection of particles? 
%
%For any interesting or physically realistic theory (i.e. one with interactions), scattering amplitudes are fiendishly difficult to compute; in general one must resort to expressing them as an asymptotic series. Physicists have come up with a lot of tricks to make computing these more routine, such as representing the terms diagrammatically as \emph{Feynman diagrams}.
%
%Here's how this works. Let's imagine for simplicity that we want to compute a particularly simple scattering amplitude: an electron and a photon go in, an electron goes out. Or to put it another way, an electron absorbs a photon.
%
%The incoming lines (from the top) represent the electron and the photon; the squiggly one is the photon and the straight one is the electron. The 
%
%
%\section{Why SUSY? The standard explanation}
%There are many standard justifications given for the existence of supersymmetry. 
%
%\begin{itemize}
%  \item The combination of the Coleman-Mandula and the Haag-{\L}opusza{\'n}ski-Sohnius theorem.
%
%  \item The low mass of the Higgs.
%\end{itemize}
%
%Roughly speaking, the Coleman-Mandula theorem says that the only possible connected Lie groups which give the symmetries of the $S$-matrix of a four-dimensional relativistic quantum field theory are a direct product of the Poincar{\'e} group and an internal symmetry group. 
%
%The Haag-{\L}opusza{\'n}ski-Sohnius theorem generalizes the assumptions of this theorem by expanding the definition of `symmetry' to include super Lie groups. The Coleman-Mandula theorem still applies to the even part; however, one finds that the odd-graded elements are allowed to mix internal and spacetime symmetries in a non-trivial way. In particular, one has the following schematic multiplication laws:
%\begin{equation*}
%  \left\{ \text{odd}, \text{odd} \right\} = \text{even};\qquad \left[ \text{even}, \text{even} \right] = \text{even};\qquad \left[ \text{odd}, \text{even} \right] = \text{odd},
%\end{equation*}
%where the even part is the direct product guaranteed by the Coleman-Mandula theorem, and the odd part is the super- component of the super-Poincar{\'e} algebra.
%
%We will prove neither the Coleman-Mandula nor the Haag-{\L}opusza{\'n}ski-Sohnius theorem. We will, however, give their statements.
%
%The statement of the Coleman-Mandula theorem, taken from \cite{muller-kirsen-wiedmann-intro-susy} is as follows.
%
%\begin{theorem}[Coleman-Mandula]
%  Let $G$ be a connected symmetry group of the $S$-matrix, i.e. a group whose generators commute with the $S$-matrix, and make the following five assumptions.
%  \begin{enumerate}
%    \item \emph{Lorentz-invariance:} $G$ contains a subgroup which is locally isomorphic to the Poincar{\'e} group.
%
%    \item \emph{Particle finiteness:} All particles types correspond to positive-energy representations of the Poincar{\'e} group. For any finite mass $M$, there is only a finite number of particles with mass less than $M$.
%
%    \item \emph{Weak elastic analyticity:} Elastic scattering amplitudes are analytic functions of center-of-mass energy squared $s$ and invariant momentum transfer squared $t$ in some neighbourhood of the physical region, except at normal thresholds.
%
%    \item \emph{Occurence of scattering:} Let $\ket{p}$ and $\ket{p'}$ be any two one-particle momentum eigenstates, and let $\ket{p, p'}$ be the two-particle state constructed from these. Then
%      \begin{equation*}
%        T\ket{p, p'} \neq 0
%      \end{equation*}
%      where $T$ is the $T$-matrix defined by
%      \begin{equation*}
%        S = \mathbf{1} = i (2\pi)^{4} \delta^{4}(p_{\mu} - p'_{\mu}) T,
%      \end{equation*}
%      except, perhaps, for certain values of $S$. In simpler terms this assumption means: Two plane waves scatter at almost any energy.
%
%    \item \emph{Technical assumption:} The generators of $G$, considered as integral operators in momentum space, have distributions for their kernels.
%  \end{enumerate}
%
%  Then the group $G$ is locally isomorphic to the direct product of a compact symmetry group and the Poincar{\'e} group.
%\end{theorem}
%
%The Haag-{\L}opusza{\'n}ski-Sohnius theorem extends the above theorem by allowing $G$ to be a super group. In the end, one finds that the Poincar{\'e} algebra can be extended to the more general \emph{super-Poincar{\'e} algebra}.
%
%If one is to use Haag-{\L}opusza{\'n}ski-Sohnius theorem to motivate supersymmetry, one must navigate a minefield of caveats and grains of salt. In order to use either theorem at all, one must conform to the assumptions in the statement of the Coleman-Mandula theorem, which are very restrictive. To view it as a motivation for supersymmetry, one must believe the following.
%\begin{enumerate}
%  \item Spacetime is 4-dimensional, and its symmetry group is the Poincar{\'e} group.
%
%  \item Spacetime and internal symmetries ought to be unified.
%
%  \item There are no more loopholes that no one has thought of yet.
%\end{enumerate}
%
%The other reason is that the mass of the Higgs boson is lower than one would expect. The Higgs 
%
%\section{What does Deligne's theorem do for us?}
%The propose of these notes is to present some physical applications of Deligne's theorem. First, we should understand exactly what it is that Deligne's theorem tells us.
%
%The following statement of Deligne's theorem was taken, mutatis mutandis, from \cite{nlab-deligne-theorem}. For us, $k = \C$.
%
%\renewcommand{\thetheorem}{\ref*{thm:delignestheorem}}
%\begin{theorem}[Deligne's theorem on tensor categories]
%  Let $\mathscr{A}$ be a $k$-tensor category such that
%  \begin{enumerate}
%    \item $k$ is an algebraically closed field of characteristic zero, and
%
%    \item $\mathscr{A}$ is of subexponential growth.
%  \end{enumerate}
%
%  Then $\mathscr{A}$ is a neutral super Tannakian category, and there exists 
%  \begin{enumerate}
%    \item an affine algebraic supergroup $G$ whose algebra of functions $\mathscr{O}(G)$ is a finitely-generated $k$-algebra, and 
%
%    \item a tensor equivalence of categories
%      \begin{equation*}
%        \mathscr{A} \simeq \mathsf{Rep}(G, \varepsilon)
%      \end{equation*}
%      between $\mathscr{A}$ and the category of representations of $G$ of finite-dimension.
%  \end{enumerate}
%\end{theorem}
%\renewcommand{\thetheorem}{\arabic{theorem}}
%
%Our first task will be to understand the statement of Deligne's theorem. Given time constraints, we will only be able to give a rough outline.
%
%First, we need the notion of a \emph{category}. Categories formalize and generalize the relationship between sets and functions between them.
%
%A category $\mathsf{C}$ consists of a collection of objects $\Obj(\mathsf{C})$ (think of sets) and for every two objects $A$, $B \in \Obj(\mathsf{C})$ a collection of morphisms $A \to B$ (think of functions). Categories also have some extra structure in the form of additional axioms to make objects and morphisms mimic sets. 
%
%The best thing I can do to give a sense of what categories are is to give some examples. Apart from the category $\mathsf{Set}$ of sets, there are many other categories:
%\begin{itemize}
%  \item The category $\mathsf{Top}$, whose objects are topological spaces and whose morphisms are continuous maps between them.
%
%  \item The category $\mathsf{Grp}$, whose objects are groups and whose morphisms are homomorphisms between them.
%
%  \item The category $\mathsf{FinVect}_{k}$, whose objects are finite-dimensional $k$-vector spaces and whose morphisms are $k$-linear maps between them.
%\end{itemize}
%
%Just as categories provide a formalism to talk about things which behave like sets and functions between them with, $k$-tensor categories are special type of category which provide a means of talking about things which behave like finite-dimensional vector spaces and linear maps between them. The prototypical example is the category $\mathsf{FinVect}_{k}$ of finite-dimensional $k$-vector spaces.
%
%Unlike the definition of a category, the definition of a tensor category is so technical that there is no hope of giving any real sense of what they are. They are categories with the following properties, as well as many others.
%\begin{itemize}
%  \item For any two objects $V_{1}$, $V_{2}$ in a $k$-tensor category, one can form the \emph{`tensor product'}
%    \begin{equation*}
%      V_{1} \otimes V_{2}
%    \end{equation*}
%
%  \item For each object $V$ there is a dual object $V^{*}$ which behaves like the dual space.
%
%  \item And \emph{many} more. 
%\end{itemize}
%
%Deligne's theorem isn't about just any tensor categories though, it's about those which satisfy a special requirement: \emph{subexponential growth}. This is a technical condition which makes sure that the objects in our tensor category have an attribute which behaves roughly like a dimension.
%
%Again, $\mathsf{FinVect}_{k}$ is the prototypical example of a tensor category with subexponential growth. However, it's far from the \emph{only} one. For example, for any group $G$ there is a category $\mathsf{FinRep}_{k}(G)$ whose objects are finite-dimensional linear representations of $G$ and whose morphisms are $G$-equivariant linear maps called \emph{intertwiners}. With the tensor product given by the tensor product of representations, these form tensor categories with subexponential growth.
%
%In fact, Deligne's theorem tells us that these are nearly all the tensor categories with subexponential growth. More specifically, it says that that any tensor category which satisfies the condition of subexponential growth arises as (in jargon, there exists a tensor-equivalence of categories between) the category of finite-dimensional representations of an algebraic super-group.
%
%But what is an algebraic super-group? Again, they have a horribly technical definition: they are group objects in the category of affine super-schemes; affine super-schemes in turn are the formal duals of commutative monoids internal to $\mathsf{FinSVect}_{k}$. For the purposes of this talk, the best definition I can give is: They are exactly the sort of groups that physicists like to study. 
%
%Admittedly, this is a very loaded definition. Here are some examples. 
%\begin{itemize}
%  \item Any matrix group is an algebraic super-group.
%    \begin{itemize}
%      \item Hence, the Lorentz group is an algebraic supergroup
%    \end{itemize}
%
%  \item For any $n$, the $n$-dimensional translation group is an algebraic super-group.
%    \begin{itemize}
%      \item Hence, the Poincar\'{e} group is an algebraic super-group.
%    \end{itemize}
%\end{itemize}
%In fact, these are simply algebraic groups, which are a special case of algebraic super-groups. 
%\begin{itemize}
%  \item The super-* examples of all of these are also algebraic super-groups.
%    \begin{itemize}
%      \item For example, the Super Poincar\'{e} group is an algebraic super-group.
%    \end{itemize}
%\end{itemize}
%
%This is where physics enters the story.
%
%In quantum mechanics, a finite-dimensional quantum system is $G$-symmetric if it admits a unitary representation of $G$. This means that finite-dimensional $G$-symmetric quantum systems live in the category $\mathsf{FinURep}_{\C}(G)$.
%
%This is a monoidal subcategory of $\mathsf{FinRep}_{\C}(G)$, (and even a full tensor subcategory if we make a slightly unnatural definition of morphisms). Therefore, Deligne's theorem tells us that the most general sort of symmetry that a finite-dimensional quantum system can have is that given by an algebraic super-group.
%
%This is nice, but few interesting quantum systems are finite-dimensional; Deligne's theorem cannot be applied to infinite-dimensional quantum systems without some finesse.
%
%We need a few physical assumptions. It is clear that some of these assumptions can be weakened, but not always obvious what is the best weakening. This is an interesting problem, but I will not have time to discuss it.
%
%Firstly, we will model spacetime with Minkowski space. The so-called \emph{little group} analysis, originally due to Wigner \cite{wigner-little-group} and generalized by Mackey \cite{mackey-induced-representations}, tells us that we can therefore restrict our attention to symmetry groups of the form 
%\begin{equation*}
%  \R^{4} \rtimes \mathcal{G}.
%\end{equation*}
%Wigner's analysis tells us that we can separate off $\mathcal{G}$ and interpret it as the symmetry group of the internal Hilbert space $\mathfrak{h}$ of the our system.
%
%This is not a terribly restrictive assumption, and it has an attractive physical interpretation. It allows us to separate external degrees of freedom (position, momentum) from internal degrees of freedom (spin, charge). It allows us, very roughly, to imagine particles as zipping around on Minkowski space, carrying with them some finite collection of internal characteristics.
%
%But Wigner's analysis means we can regard $\mathfrak{h}$ as a finte-dimensional quantum system in its own right. This means that we can apply Deligne's theorem to it! 
%
%Under our assumptions, Deligne's theorem tells us that the most general type of group that can act on the internal space $\mathfrak{h}$ of a special-relativistic quantum theory is an algebraic super-group. To put it another way, it says that the most general sort of symmetry that a Poincar\"{e}-invariant theory can exhibit is supersymmetry.
%
%\section{Wigner classification with a general global symmetry group}
%
%The axioms of quantum mechanics tell us that the states of a quantum system correspond to vectors $\psi$ belonging to some Hilbert space $\mathscr{H}$. However the observable quantities built from the states, i.e. the conditional probabilities
%\begin{equation*}
%  P(\varphi | \psi) = \frac{\left|\left\langle \varphi | \psi \right\rangle\right|^{2}}{\norm{\varphi}^{2} \cdot \norm{\psi}^{2}}
%\end{equation*}
%are invariant under a rescaling of either $\varphi$ or $\psi$ by any nonzero complex number: that is,
%\begin{equation*}
%  \psi \sim \psi' \qquad\text{if}\qquad \psi' = \lambda \psi\qquad\text{for some }\lambda \in \C\setminus\left\{ 0 \right\},
%\end{equation*}
%where $\sim$ should be read as `is physically indistinguishable from.'
%
%Therefore, we should think about two states which differ only by a nonzero complex factor as being physically equivalent. That is, the true space of states of our quantum system is not $\mathscr{H}$, but $\mathscr{H} / \sim$, \emph{projective Hilbert space}. Physically inequivalent quantum states are in one-to-one correspondence to the equivalence classes in $\mathscr{H}/\sim$.
%
%Now suppose we want our quantum theory to have a symmetry given by some group $G$. Pick any ray $[\psi_{0}] \in \mathscr{H}/\sim$. For any group element $g \in G$, there should be another state (i.e. ray) $[\psi]_{g}$, which corresponds to the result of acting on the system with $g$. Furthermore, we want the result of acting on $[\psi_{0}]$ with a sequence of group elements to be the same no matter how we imagine them to be bracketed. Mathematically, this means that if we are to consider a quantum theory which is $G$-symmetric, we want a Hilbert space $\mathscr{H}$ which admits an injective homomorphism
%\begin{equation*}
%  \rho\colon G \hookrightarrow \mathrm{Aut}(\mathscr{H}/\sim).
%\end{equation*}
%
%Of course, having such a $\rho$ is nice, but quantum mechanics does not deal with projective spaces and ray transformations. If we are to use these results as-is, we need to translate them to results about $\mathscr{H}$ itself.
%
%Pick one specific element $g \in G$. To this element there is associated a bijective ray map on $\mathscr{H}$. Obviously, we can mimic this bijective ray map with a function $\bar{U}(g)\colon \mathscr{H} \to \mathscr{H}$; we need only ensure that the vectors making up individual rays are mapped to the correct target rays. Equally obviously however, this does not specify $\bar{U}(g)$ uniquely; there are many possible ways of mixing the vectors composing the rays. We have some freedom to choose this mixing in as nice as possible a way.
%
%It turns out, due to a theorem of Bargmann {\color{red}FINISH THIS}
%
%% By a theorem of Wigner, we can get away with demanding that $\bar{U}(g)$ be additive and either
%% \begin{itemize}
%%   \item linear and unitary or
%%   \item antilinear and antiunitary.
%% \end{itemize}
%% In doing so, we specify $\bar{U}(g)$ up to an arbitrary phase.
%% 
%% But we have many $\bar{U}(g)$---one for each element of $G$. The arbitrarity in the phases translates to a change in representation law for multiplication of the $\bar{U}(g)$; we now have 
%% \begin{equation*}
%%   \bar{U}(g) \bar{U}(g') = \eta(g, g')\bar{U}(gg')
%% \end{equation*}
%% where the phases $\eta$ are arbitrary.
%% 
%% Many groups have the property that every group element $g$ can be written as a product of squares
%% \begin{equation*}
%%   g = (g_{1})^{2} (g_{2})^{2} \cdots (g_{n})^{2}.
%% \end{equation*}
%% Connected Lie groups are of this form, for example. In this case, we can eliminate the possibility that the representation could be anti-linear and anti-unitary, since the product of an even number of antiunitary operators is even.
%% 
%% In fact, we can do even more. By another theorem of Wigner, we can use the phase factor $\eta$ to replace our unitary ray representation of $G$ by a bona fide unitary representation of its universal covering group $\bar{G}$.
%
%The moral of the story so far: the Hilbert space of any quantum mechanical theory which is invariant under a symmetry group $G$ must admit a linear representation of the covering group $\bar{G}$.
%
%Perhaps now is a good time to make the following remark. If we wish to view two quantum systems with Hilbert spaces $\mathscr{H}_{1}$ and $\mathscr{H}_{2}$ as a single composite system, the Hilbert space we consider is $\mathscr{H}_{1} \otimes \mathscr{H}_{2}$. That is to say, composite systems are the tensor product of their constituents. And the representation carried by the composite system representation of $\bar{G}$ is given by the tensor product of the representations of its constituents.
%
%This would seem to imply that given a $G$-invariant quantum system and its associated representation of $\bar{G}$, we could break it and its representation down into smaller and smaller subsystems, until we hit `atomic' subsystems whose representations were irreducible, i.e. could not be broken down any further. The resulting atomic representations would be the building blocks of our original system. 
%
%But we can pull this trick with \emph{any} system! This means that cataloging the irreducible representations of $\bar{G}$ will give us a list of the building blocks of any $G$-invariant system: the fundamental particles.
%
%This line of reasoning led Wigner to \emph{identify} the fundamental particles of any quantum theory with symmetry group $G$ with irreducible representations of $\bar{G}$. \footnote{As an aside: the symmetry group of the standard model is
%\begin{equation*}
%  G = \overbrace{\mathrm{ISO}(1, 3)}^{\text{Poincar\'{e}}} \times \overbrace{\mathrm{SU}(3) \times \mathrm{SU}(2) \times \mathrm{U}(1)}^{\text{gauge}} \times \mathrm{U}(3),
%\end{equation*}
%where the final $\mathrm{U}(3)$ factor interchanges the three generations of particles which couple to the weak interaction.
%
%All of the fundamental particles in the standard model are correspond to irreducible representations of $G$. Contrary to popular belief, $\mathrm{ISO}(3, 1)$ by itself is not enough. Neutrinos, for example, can exist in a superposition of different mass states; the reason for this is that mass-squared is not a casimir operator of the representation to which they belong.\cite{neumaier}}
%
%The reason for the previous discussion is this: Every quantum theory which makes any attempt to describe the world we see must account for the the existence of spacetime, so every quantum system will admit \emph{some} symmetry group $G$, namely the (covering group of the) symmetries of spacetime. This means that, by Wigner's analysis, any quantum system can be viewed as a composite system made up a (large) number of fundamental particles which are irreducible representations of $\bar{G}$. This ensures that quantum systems will have certain properties. 
%\begin{enumerate}
%  \item One can always consider two or more quantum systems as a single composite system (for example, a proton is a composite system comprised of three quarks). Mathematically, the Hilbert space underlying a composite system is the tensor product of the Hilbert spaces of the component systems, i.e. 
%    \begin{equation*}
%      \mathscr{H}_{\text{composite}} = \overbrace{\mathscr{H}_{1} \otimes \mathscr{H}_{2} \otimes \cdots \otimes \mathscr{H}_{n}}^{\text{subsystems}}.
%    \end{equation*}
%
%    The fact that $\mathscr{H}_{\text{composite}}$ always admits a $G$-representation, namely the tensor product representation
%    \begin{equation*}
%      \varrho_{1} \otimes \varrho_{2} \otimes \cdots \otimes \varrho_{n} \colon G \to \mathscr{H}_{1} \otimes \mathscr{H}_{2} \otimes \cdots \otimes \mathscr{H}_{n}
%    \end{equation*}
%    reflects the fact that the composite system always shares the symmetries of its constituents. That is, we can `multiply' any two quantum systems which admit a $\bar{G}$-symmetry to get another. 
%
%  \item The order in which we form composites does not matter since the tensor product is naturally associative,\footnote{The keyword here is \emph{naturally}, which is a term from category theory which formalizes the intuitive notion of `canonical'.} i.e. there is a natural isomorphism
%    \begin{equation*}
%      \alpha_{1,2,3}\colon (\mathscr{H}_{1} \otimes \mathscr{H}_{2}) \otimes \mathscr{H}_{3} \simeq \mathscr{H}_{1} \otimes (\mathscr{H}_{2} \otimes \mathscr{H}_{3}).
%    \end{equation*}
%
%  \item There is a trivial quantum system with $\mathscr{H}_{\text{trivial}} = \C$ which admits the trivial representation of $\bar{G}$, and which has the property that
%    \begin{equation*}
%      \mathscr{H}_{\text{trivial}} \otimes \mathscr{H} \simeq \mathscr{H} \simeq \mathscr{H} \otimes \mathscr{H}_{\text{trivial}}.
%    \end{equation*}
%\end{enumerate}
%
%These properties, phrased in mathematical language, say that the collection of all quantum systems which admit a $\bar{G}$-symmetry has the structure of a \textbf{monoidal category}. \footnote{Of course, one must also check that the triangle and pentagon diagram are satisfied, but this is trivial and devoid of physical meaning.} 
%
%\begin{enumerate}
%    \setcounter{enumi}{3}
%  \item The order in which we place our quantum systems before considering them as a composite does not matter, since there is an isomorphism
%    \begin{equation*}
%      \gamma_{1,2}\colon \mathscr{H}_{1} \otimes \mathscr{H}_{2} \to \mathscr{H}_{2} \otimes \mathscr{H}_{1}.
%    \end{equation*}
%    This means that our collection of $\bar{G}$-symmetric quantum systems is a \textbf{braided monoidal category}.
%
%  \item If we swap the order in which we adjoin two quantum systems twice, we get the same thing we started with:
%    \begin{equation*}
%      \gamma_{2,1} \circ \gamma_{1,2} = \mathrm{id}_{\mathscr{H}_{1} \otimes \mathscr{H}_{2}} \colon \mathscr{H}_{1} \otimes \mathscr{H}_{2}
%    \end{equation*}
%    This means $\bar{G}$-symmetric quantum systems form a \textbf{symmetric monoidal category}. We'll denote this by $\mathsf{Sym}_{G}$.
%\end{enumerate}
%
%If $\bar{G}$ is completely general, then we can say no more about the category of $G$-symmetric quantum systems. However, one may make further physical assumptions to tame the group $G$. Here we need to do a bit of physics.
%
%First, we must assume that the spacetime on which our quantum theory takes place is Minkowski space, that is, $\R^{4}$ together with the semi-Riemannian metric
%\begin{equation*}
%  \eta_{\mu\nu} =
%  \begin{pmatrix}
%    -1 & 0 & 0 & 0 \\
%    0 & 1 & 0 & 0 \\
%    0 & 0 & 1 & 0 \\
%    0 & 0 & 0 & 1
%  \end{pmatrix}.
%\end{equation*}
%
%The isometry group of Minkowski space is $\mathcal{P} = \mathrm{ISO}(1, 3)$. We can factorize $\mathcal{P}$ as follows:
%\begin{equation*}
%  \mathcal{P} = \left( \text{translations} = \R^{4} \right) \rtimes \left( \text{rotations} = \mathrm{SO}(1, 3) \right).
%\end{equation*}
%
%The group $\bar{G}$ which acts on the Hilbert space of any quantum system which takes place on Minkowski space includes $\bar{\mathcal{P}}$ as a subgroup.
%
%It is here that we make the core simplifying assumption: we demand that $\bar{G}$ can be written
%\begin{equation*}
%  \bar{G} = \R^{4} \rtimes H,
%\end{equation*}
%where $H$ admits finite-dimensional representations, and we consider only these finite-dimensional representations.
%
%This is not a terribly restrictive assumption, and it has an attractive physical interpretation. It allows us to separate external degrees of freedom (position, momentum) from internal degrees of freedom (spin, charge). It allows us, very roughly, to imagine particles as zipping around on Minkowski space, carrying with them some finite collection of internal characteristics.
%
%With this assumption, we can use the little group method (the so-called \emph{Mackey machine}) to factor out the $\R^{4}$ sector of $\bar{G}$ and focus on $H$. Since we deal with only finite-dimensional representations, we don't have to deal with the eccentricities of infinite-dimensional Hilbert spaces. Thus, we can say more about our category.
%\begin{enumerate}
%    \setcounter{enumi}{5}
%  \item Each fundamental particle has an antiparticle, and we can construct from any composite system an anti-system, in which all of its particles are replaced by antiparticles. This corresponds mathematically to the fact that our finite-dimensional Hilbert spaces $\mathscr{H}$ have duals $\mathscr{H}^{*}$ which admit the contragredient representation of $H$.
%
%    This means that we are dealing with a \textbf{rigid symmetric monoidal category}.
%\end{enumerate}
%
%These, plus several other technical requirements ()
%
%% Aficionados of category theory will notice that we have not yet said anything about what is arguably the most important aspect of a category: the morphisms.\footnote{Morphisms are a category-theoretic generalization of the idea of a function. Just as between any two sets $A$ and $B$ one has a collection of functions $A \to B$, between any two objects in a category one has a collection of morphisms $A \to B$.}
%% 
%% The objects in our category are built from tensor products of Hilbert spaces of fundamental particle species, understood as carrying the tensor product representation of their constituents. For concreteness, we can take two objects 
%% \begin{equation*}
%%   \mathscr{H}_{1} = \mathscr{H}_{e} \otimes \mathscr{H}_{\gamma}\qquad\text{and}\qquad \mathscr{H}_{2} = \mathscr{H}_{e}.
%% \end{equation*}
%% The Hilbert space $\mathscr{H}_{1}$ represents an electron and a photon, taken as a composite system, while $\mathscr{H}_{2}$ represents an electron alone in the world.
%% 
%% A morphisms $\mathscr{H}_{1} \to \mathscr{H}_{2}$ is given by a linear map $L\colon \mathscr{H}_{1} \to \mathscr{H}_{2}$ which is group-action equivariant: that is to say, 
%% \begin{equation*}
%%   L \rho_{1}(g) = \rho_{2}(g) L \qquad\text{for all }g \in \bar{G}.
%% \end{equation*}
%% These are known as \emph{intertwiners}.
%% 
%% \begin{note}
%%   For the mathematically-inclined: the symmetric monoidal category we are working with is $\mathsf{Rep}_{\mathsf{Hilb}_{\C}}(\bar{G})$, i.e. the functor category $\mathsf{Func}(\mathbf{B}\bar{G}, \mathsf{Hilb}_{\C})$ (where $\mathbf{B}\bar{G}$ is the delooping groupoid of $\bar{G}$ and $\mathsf{Hilb}_\mathsf{C}$ is the category whose objects are complex separable Hilbert spaces and whose morphisms are bounded linear maps). The objects in $\mathsf{Rep}_{\mathsf{Hilb}_{\C}}(\bar{G})$ are functors $\mathbf{B}\bar{G} \to \mathsf{Hilb}_{\C}$ and the morphisms are natural transformations between them. 
%% 
%%   So far, this is all okay; however, contrary to what is claimed in \cite{nlab-deligne-theorem} and \cite{physicsforums-why-deligne}, $\mathsf{Rep}_{\mathsf{Hilb}_{\C}}(\bar{G})$ cannot be made into a tensor category because it is not abelian. More specifically, 
%% \end{note}
%% 
%% There are several subclasses of problems to which Deligne's theorem can be applied perfectly rigorously: those in which the representations involved are finite. For example, it says that the only allowed symmetries of a \emph{finite-dimensional} quantum system are given by algebraic super-groups, since the completeness requirement is satisfied trivially in finite dimensions.
%
\appendix
\chapter{Algebra}

In this chapter we collect some basic definitions and corollaries so that we may refer to them. The methods of proof are often non-standard, as we prefer to provide proofs whose methods lend themselves to categorification.
\section{Monoids}
\begin{definition}[monoid]
  \label{def:monoid}
  A \defn{monoid} is a set $M$ together with a binary operation $\cdot\colon M \times M \to M$ such that
  \begin{itemize}
    \item $a \cdot (b \cdot c)=(a \cdot b) \cdot c$ for all $a$, $b$, $c$, and
    \item There exists an element $1 \in M$ such that $1 \cdot a=a \cdot 1=a$ for all $a \in M$.
  \end{itemize}
\end{definition}

\begin{lemma}
  \label{lemma:monoidalmultiplicationbyunitbijective}
  The above definition is equivalent to the following.

  Let $M$ be a set, $\cdot$ an associative binary operation on $M$. Then we say $M$ is a \defn{monoid} if there is an element $1 \in M$ such that $1\cdot 1 = 1$ and the maps $M \to M$, $a \mapsto 1 \cdot a$ and $ a \mapsto a \cdot 1$ are bijections.
\end{lemma}
\begin{proof}
  If $1 \cdot a = a$ for all $a$, then the maps $a \to 1\cdot a$ and $a \mapsto a \cdot 1$ are trivially bijections.

  Now, if the map $a \mapsto 1 \cdot a$ is a bijection, then every element $a$ of $M$ can be written $a = 1 \cdot a'$ for some $a'$ in $M$. Then
  \begin{eqnarray*}
    1 \cdot a &=& 1 \cdot (1 \cdot a') \\
    &=& (1 \cdot 1) \cdot a' \\
    &=& 1 \cdot a' \\
    &=& a.
  \end{eqnarray*}

  The proof for right-multiplication by $1$ is identical.
\end{proof}

\section{Groups}
\begin{definition}[group]
  \label{def:group}
  A \defn{group} $(G, \cdot)$ is a monoid with inverses, i.e. a set $G$ with a function $\cdot\colon G \times G \to G$ which is
  \begin{enumerate}
    \item Associative: $(ab)c = a(bc)$
    \item Unital: There is an element $e \in G$ such that $eg = ge$ for all $g \in G$
    \item Invertible: for all $g \in G$, there is an element $g^{-1} \in G$ such that $gg^{-1} = g^{-1}g = e$
  \end{enumerate}
\end{definition}

\begin{lemma}
  The above definition is equivalent to the following. 

  A group is a monoid such that for all $g \in G$, the maps $G \to G$; $h \mapsto gh$ and $h \mapsto hg$ are bijections.
\end{lemma}
\begin{proof}
  If all elements of $G$ are invertible, left-multiplication is obviously bijective. Now suppose left-multiplication by $g$ is a bijection. Then for any $h \in G$, there exists an element $h' \in G$ such that $g \cdot h' = h$. But this is certainly true in particular when $h = e$, so all elements of $G$ are invertible.

  The right-multiplication case is identical.
\end{proof}

\begin{definition}[abelian group]
  \label{def:abeliangroup}
  A group $G$ is \defn{abelian} if for all $a$, $b \in G$, $ab=ba$.
\end{definition}

\begin{definition}[free abelian group]
  \label{def:freeabeliangroup}
  Let $E$ be a set. The \defn{free abelian group generated by $E$} is the group whose elements are formal finite sums of elements of $E$.
\end{definition}

\begin{definition}[direct sum of abelian groups]
  \label{def:directsumofabeliangroup}
  Let $A$, $B$ be abelian groups. Their \defn{direct sum}, denote $A \oplus B$, is the group whose
  \begin{enumerate}
    \item underlying set is $A \times B$, the cartesian product of the underlying sets of $A$ and $B$, and whose

    \item multiplication is given component-wise.
  \end{enumerate}
\end{definition}
\section{Rings}

\begin{definition}(ring)
  \label{def:ring}
  A \defn{ring} $(R, +, \cdot )$ is a set $R$ with two binary operations $+$ and $\cdot $ such that
  \begin{enumerate}
      \setcounter{enumi}{-1}
    \item $R$ is closed under $+$ and $\cdot$.
    \item $R$ is an Abelian group with respect to $+$.
    \item $\cdot$ is associative: $(xy)z = x(yz)$.
    \item $\cdot$ is distributive from the left and from the right: $x(y+z) = xy + xz$, $(x+y)z = xz + yz$.
  \end{enumerate}

  Further,
  \begin{itemize}
    \item $R$ is a \defn{commutative ring} if $a\cdot b = b \cdot a$ for all $a$, $b \in R$.
    \item $R$ is a \defn{ring with identity} if $R$ contains an element $1_{R}$ such that $1_{R}\cdot  x = x\cdot 1_{R}$ for all $x \in R$.
  \end{itemize}
\end{definition}

\begin{definition}[zero-divisor]
  \label{def:zerodivisor}
  A nonzero element $a$ in a ring $R$ is said to be a \defn{left (right) zero divisor} if there exists $b \neq 0$ such that $ab=0$ ($ba=0$). A \defn{zero divisor} is an element of $R$ which is both a left and a right zero divisor.
\end{definition}

\begin{definition}[invertible element]
  \label{def:ringinvertible}
  An element $a$ in a ring $R$ is said to be \defn{left- (right-)invertible} if there exists $b \in R$ such that $ab=1$ ($ba=1$). An element $a \in \R$ which is both left- and right-invertible is said to be \defn{invertible}, or a \defn{unit}.
\end{definition}

\begin{definition}[field]
  \label{def:field}
  A \defn{field} is a commutative ring such that every nonzero element has a multiplicative inverse.
\end{definition}

\begin{example}[important example]
  Let $X$ be a topological space. The set
  \begin{equation*}
    C^{1}(X) = \left\{ f\colon X \to \R\,\big|\, f \text{ continuous} \right\}
  \end{equation*}
  is a commutative ring with identity, with addition and multiplication defined pointwise. To see this, we need to check the axioms in \hyperref[def:ring]{Definition \ref*{def:ring}}:
  \begin{enumerate}
      \setcounter{enumi}{-1}
    \item The sum of two continuous functions is continuous; the product of two continuous functions is continuous.
    \item $C^{1}(X)$ inherit its abelian group structure from the real numbers; the additive identity is the function which maps all $x \in X$ to $0 \in \R$.
    \item Associativity is inherited from the real numbers
    \item Distributivity is inherited from the real numbers.
  \end{enumerate}
\end{example}

\begin{definition}[ring homomorphism]
  \label{def:ringhomomorphism}
  Let $R$ and $S$ be rings. A function $f\colon R \to S$ is a \defn{ring homomorphism} if
  \begin{equation*}
    f(a+b) = f(a) + f(b), \qquad \text{and}\qquad f(ab) = f(a)f(b).
  \end{equation*}
\end{definition}

\begin{definition}[ring isomorphism]
  \label{def:ringisomorphism} 
  A \defn{ring isomorphism} is a bijective ring homomorphism.
\end{definition}

\begin{definition}[ideal]
  \label{def:ideal}
  Let $R$ be a ring and $I$ a nonempty subset of $R$. $I$ is called a \defn{left (right) ideal} if
  \begin{enumerate}
    \item $I$ is closed under addition:
      \begin{equation*}
        a,\,b \in I \implies a+b\in I.
      \end{equation*}

    \item $I$ absorbs elements of $R$ under multiplication:
      \begin{equation*}
        r \in R \quad\text{and}\quad x\in I \implies rx \in I\quad (xr \in I).
      \end{equation*}
  \end{enumerate}
  If $I$ is both a left and a right ideal, it is called an \defn{ideal}.
\end{definition}
\begin{theorem}
  Let $R$ be a ring, and let $\left\{ A_{i}\,\big|\, i \in I \right\}$ be a set of [left] ideals of $X$. Then 
  \begin{equation*}
    A = \bigcap_{i \in I}A_{i}
  \end{equation*}
  is an ideal.
  \label{thm:intersectionofidealsisideal}
\end{theorem}
\begin{proof}
  According to \hyperref[def:ideal]{Definition \ref*{def:ideal}}, we need to check two things:
  \begin{enumerate}
    \item Closure under addition: let $a$, $b\in A$. Then $a+b$ is in each of the $A_{i}$, so it must be in their intersection.
    \item Absorption: for any $a \in A$ and any $r \in R$, $ra$ must be in $A_{i}$ for all $i$; hence it must be in their intersection.
  \end{enumerate}
\end{proof}

\begin{definition}[ideal generated by a subset]
  \label{def:idealgenerated}
  Let $X$ be a subset of a ring $R$. Let $\left\{ A_{i}\,\big|\, i \in I \right\}$ be the family of all [left] ideals in $R$ which contain $X$. Then 
  \begin{equation*}
    \bigcap_{i \in I} A_{i}
  \end{equation*}
  is called the \defn{[left] ideal generated by $X$}, and is denoted $(X)$. The elements of $X$ are called the \defn{generators} of $(X)$.
\end{definition}

\begin{definition}[localization]
  \label{def:localizationofaring}
  Let $R$ be a commutative ring with unity, and let $S$ be a subset of $R$. The \defn{localization of $R$ at $S$} is given, as a set, by the quotient 
  \begin{equation*}
    R \times S / \sim,
  \end{equation*}
  where $(r_{1}, r_{2}) \sim (s_{1}, s_{2})$ if there exists $t \in S$ such that 
  \begin{equation*}
    t(r_{1} s_{2} - r_{2} s_{1}) = 0.
  \end{equation*}
  It is easy to check that this is an equivalence relation.

  The multiplication and addition on $S^{-1}R$ are defined via
  \begin{equation*}
    (r_{1}, s_{2}) \times (r_{2}, s_{2}) = (r_{1}r_{2}, s_{1}s_{2}),
  \end{equation*}
  and
  \begin{equation*}
    (r_{1}, s_{2}) + (r_{2}, s_{2}) = (r_{1}s_{2} + r_{2}s_{1}, s_{1}s_{2}).
  \end{equation*}
\end{definition}

\begin{note}
  The idea is to think of $S^{-1}R$ as consisting of fractions $\frac{r}{s}$. The addition and multiplication laws are therefore just mimic the addition and multiplication of fractions.

  There is a ring homomorphism $R \to S^{-1}R$ which maps $r \mapsto \frac{r}{1}$. This is not injective in general.
\end{note}

\section{Modules}
\begin{definition}[module]
  \label{def:module}
  Let $R$ be a ring. A (left) \defn{$R$-module} is an additive abelian group $A$ together with a function $*\colon R \times A \to A$ such that for all $r$, $s \in R$ and $a$, $b \in A$,
  \begin{enumerate}
    \item $r*(a+b) = r*a + r*b$
    \item $(r+s)*a = r*a + s+a$
    \item $(rs)*a = r*(s*a)$.
  \end{enumerate}
  If $R$ has an identity element $1_{R}$ and $1_{R}*a = a$ for all $a \in A$, then $A$ is said to be a \defn{unitary $R$-module}.
\end{definition}

\begin{note}
  Right $R$-modules are defined in the obvious way.
\end{note}

\begin{note}
  We will now stop notationally differentiating between $R$-multiplication and $A$-multiplication, using juxtaposition for both.
\end{note}

\begin{definition}[bimodule]
  \label{def:bimodule}
  A \defn{R-S-bimodule} is an Abelian group $A$ which is a left $R$-module and a right $S$-module, such that 
  \begin{equation*}
    (ra)s = r(as)\qquad\text{for all } r\in R,\quad s\in S,\quad a\in A.
  \end{equation*}

  We say that the left-multiplication and the right multiplication are \emph{consistent}.
\end{definition}

\begin{theorem}
  \label{thm:leftmoduletransformstoothermodules}
  If $R$ is a commutative ring and $A$ is a left $R$-module, then $A$ can be canonically transformed into a right module or a bimodule.
\end{theorem}
\begin{proof}
  With $ar \equiv ra$, $A$ is both a right $R$-module and an $R$-bimodule. The axioms in \hyperref[def:module]{Definition \ref*{def:module}} are immediate.
\end{proof}


\begin{definition}[nilpotent]
  \label{def:nilpotent}
  Let $R$ be a ring. An element $r \in R$ is said to be \defn{nilpotent} if 
  \begin{equation*}
    r^{n} = 0\qquad\text{for some } n \in \N^{+}.
  \end{equation*}
\end{definition}
\begin{definition}[commutator]
  \label{def:commutator}
  Let $R$ be a ring, $a$, $b \in R$. The \defn{commutator} of $a$ and $b$ is 
  \begin{equation*}
    [a,b] \equiv ab-ba.
  \end{equation*}
\end{definition}
\begin{theorem}
  \label{thm:propertiesofcommutator}
  Let $R$ be a ring, $a$, $b \in R$. The commutator satisfies the following properties.
  \begin{itemize}
    \item $[a,b] = -[b,a]$
    \item $[a,[b,c]] + [b,[c,a]] + [c,[a,b]] = 0$.
  \end{itemize}
  Further, if $A$ is a ring and $B$ is an $A$-algebra (in the sense of \hyperref[def:algebraoveraring]{Definition \ref*{def:algebraoveraring}}), then for any $a\in A$, $b$, $c \in B$, the following identity holds.
  \begin{equation*}
    a[b,c] = [ab,c] = [b,ac] = [b,ca] = [b,a]c.
  \end{equation*}
\end{theorem}
\begin{proof}
  $\,$
  \begin{itemize}
    \item $[a,b] = ab-ba=-(ba-ab)=-[b,a]$.
    \item We have 
      \begin{align*}
        [a,[b,c]] &= a(bc-cb) - (bc-cb)a \\
        [b,[c,a]] &= b(ca-ac) - (ca-ac)b \\
        [c,[a,b]] &= c(ab-ba) - (ab-ba)c,
      \end{align*}
      so
      \begin{align*}
        [a,[b,c]]+[b,[c,a]]+[c,[a,b]] =\quad& abc - acb - bca + cba \\
        +& bca - bac - cab + acb \\
        +& cab - cba - abc + bac.
      \end{align*}

      Terms cancel in pairs.

    \item $a[b,c] = a(bc-cb) = (ab)c - c(ab) = [ab,c] = b(ac)-(ac)b = [b,ac] = (bc-cb)a = [b,c]a$.
  \end{itemize}
\end{proof}

\begin{definition}[free module]
  \label{def:freemodule}
  Let $E$ be a set, $R$ be a ring. The \defn{free left module of $E$ over $R$}, denoted $R^{(E)}$, is the module whose elements are are finite formal $R$-linear combinations of the elements of $E$.
\end{definition}

\begin{example}
  The free abelian group over a set $E$ is also a free $\Z$-module over $E$ with the identification
  \begin{equation*}
    \underbrace{a + a + \cdots + a}_{n\text{ times}} = na.
  \end{equation*}
\end{example}

\begin{definition}[projective module]
  \label{def:projectivemodule}
  Let $A$ be a commutative ring, and $P$ an $A$-module. One says that $P$ is \defn{projective} if for any $A$-module epimorphism $\varphi\colon Q \to R$ and any homomorphism $\psi\colon P \to R$. there exists a homomorphism $\xi\colon P \to Q$ such that $\varphi \circ \xi = \psi$, i.e. the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      & P
      \arrow[dl, swap, dashed, "\exists\xi"]
      \arrow[d, "\psi"]
      \\
      Q \arrow[r, swap, twoheadrightarrow, "\varphi"]
      & R
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{definition}[finitely generated module]
  \label{def:finitelygeneratedmodule}
  Let $R$ be a ring, $M$ be an $R$-module. We say that $M$ is \defn{finitely generated} if there exist $m_{1}$, $m_{2}$, \dots, $m_{k}$ such that any $m \in M$ can be expressed 
  \begin{equation*}
    m = r_{1} m_{1} + r_{2} m_{2} + \cdots + r_{k} m_k
  \end{equation*}
  for some $r_{i} \in R$.
\end{definition}
\begin{note}
  We are \emph{not} demanding that the $m_{1}$ be linearly independent.
\end{note}

\section{Algebras}
\begin{definition}[algebra over a ring]
  \label{def:algebraoveraring}
  Let $R$ be a commutative ring. An \defn{$R$-algebra} is a ring $A$ which is also an $R$-module (with left-multiplication $*\colon R \times A \to A$) such that the multiplication map $\cdot\colon A \times A \to A$ is $R$-bilinear, i.e. 
  \begin{equation*}
    r*(a\cdot b) = (r*a)\cdot b = a\cdot (r*b),\qquad\text{for any }a,b \in A,\quad r \in R.
  \end{equation*}
\end{definition}

\begin{theorem}
  \label{thm:ringhomomorphisminducesalgebra}
  Let $A$, $R$ be unital rings, $R$ commutative, and let $f\colon R \to A$ be a unital ring homomorphism. Then $A$ naturally has the structure of an $R$-module. Furthermore, if the image of $R$ is in the center of $A$, then $A$ naturally has the structure of an $R$-algebra.
\end{theorem}
\begin{proof}
  Define the action
  \begin{equation*}
    *\colon R \times A \to A;\qquad (r,a) \mapsto f(r)\cdot a.
  \end{equation*}
  The verification that this makes $A$ into an $R$-module is trivial; we need only check left and right distributivity, associativity, and that $1_{R}*a = a$. We omit this.

  To see that what we have is even an algebra, we need to check that $*$ is bilinear, i.e.
  \begin{equation*}
    r*(a\cdot b) = f(r)\cdot (a\cdot b) = (f(r)\cdot a)\cdot b = (r*a)\cdot b
  \end{equation*}
  and
  \begin{equation*}
    r*(a\cdot b) = f(r)\cdot a \cdot b = a \cdot f(r) \cdot b = a\cdot (r*b).
  \end{equation*}
\end{proof}

Often we are interested specifically in algebras over a field. In that case we have the following.
\begin{definition}[algebra over a field]
  \label{def:algebraoverafield}
  An \defn{algebra} $A$ over a field $k$ is a $k$-vector space with a $k$-bilinear product $A \times A \to A$.

  Further, an algebra $A$ is
  \begin{itemize}
    \item \defn{associative} if for all $a$, $b$, $c \in A$, $(a \times b) \times c = a \times (b \times c)$.

    \item \defn{unitary} if there exists an element $1 \in A$ such that $1 \times a = a \times 1 = a$ for all $a \in A$.
  \end{itemize}
\end{definition}

\begin{note}
  In general, algebras are taken to be associative and unital by default. A notable exception is Lie algebras.
\end{note}


\section{Tensor products}
In this section we give a brief overview of the construction of the tensor product.

\begin{definition}[middle-linear map]
  \label{def:middlelinearmap}
  Let $R$ be a ring, $M$ be a right $R$-module, $N$ a left $R$-module, and $G$ an abelian group. A map $\varphi\colon M \times N \to G$ is said to be \defn{$R$-middle-linear} if for all $m$, $m' \in M$, $n$, $n' \in N$, and $r \in R$ we have
  \begin{enumerate}
    \item $\varphi(m, n + n') = \varphi(m, n) + \varphi(m, n')$
    \item $\varphi(m + m', n) = \varphi(m, n) + \varphi(m', n)$
    \item $\varphi(m \cdot r, n) = \varphi(m, r \cdot n)$.
  \end{enumerate}
\end{definition}

\begin{definition}[tensor product of modules]
  \label{def:tensorproductofmodules}
  Let $R$ be a ring, $M$ be a right $R$-module, $N$ a left $R$-module. The \defn{tensor product} of $M$ and $N$, denoted $M \otimes_{R} N$, is constructed as follows.

  Let $F$ be the free abelian group generated by $M \times N$ (\hyperref[def:freeabeliangroup]{Definition \ref*{def:freeabeliangroup}}). Let $K$ be the subgroup of $F$ generated by elements of the form
  \begin{itemize}
    \item $(m, n + n') - (m, n) - (m, n')$
    \item $(m + m', n) - (m, n) - (m' n)$
    \item $(m \cdot r, n) - (m, r \cdot n)$.
  \end{itemize}

  The tensor product of $M$ and $N$ is then
  \begin{equation*}
    M \otimes_{R} N = F / K.
  \end{equation*}
\end{definition}

\begin{note}
  If $R$ is commutative, then we can canonically make $M$ and $N$ into bimodules. In this case we don't need to distinguish between left and right multiplication. If $R$ is a field, we recover the notion of the `standard' tensor product, which is usually given by the following definition.
\end{note}

\begin{definition}[tensor product of vector spaces]
  \label{def:tensorproductofvectorspaces}
  Let $k$ be a field, $V$ and $W$ $k$-vector spaces. The \defn{tensor product} $V \otimes W$ is the vector space defined as follows.

  Denote by $\mathcal{F}(V \times W)$ the free vector space over $V \times W$. Consider the vector subspace $K$ of $\mathcal{F}(V \times W)$ generated by elements of the forms 
  \begin{itemize}
    \item $(v_{1} + v_{2}, w) - (v_{1}, w) - (v_{2}, w)$
    \item $(v, w_{1} + w_{2}) - (v, w_{1}) - (v, w_{2})$
    \item $(\alpha v, w) - \alpha (v, w)$
    \item $(v, \alpha w) - \alpha (v, w)$
  \end{itemize}
  for all $v_{1}$, $v_{2} \in V$, $w_{1}$, $w_{2} \in W$, $\alpha \in k$. Then define
  \begin{equation*}
    V \otimes W = \mathcal{F}(V \times W) / K.
  \end{equation*}

  Pairs $(v, w) \in V \times W$ are called \defn{representing tuples}.
\end{definition}

\begin{definition}[tensor product of linear maps]
  \label{def:tensorproductoflinearmaps}
  Let $V$, $W$, $X$, and $Y$ be $k$-vector spaces, and let $S\colon V \to X$, $T\colon W \to Y$. Then the \defn{tensor product} of $S$ and $T$ is the map
  \begin{equation*}
    S \otimes T\colon V \otimes W \to X \otimes Y;\qquad (v \otimes w) \mapsto S(v) \otimes T(w).
  \end{equation*}

  Of course, one must show that this is well-defined by showing that it vanishes on the subspace by which one quotients in the definition of the tensor product. But this is trivial.
\end{definition}

\chapter{Categories} \label{ch:categories}
In this chapter, I will do my best to stick to the conventions used at the nlab (\cite{nlab}).

\section{Basic definitions} \label{sec:categoriesbasicdefinitions}
\begin{definition}[category] 
  \label{def:category} 
  A \defn{category} $\mathsf{C}$ consists of 
  \begin{itemize} 
    \item a class $\Obj(\mathsf{C})$ of \emph{objects}, and \item for every two objects $A$, $B \in \Obj(\mathsf{C})$, a class $\Hom(A,B)$ of \emph{morphisms} with the following properties.  
      \begin{enumerate} 
        \item \label{item:compositionofmorphisms} For $f \in \Hom(A,B)$ and $g \in \Hom(B,C)$, there is an associated morphism 
          \begin{equation*} 
            g \circ f \in \Hom(A,C), 
          \end{equation*} called the \emph{composition} of $f$ and $g$.

        \item This composition is associative.

        \item \label{item:existenceofidentitymorphism} For every $A \in \Obj(\mathsf{C})$, there is at least one morphism $1_{A}$, called the \emph{identity morphism} which functions as both a left and right identity with respect to the composition of morphisms.

      \end{enumerate} 
  \end{itemize} 
\end{definition}

\begin{note}
  There is a reason we say that the objects and morphisms of a category are a \emph{class} rather than a set. It may be that there may be `too many' objects to be contained in a set. For example, we will see that there is a category of sets, but there is no set of all sets. Categories whose objects and/or morphisms \emph{are} small enough to be contained in a set will play an especially important role.
\end{note}

\begin{notation}
  Following Aluffi (\cite{aluffi-algebra-chapter-0}), we will use the sans-serif font \texttt{mathsf} to denote categories. For example $\mathsf{C}$, $\mathsf{Set}$.

  If ever it is potentially unclear which category we are talking about, we will add a subscript to $\Hom$, writing for example $\Hom_{\mathsf{C}}(A,B)$ instead of $\Hom(A,B)$.
\end{notation}

\begin{notation}
  The identity morphism $1_{A}$ is often simply denoted by $A$. We will avoid this in the earlier chapters since it is potentially confusing, but use it freely in later chapters.
\end{notation}

\begin{example}
  The prototypical category is $\mathsf{Set}$, the category whose objects are sets and whose morphisms are set functions.  
\end{example}


\begin{example}[category with one object]
  \label{eg:categorywithoneobject}
  The category $\mathsf{1}$, where $\Obj(\mathsf{1})$ is the singleton $\{*\}$, and the only morphism is the identity morphism $\mathrm{id}_{*}\colon * \to *$.
\end{example}
\begin{example}
  \label{eg:examplesofcategories}
  Pretty much all standard algebraic constructions naturally live in categories. For example, we have 
  \begin{itemize} 
    \item the category $\mathsf{Grp}$, whose objects are groups and whose morphisms are group homomorphisms;

    \item \label{item:categoryab} the category $\mathsf{Ab}$, whose objects are abelian groups and whose morphisms are group homomorphisms;

    \item the category $\mathsf{Ring}$, whose objects are rings and whose morphisms are ring homomorphisms;

    \item the category $R\mhyp\mathsf{Mod}$, whose objects are modules over a ring $R$ and whose morphisms are module homomorphisms;

    \item the category $\mathsf{Vect}_{k}$, whose objects are vector spaces over a field $k$ and whose morphisms are linear maps;
    \item the category $\mathsf{FinVect}_{k}$, whose objects are finite-dimensional vector spaces over a field $k$ and whose morphisms are linear maps;
    \item the category $\mathsf{Alg}_{k}$, whose objects are algebras over a field $k$ and whose morphisms are algebra homomorphisms.
  \end{itemize}
\end{example}

\begin{example}
  \label{eg:moreexamplesofcategories}
  In addition to algebraic structures, categories help to formulate geometric structures, such as
  \begin{itemize}
    \item the category $\mathsf{Top}$, whose objects are topological spaces and whose morphisms are continuous maps;
    \item the category $\mathsf{Met}$, whose objects are metric spaces and whose morphisms are metric maps;
    \item the category $\mathsf{Man}^{p}$, whose objects are manifolds of class $C^{p}$ and whose morphisms are $p$-times differentiable functions;

    \item the category $\mathsf{SmoothMfd}$, whose objects are $C^{\infty}$ manifolds and whose morphisms are smooth functions
  \end{itemize}
  and many more.
\end{example}

Here are a few ways we can use existing categories to create new ones.
\begin{definition}[opposite category]
  \label{def:oppositecategory}
  Let $\mathsf{C}$ be a category. The \defn{opposite category} $\mathsf{C}^{\mathrm{op}}$ is the category whose objects are the same as the objects $\Obj(\mathsf{C})$ and whose morphisms $f \in \Hom_{\mathsf{C}^{\mathrm{op}}}(A, B)$ are defined to be the morphisms $\Hom_{\mathsf{C}}(B, A)$.

  That is to say, the opposite category is the category one gets by formally reversing all the arrows in a category. If $f \in \Hom_{\mathsf{C}}(A, B)$, i.e. $f \colon A \to B$, then in $\mathsf{C}^{\mathrm{op}}$, $f\colon B \to A$. 
\end{definition} 

\begin{definition}[product category]
  \label{def:productcategory}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories. The \defn{product category} $\mathsf{C} \times \mathsf{D}$ is the category whose 
  \begin{itemize}
    \item objects are ordered pairs $(C, D)$, where $C \in \Obj(\mathsf{C})$ and $D \in \Obj(\mathsf{D})$, whose
    \item morphisms are ordered pairs $(f,g)$, where $f \in \Hom_{\mathsf{C}}(C_{1}, C_{2})$ and $g \in \Hom_{\mathsf{D}}(D_{1}, D_{2})$, in which
    \item composition is taken componentwise, so that
      \begin{equation*}
        (f_{1}, g_{1}) \circ (f_{2},g_{2}) = (f_{1} \circ g_{1}, f_{2} \circ g_{2}),
      \end{equation*}
      and
    \item the identity morphisms are given in the obvious way:
      \begin{equation*}
        1_{(C,D)} = (1_{C}, 1_{D}).
      \end{equation*}
  \end{itemize}
\end{definition}

\begin{definition}[subcategory]
  \label{def:subcategory}
  Let $\mathsf{C}$ be a category. A category $\mathsf{S}$ is a \defn{subcategory} of $\mathsf{C}$ if
  \begin{itemize}
    \item The objects $\Obj(\mathsf{S})$ of $\mathsf{S}$ are a subcollection of the objects of $\mathsf{C}$

    \item For $S$, $T \in \Obj(\mathsf{S})$, the morphisms $\Hom_{\mathsf{S}}(S, T)$ are a subcollection of the morphisms $\Hom_{\mathsf{C}}(S, T)$ such that
      \begin{itemize}
        \item for every $S \in \Obj(S)$, the identity $1_{C} \in \Hom_{\mathsf{S}}(S, S)$.

        \item for all $f \in \Hom_{\mathsf{S}}(S, T)$ and $g \in \Hom_{\mathsf{S}}(T, U)$, the composite $g \circ f \in \Hom_{\mathsf{S}}(S, U)$.
      \end{itemize}
  \end{itemize} 

  If $\mathsf{S}$ is a subcategory of $\mathsf{C}$, we will write $\mathsf{S} \subseteq \mathsf{C}$.
\end{definition}

\begin{definition}[full subcategory]
  \label{def:fullsubcategory}
  Let $\mathsf{C}$ be a category, $\mathcal{I}\colon \mathsf{S} \hookrightarrow \mathsf{C}$ a subcategory. We say that $\mathsf{S}$ is \defn{full} in $\mathsf{C}$ if for every $S$, $T \in \Obj(\mathsf{S})$, $\Hom_{\mathsf{S}}(S, T) \simeq \Hom_{\mathsf{C}}(S, T)$. That is, if there are no morphisms in $\Hom_{\mathsf{C}}(S, T)$ which cannot be written $\mathcal{I}(f)$ for some $f \in \Hom_{\mathsf{S}}(S, T)$.
\end{definition}

\begin{example}
  \label{eg:finvectfullsubcategoryofvect}
  Recall that $\mathsf{Vect}_{k}$ is the category of vector spaces over a field $k$, and $\mathsf{FinVect}_{k}$ is the category of finite dimensional vector spaces. 

  It is not difficult to see that $\mathsf{FinVect}_{k} \subseteq \mathsf{Vect}_{k}$: all finite dimensional vector spaces are vector spaces, and all linear maps between finite-dimensional vector spaces are maps between vector spaces. In fact, since for $V$ and $W$ finite-dimensional, one does not gain any maps by moving from $\Hom_{\mathsf{FinVect}_{k}}(V, W)$ to $\Hom_{\mathsf{Vect}_{k}}(V, W)$, $\mathsf{FinVect}_{k}$ is even a \emph{full} subcategory of $\mathsf{Vect}_{k}$.
\end{example}

Category theory has many essences, one of which is as a major generalization of set theory. We would like to upgrade definitions and theorems about functions between sets to definitions and theorems about morphisms between objects in a category. It is here that we run into our first major challenge: in general, the objects of a category are \emph{not} sets, so we cannot talk about their elements. We therefore have to find definitions which we can give purely in terms objects and morphisms between them.
\begin{definition}[isomorphism]
  \label{def:isomorphism}
  Let $\mathsf{C}$ be a category, $A$, $B \in \Obj(\mathsf{C})$. A morphism $f \in \Hom(A,B)$ is said to be an \defn{isomorphism} if there exists a morphism $g \in \Hom(B,A)$ such that 
  \begin{equation*}
    g \circ f = 1_{A},\qquad\text{and}\quad f \circ g = 1_{B}.
  \end{equation*}

  If we have an isomorphism $f\colon A \to B$, we say that $A$ and $B$ are \defn{isomorphic}, and write $A \simeq B$.
\end{definition}

\begin{definition}[monomorphism]
  \label{def:monomorphism}
  Let $\mathsf{C}$ be a category, $A$, $B\in \Obj(\mathsf{C})$. A morphism $f\colon A \to B$ is said to be a \defn{monomorphism} if for all $Z \in \Obj(\mathsf{C})$ and all $g_{1}$, $g_{2}\colon Z \to A$, $f \circ g_{1} = f\circ g_{2}$ implies $g_{1} = g_{2}$.
  \begin{equation*}
    \begin{tikzcd}
      Z \arrow[r, shift left, "g_{1}"] \arrow[r, shift right, swap, "g_{2}"] & A \arrow[r, "f"] & B
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{note}
  When we wish to notationally distinguish monomorphisms, we will denote them by hooked arrows: if $f\colon A \to B$ is mono, we will write
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[r, hookrightarrow, "f"]
      & B
    \end{tikzcd}.
  \end{equation*}
\end{note}

\begin{theorem}
  In $\mathsf{Set}$, a morphism is a monomorphism precisely when it is injective.
\end{theorem}
\begin{proof}
  Suppose $f\colon A \to B$ is a monomorphism. Then for any set $Z$ and any maps $g_{1}$, $g_{2}\colon Z \to A$, $f \circ g_{1} = f \circ g_{2}$ implies $g_{1} = g_{2}$. In particular, take $Z = \{*\}$ and suppose $g_{1}(*) = a_{1}$ and $g_{2}(*) = a_{2}$. Then $(f \circ g_{1})(*) = f(a_{1})$ and $(f \circ g_{2})(*) = f(a_{2})$, so
  \begin{equation*}
    f(a_{1}) = f(a_{2}) \implies a_{1} = a_{2}.
  \end{equation*}
  But this is exactly the definition of injectivity.

  Now suppose that $f$ is injective. Then for any $Z$ and $g_{1}$, $g_{2}$ as above,
  \begin{equation*}
    (f \circ g_{1})(z) = (f \circ g_{2})(z) \implies g_{1}(z) = g_{2}(z)\qquad\text{for all } z \in Z.
  \end{equation*}
  But this means that $g_{1} = g_{2}$, so $f$ is mono.
\end{proof}

\begin{example}
  \label{eg:monomorphismsinkvect}
  In $\mathsf{Vect}_{k}$, monomorphisms are injective linear maps.
\end{example}

\begin{definition}[epimorphism]
  \label{def:epimorphism}
  Let $\mathsf{C}$ be a category, $A$, $B\in \Obj(\mathsf{C})$. A morphism $f\colon A \to B$ is said to be a \defn{epimorphism} if for all $Z \in \Obj(\mathsf{C})$ and all $g_{1}$, $g_{2}\colon B \to Z$, $g_{1} \circ f = g_{2}\circ f$ implies $g_{1} = g_{2}$.
  \begin{equation*}
    \begin{tikzcd}
      A \arrow[r, "f"] & B \arrow[r, shift left, "g_{1}"] \arrow[r, shift right, swap, "g_{2}"] & Z
    \end{tikzcd}.
  \end{equation*}
\end{definition}

\begin{notation}
  We will denote epimorphisms by two-headed right arrows. That is, if $f\colon A \to B$ is epi, we will write
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[r, twoheadrightarrow, "f"]
      & B
    \end{tikzcd}
  \end{equation*}
\end{notation}

\begin{example}
  \label{eg:epimorphismsinkvect}
  In $\mathsf{Set}$, epimorphisms are surjections.
\end{example}

\begin{example}
  In $\mathsf{Vect}_{k}$, epimorphism are surjective linear maps.
\end{example}

\begin{note}
  In $\mathsf{Set}$, isomorphisms are bijections, bijections are injective surjections, and injective surjections are monic epimorphisms, so a morphism is iso if and only if it is monic and epic. This is \emph{not} true in general: a morphism can be monic and epic without being an isomorphism.

  Take, for example, $\mathsf{Top}$, where the morphisms are continuous maps. In order for a morphism $f$ to be monic and epic, it is necessary only that it be injective and surjective; it must have a set-theoretic inverse. However its inverse does not have to be continuous, and therefore may not be a morphism in $\mathsf{Top}$.
\end{note}

Set theory has some foundational annoyances: not every collection of objects is small enough to be a set, for example. Category theory has its own foundational issues, which for the most part we will avoid. However, there are a few important situations in which foundational questions play an unavoidably important role.
\begin{definition}[small, locally small, hom-set]
  \label{def:smalllocallysmallcategoryhomset}
  A category $\mathsf{C}$ is 
  \begin{itemize}
    \item \defn{small} if $\Obj(\mathsf{C})$ is a set and for all objects $A$, $B \in \Obj(\mathsf{C})$, $\Hom_{\mathsf{C}}(A, B)$ is a set.

    \item \defn{locally small} if for all $A$, $B \in \Obj(\mathsf{C})$, $\Hom_{\mathsf{C}}(A, B)$ is a set. In this case, we call $\Hom_{\mathsf{C}}(A, B)$ the \defn{hom-set}. (Actually, terminology is often abused, and $\Hom_{\mathsf{C}}(A, B)$ is called a hom-set even if it is not a set.)
  \end{itemize}
\end{definition}

\begin{example}
  \label{eg:groupsaregroupoidswithoneobject}
  Here is a slightly whimsical example of a category, which will turn out to have great relevance for Deligne's theorem.

  Let $G$ be a group. Let us create a category $\mathsf{G}$ which behaves like this group.
  \begin{itemize}
    \item Our category $\mathsf{G}$ has only one object, called $*$.

    \item The set $\Hom_{\mathsf{G}}(*, *)$ is equal to the underlying set of the group $G$, and for $f$, $g \in \Hom_{\mathsf{G}}(*, *)$, the compositions $f \circ g = f\cdot g$, where $\cdot$ is the group operation in $G$. The identity $e \in G$ is the identity morphism $1_{*}$ on $*$.
  \end{itemize}
  \begin{equation*}
    \begin{tikzcd}
      *
      \arrow[loop right, "e = 1_{*}"]
      \arrow[loop above, "g"]
      \arrow[loop left, "h"]
      \arrow[loop below, "g \circ h"]
    \end{tikzcd}
  \end{equation*}

  Note: since each $g \in G$ has an inverse, every morphism in $\mathsf{G}$ is an isomorphism.
\end{example}

\section{Functors} \label{sec:functors}
Just as morphisms connect objects in the same category, functors allow us to connect different categories. In fact, functors can be viewed as morphisms in a `category of categories.'

\begin{definition}[functor] 
  \label{def:functor} 
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories. A \defn{functor} $\mathcal{F}$ from $\mathsf{C}$ to $\mathsf{D}$ is a mapping which associates 
  \begin{itemize} 
    \item to each object $X \in \Obj(\mathsf{C})$ an object $\mathcal{F}(X) \in \Obj(\mathsf{D})$.

    \item to each morphism $f \in \Hom(X, Y)$ a morphism $\mathcal{F}(f)$ such that $\mathcal{F}(1_{X}) = 1_{\mathcal{F}(X)}$ for all $X$, and one of the two following properties are satisfied.
      \begin{itemize} 
        \item The morphism $f \in \Hom(\mathcal{F}(X), \mathcal{F}(Y))$ and if $f:X \to Y$ and $g\colon Y \to Z$, then 
          \begin{equation*}
            \mathcal{F}(g \circ f) = \mathcal{F}(g) \circ \mathcal{F}(f).
          \end{equation*}
          In this case we say that $\mathcal{F}$ is \defn{covariant}.

        \item The morphism $f \in \Hom(\mathcal{F}(Y), \mathcal{F}(X))$, and if $f\colon X \to Y$ and $g\colon Y \to Z$, then
          \begin{equation*}
            \mathcal{F}(g \circ f) = \mathcal{F}(f) \circ \mathcal{F}(g).
          \end{equation*}
          In this case, we say that $\mathcal{F}$ is \defn{contravariant}.
      \end{itemize} 
  \end{itemize} 
\end{definition}

\begin{notation}
  We will typeset functors with calligraphic letters using the font \texttt{eucal}, and notate them with squiggly arrows. For example, if $\mathsf{C}$ and $\mathsf{D}$ are categories and $\mathcal{F}$ is a functor from $\mathsf{C}$ to $\mathsf{D}$, then we would write
  \begin{equation*}
    \mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}.
  \end{equation*}
\end{notation}

\begin{note}
  \label{note:contravariantfunctorisfunctorfromoppositecategory}
  One can also define a contravariant functor $\mathsf{C} \rightsquigarrow \mathsf{D}$ as a covariant functor $\mathsf{C}^{\mathrm{op}} \rightsquigarrow \mathsf{D}$.
\end{note}

\begin{note}
  When the adjective is unspecified, a \emph{functor} will mean a \emph{covariant functor}.
\end{note}

\begin{example}
  \label{eg:functorfrom1category}
  Let $\mathsf{1}$ be the category with one object $*$ (\hyperref[eg:categorywithoneobject]{Example \ref*{eg:categorywithoneobject}}). Let $\mathsf{C}$ be any category. Then for each $X \in \Obj(\mathsf{C})$, we have the functor
  \begin{equation*}
    \mathcal{F}_{X}\colon \mathsf{1} \rightsquigarrow \mathsf{C};\qquad \mathcal{F}(*) = X,\quad \mathcal{F}(\mathrm{id}_{*}) = \mathrm{id}_{X}.
  \end{equation*}
\end{example}

\begin{example}
  \label{eg:twofunctorsgrptocring}
  Recall that $\mathsf{Grp}$ is the category of groups. Denote by $\mathsf{CRing}$ the category of commutative rings.

  We can construct the following functors $\mathsf{CRing} \rightsquigarrow \mathsf{Grp}$.
  \begin{itemize}
    \item The functor $\GL_{n}$, which assigns to each commutative ring the group of all $n \times n$ matrices with nonzero determinant, and to each morphism $f\colon K \to K'$ the homomorphism $\GL_{n}(K) \to \GL_{n}(K')$ which maps a matrix with entries in $K$ to a matrix with entries in $K'$ by mapping each entry individually.

    \item The functor $(\,\cdot\,)^{*}$, which maps each commutative ring $K$ to its group of units $K^{*}$, and each morphism $K \to K'$ to its restriction to $K^{*}$.
  \end{itemize}
\end{example}

One of the nice things about functors is that they map commutative diagrams to commutative diagrams. For example, if $\mathcal{F}$ is a contraviariant functor, then one might see the following.
\begin{equation*}
  \begin{tikzcd}
    A \arrow[rr, "h = g \circ f"] \arrow[rd,"f", swap] & & B \\
    & C \arrow[ur, "g", swap] 
  \end{tikzcd} 
  \begin{tikzcd}
    \ \arrow[r, rightsquigarrow, "\mathcal{F}"] & \ 
  \end{tikzcd}
  \begin{tikzcd}
    \mathcal{F}(A) \arrow[rr, "\mathcal{F}(h) = \mathcal{F}(f) \circ \mathcal{F}(g)", leftarrow] \arrow[rd,"\mathcal{F}(f)", leftarrow, swap] & & \mathcal{F}(B) \\
    & \mathcal{F}(C) \arrow[ur, "\mathcal{F}(g)", leftarrow, swap] 
  \end{tikzcd}
\end{equation*}

\begin{definition}[full, faithful]
  \label{def:fullfaithfulfunctor}
  Let $\mathsf{C}$ and $\mathsf{D}$ be locally small categories (\hyperref[def:smalllocallysmallcategoryhomset]{Definition \ref*{def:smalllocallysmallcategoryhomset}}), and let $\mathcal{F}: \mathsf{C} \rightsquigarrow \mathsf{D}$. Then $\mathcal{F}$ induces a family of set-functions
  \begin{equation*}
    \mathcal{F}_{X, Y}\colon \Hom_{\mathsf{C}}(X, Y) \to \Hom_{\mathsf{D}}(\mathcal{F}(X), \mathcal{F}(Y)).
  \end{equation*}

  We say that $\mathcal{F}$ is
  \begin{itemize}
    \item \defn{full} if $\mathcal{F}_{X, Y}$ is surjective for all $X$, $Y \in \Obj(\mathsf{C})$
    \item \defn{faithful} if $\mathcal{F}_{X, Y}$ is injective for all $X$, $Y \in \Obj(\mathsf{C})$,
    \item \defn{fully faithful} if $\mathcal{F}$ is full and faithful.
  \end{itemize}
\end{definition}

\begin{note}
  Fullness and faithfulness are \emph{not} the functorial analogs of surjectivity and injectivity. A functor between small categories can be full (faithful) without being surjective (injective) on objects. Instead, we have the following result.
\end{note}

\begin{lemma}
  \label{lemma:fullyfaithfulfunctorinjectiveuptoisomorphism}
  A fully faithful functor is injective on objects up to isomorphism. That is, if $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ is a fully faithful functor and $\mathcal{F}(A) \simeq \mathcal{F}(B)$, then $A \simeq B$.
\end{lemma}
\begin{proof}
  Let $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ be a fully faithful functor, and suppose that $\mathcal{F}(A) \simeq \mathcal{F}(B)$. Then there exist $f'\colon \mathcal{F}(A) \to \mathcal{F}(B)$ and $g'\colon \mathcal{F}(B) \to \mathcal{F}(A)$ such that $f' \circ g' = 1_{\mathcal{F}(B)}$ and $g' \circ f' = 1_{\mathcal{F}(A)}$. Because the function $\mathcal{F}_{A, B}$ is bijective it is invertible, so there is a unique morphism $f \in \Hom_{\mathsf{C}}(A, B)$ such that $\mathcal{F}(f) = f'$, and similarly there is a unique $g \in \Hom_{\mathsf{C}}(B, A)$ such that $\mathcal{F}(g) = g'$.

  Now, 
  \begin{equation*}
    1_{\mathcal{F}(A)} = g' \circ f' = \mathcal{F}(g) \circ \mathcal{F}(f) = \mathcal{F}(g \circ f), 
  \end{equation*}
  and since $\mathcal{F}$ is injective, we must have $g \circ f = 1_{A}$. Identical logic shows that we must also have $f \circ g = 1_{B}$. Thus $A \simeq B$.
\end{proof}

\begin{definition}[essentially surjective]
  \label{def:essentiallysurjective}
  A functor $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ is \defn{essentially surjective} if for every $A' \in \Obj(\mathsf{D})$, there exists $A \in \Obj(\mathsf{C})$ such that $A' \simeq \mathcal{F}(A)$.
\end{definition}

\begin{example}[diagonal functor]
  \label{eg:diagonalfunctor}
  Let $\mathsf{C}$ be a category, $\mathsf{C} \times \mathsf{C}$ the product category of $\mathsf{C}$ with itself. The \defn{diagonal functor} $\Delta\colon \mathsf{C} \rightsquigarrow \mathsf{C} \times \mathsf{C}$ is the functor which sends 
  \begin{itemize}
    \item each object $A \in \Obj(\mathsf{C})$ to the pair $(A,A) \in \Obj(\mathsf{C} \times \mathsf{C})$, and
    \item each morphism $f\colon A \to B$ to the ordered pair 
      \begin{equation*}
        (f,f) \in \Hom_{\mathsf{C}\times\mathsf{C}}(A\times B,A \times B).
      \end{equation*}
  \end{itemize}
\end{example}

\begin{definition}[bifunctor]
  \label{def:bifunctor}
  A \defn{bifunctor} is a functor whose domain is a product category (\hyperref[def:productcategory]{Definition \ref*{def:productcategory}}).
\end{definition}

\begin{example}
  \label{eg:functorscanbegrouprepresentation}
  Recall \hyperref[eg:groupsaregroupoidswithoneobject]{Example \ref*{eg:groupsaregroupoidswithoneobject}}. Let $G$ be a group, and $\mathsf{G}$ the category which mimics it.

  Then functors $\rho\colon \mathsf{G} \rightsquigarrow \mathsf{Vect}_{k}$ are $k$-linear representations of $G$! 

  To see this, let us unwrap the definition. The functor $\rho$ assigns to $* \in \Obj(\mathsf{G})$ an object $\rho(*) = V \in \Obj(\mathsf{Vect})$, and to each morphism $g\colon * \to *$ a morphism $\rho(g)\colon V \to V$. This assignment sends units to units, and this composition is associative.
\end{example}

\section{Natural transformations} \label{sec:naturaltransformations}
Saunders Mac Lane, one of the fathers of category theory, used to say that he invented categories so he could talk about functors, and he invented functors so he could talk about natural transformations. Indeed, the first paper ever published on category theory, published by Eilenberg and Mac Lane in 1945, was titled ``General Theory of Natural Equivalences.'' \cite{awodey-intro-to-categories}

Natural transformations provide a notion of `morphism between functors.' They allow us much greater freedom in talking about the relationship between two functors than equality.

Here is a motivating example. Let $A$ and $B$ be sets. There is an isomorphism from $A \times B$ to $B \times A$ which is given by switching the order of the ordered pairs:
\begin{equation*}
  \mathrm{swap}_{A, B}\colon (a, b) \mapsto (b, a).
\end{equation*}
Similarly, for sets $C$ and $D$, there is an isomorphism $C \times D$ to $D \times C$ which is given by switching the order of ordered pairs.
\begin{equation*}
  \mathrm{swap}_{C, D}\colon (c, d) \mapsto (d, c).
\end{equation*}
In some obvious intuitive sense, these isomorphisms are really the same isomorphism. However, it is not obvious how to formalize this, since the definition of a function contains the information about the image and coimage, so they cannot be equal as functions. 

Here is how it is done. Notice that for \emph{any} functions $f\colon A \to C$ and $g\colon B \to D$, the following diagram commutes.
\begin{equation*}
  \begin{tikzcd}
    A \times B
    \arrow[r, "{(f, g)}"]
    \arrow[d, swap, "{\text{swap}_{A, B}}"]
    & C \times D
    \arrow[d, "{\text{swap}_{C, D}}"]
    \\
    B \times A
    \arrow[r, "{(g, f)}"]
    & D \times C
  \end{tikzcd}
\end{equation*}
That is to say, if you're trying to go from $A \times B$ to $D \times C$ and all you've got is functions $f\colon A \to B$ and $g\colon C \to D$ and the two swap isomorphisms, it doesn't matter whether you use the swap isomorphism on $A \times B$ and then use $(g, f)$ to get to $D \times C$, or immediately go to $C \times D$ with $(f, g)$, then use the swap isomorphism there: the result is the same. Furthermore, this is true for \emph{any} functions $f$ and $g$.

As we will see, the cartesian product of sets is a functor from the product category (\hyperref[def:productcategory]{Definition \ref*{def:productcategory}}) $\mathsf{Set} \times \mathsf{Set}$ to $\mathsf{Set}$ which maps $(A, B)$ to $A \times B$. There is another functor $\mathsf{Set} \times \mathsf{Set} \rightsquigarrow \mathsf{Set}$ which sends $(A, B)$ to $B \times A$. The above means that there is a natural isomorphism, called $\mathrm{swap}$, between them.

This example makes clear another common theme of natural transformations: they formalize the idea of `the same function between different objects.' They allow us to make precise statements like `$\mathrm{swap}_{A, B}$ and $\mathrm{swap}_{C, D}$ are somehow the same, despite the fact that they are in no way equal as functions.'

\begin{definition}[natural transformation]
  \label{def:naturaltransformation}
  let $\mathsf{C}$ and $\mathsf{D}$ be categories, and let $\mathcal{F}$ and $\mathcal{G}$ be covariant functors from $\mathsf{C}$ to $\mathsf{D}$. a \defn{natural transformation} $\eta$ between $\mathcal{F}$ and $\mathcal{G}$ consists of 
  \begin{itemize}
    \item for each object $A \in \Obj(\mathsf{C})$ a morphism $\eta_{A}\colon \mathcal{F}(A) \to \mathcal{G}(A)$, such that
    \item for all $A$, $B \in \Obj(C)$, for each morphism $f \in \Hom(A,B)$, the diagram
      \begin{equation*}
        \begin{tikzcd}
          \mathcal{F}(A)\arrow[d, swap, "\eta_{A}"] \arrow[r, "\mathcal{F}(f)"] & \mathcal{F}(B)\arrow[d, "\eta_{B}"] \\
          \mathcal{G}(A) \arrow[r, swap, "\mathcal{G}(f)"] & \mathcal{G}(B) 
        \end{tikzcd}
      \end{equation*}
      commutes.
  \end{itemize}

  If the functors $\mathcal{F}$ and $\mathcal{G}$ are contravariant, the changes needed to make the definition make sense are obvious: basically, some arrows need to be reversed. However, this is one of those times where it's easier, in line with \hyperref[note:contravariantfunctorisfunctorfromoppositecategory]{Note \ref*{note:contravariantfunctorisfunctorfromoppositecategory}}, to just pretend that we have a covariant functor from the opposite category.

\end{definition}

\begin{definition}[natural isomorphism]
  \label{def:naturalisomorphism}
  A \defn{natural isomorphism} $\eta\colon \mathcal{F} \Rightarrow \mathcal{G}$ is a natural transformation such that each $\eta_{A}$ is an isomorphism.
\end{definition}


\begin{notation}
  We will use double-shafted arrows to denote natural transformations: if $\mathcal{F}$ and $\mathcal{G}$ are functors and $\eta$ is a natural transformation from $\mathcal{F}$ to $\mathcal{G}$, we will write
  \begin{equation*}
    \eta\colon \mathcal{F} \Rightarrow \mathcal{G}.
  \end{equation*}
\end{notation}

\begin{definition}[set of all natural transformations]
  \label{def:setofallnaturaltransformations}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories, $\mathcal{F}$ and $\mathcal{G}$ functors $\mathsf{C} \rightsquigarrow \mathsf{D}$. The \defn{set of all natural transformations} $\mathcal{F} \Rightarrow \mathcal{G}$ is denoted $\Nat(\mathcal{F}, \mathcal{G})$.
\end{definition} 

\begin{example}
  Recall the functors $\mathrm{GL}_{n}$ and $(\cdot)^{*}$ from \hyperref[eg:twofunctorsgrptocring]{Example \ref*{eg:twofunctorsgrptocring}}.

  Denote by $\mathrm{det}_{K}M$ the determinant of a matrix $M$ with its entries in a commutative ring $K$. Then the determinant is a map
  \begin{equation*}
    \mathrm{det}_{K}\colon \GL_{n}(K) \to K^{*}.
  \end{equation*}

  Because the determinant is defined by the same formula for each $K$, the action of $f$ commutes with $\det_{K}$: it doesn't matter whether we map the entries of $M$ with $f$ first and then take the determinant, or take the determinant first and then feed the result to $f$.

  That is to say, the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      \GL_{n}(K)
      \arrow[r, "\det_{K}"]
      \arrow[d, swap, "\GL_{n}(f)"]
      & K^{*}
      \arrow[d, "f^{*}"]
      \\
      \GL_{n}(K')
      \arrow[r, "\det_{K'}"]
      & K'^{*}
    \end{tikzcd}
  \end{equation*}
  But this means that $\det$ is a natural transformation $\GL_{n} \Rightarrow \mathrm{(\,\cdot\,)^{*}}$.
\end{example}

We can compose natural transformations in the obvious way.
\begin{lemma}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories, $\mathcal{F}$, $\mathcal{G}$, $\mathcal{H}$ be functors, and $\Phi$ and $\Psi$ be natural transformations as follows.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C}
      \arrow[r, rightsquigarrow, bend left=60, "\mathcal{F}"{name=U}]
      \arrow[r, rightsquigarrow, "\mathcal{G}"{name=M} description]
      \arrow[r, rightsquigarrow, bend right=60, swap, "\mathcal{H}"{name=D}]
      & \mathsf{D}
      \arrow[from=U, to=M, Rightarrow, "\Phi"]
      \arrow[from=M, to=D, Rightarrow, "\Psi"]
    \end{tikzcd}
  \end{equation*}

  This induces a natural transformation $\mathcal{F} \Rightarrow \mathcal{H}$.
\end{lemma}
\begin{proof}
  For each object $A \in \Obj(\mathsf{C})$, the composition $\Psi_{A}\circ \Phi_{A}$ exists and maps $\mathcal{F}(A) \to \mathcal{H}(A)$. Let's write
  \begin{equation*}
    \Psi_{A} \circ \Phi_{A} = (\Psi \circ \Phi)_{A}.
  \end{equation*}
  We have to show that these are the components of a natural transformation, i.e. that they make the following diagram commute for all $A$, $B \in \Obj(\mathsf{C})$, all $f: A \to B$.
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}(A) 
      \arrow[d, swap, "(\Psi \circ \Phi)_{A}"]
      \arrow[r, "\mathcal{F}(f)"]
      & \mathcal{F(B)}
      \arrow[d, "(\Psi \circ \Phi)_{B}"]
      \\
      \mathcal{H}(A)
      \arrow[r, "\mathcal{H}(f)"]
      & \mathcal{H}(B)
    \end{tikzcd}
  \end{equation*}
  We can do this by adding a middle row.
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}(A) 
      \arrow[d, swap, "\Phi_{A}"]
      \arrow[r, "\mathcal{F}(f)"]
      & \mathcal{F(B)}
      \arrow[d, "\Phi_{B}"]
      \\
      \mathcal{G}(A)
      \arrow[r, "\mathcal{G}(f)"]
      \arrow[d, swap, "\Psi_{A}"]
      & \mathcal{G}(B)
      \arrow[d, "\Psi_{B}"]
      \\
      \mathcal{H}(A)
      \arrow[r, "\mathcal{H}(f)"]
      & \mathcal{H}(B)
    \end{tikzcd}
  \end{equation*}

  The top and bottom squares are the naturality squares for $\Phi$ and $\Psi$ respectively. The outside square is the one we want to commute, and it manifestly does because each of the inside squares does.
\end{proof}

\begin{definition}[vertical composition]
  \label{def:verticalcomposition}
  The above composition $\Psi \circ \Phi$ is called \defn{vertical composition}.
\end{definition}

\begin{definition}[functor category]
  \label{def:functorcategory}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories. The \defn{functor category} $\mathsf{Func}(\mathsf{C}, \mathsf{D})$ (sometimes $\mathsf{D}^{\mathsf{C}}$ or $[\mathsf{C},\mathsf{D}]$) is the category whose objects are functors $\mathsf{C} \rightsquigarrow \mathsf{D}$, and whose morphisms are natural transformations between them. The composition is given by vertical composition.
\end{definition}

We can also compose natural transformations in a not so obvious way.
\begin{lemma}
  Consider the following arrangement of categories, functors, and natural transformations.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathsf{C}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{F}"{name=U1}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{F}'"{name=D1}]
      & \mathsf{D}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{G}"{name=U2}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{G}'"{name=D2}]
      & \mathsf{E}
      \arrow[from=U1, to=D1, Rightarrow, "\Phi"]
      \arrow[from=U2, to=D2, Rightarrow, "\Psi"]
    \end{tikzcd}
  \end{equation*}

  This induces a natural transformation $\mathcal{G} \circ \mathcal{F} \Rightarrow \mathcal{G}' \circ \mathcal{F}'$.
\end{lemma}
\begin{proof}
  By definition, $\Phi$ and $\Psi$ make the diagrams
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}(A)
      \arrow[d, swap, "\Phi_{A}"] 
      \arrow[r, "\mathcal{F}(f)"] 
      & \mathcal{F}(B)
      \arrow[d, "\Phi_{B}"] 
      \\
      \mathcal{F}'(A) 
      \arrow[r, swap, "\mathcal{F}'(f)"] 
      & \mathcal{F}'(B) 
    \end{tikzcd}
    \qquad\text{and}\qquad
    \begin{tikzcd}
      \mathcal{G}(A)
      \arrow[d, swap, "\Psi_{A}"] 
      \arrow[r, "\mathcal{G}(f)"] 
      & \mathcal{G}(B)
      \arrow[d, "\Psi_{B}"] 
      \\
      \mathcal{G}'(A) 
      \arrow[r, swap, "\mathcal{G}'(f)"] 
      & \mathcal{G}'(B) 
    \end{tikzcd}
  \end{equation*}
  commute respectively. Since functors take commutative diagrams to commutative diagrams, we can map everything in the first diagram to $\mathsf{E}$ with $\mathcal{G}$.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G} \circ \mathcal{F})(A)
      \arrow[d, swap, "\mathcal{G}(\Phi_{A})"] 
      \arrow[r, "(\mathcal{G} \circ \mathcal{F})(f)"] 
      & (\mathcal{G} \circ \mathcal{F})(B)
      \arrow[d, "\mathcal{G}(\Phi_{B})"] 
      \\
      (\mathcal{G} \circ \mathcal{F}')(A) 
      \arrow[r, "(\mathcal{G} \circ \mathcal{F}')(f)"] 
      & (\mathcal{G} \circ \mathcal{F}')(B) 
    \end{tikzcd}
  \end{equation*}
  Also, since $\mathcal{F}'(f)\colon \mathcal{F}'(A) \to \mathcal{F}'(B)$ is a morphism in $\mathsf{D}$ and $\Psi$ is a natural transformation, the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G} \circ \mathcal{F}')(A)
      \arrow[r, "(\mathcal{G} \circ \mathcal{F}')(f)"]
      \arrow[d, swap, "\Psi_{\mathcal{F}'(A)}"]
      & (\mathcal{G} \circ \mathcal{F}')(B)
      \arrow[d, "\Psi_{\mathcal{F}'(B)}"]
      \\
      (\mathcal{G}' \circ \mathcal{F}')(A)
      \arrow[r, "(\mathcal{G}' \circ \mathcal{F}')(f)"]
      & (\mathcal{G}' \circ \mathcal{F}')(B)
    \end{tikzcd}
  \end{equation*}

  Sticking these two diagrams on top of each other gives a new commutative diagram.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G} \circ \mathcal{F})(A)
      \arrow[d, swap, "\mathcal{G}(\Phi_{A})"] 
      \arrow[r, "(\mathcal{G} \circ \mathcal{F})(f)"] 
      & (\mathcal{G} \circ \mathcal{F})(B)
      \arrow[d, "\mathcal{G}(\Phi_{B})"] 
      \\
      (\mathcal{G} \circ \mathcal{F}')(A) 
      \arrow[r, "(\mathcal{G} \circ \mathcal{F}')(f)"] 
      \arrow[d, swap, "\Psi_{\mathcal{F}'(A)}"]
      & (\mathcal{G} \circ \mathcal{F}')(B) 
      \arrow[d, "\Psi_{\mathcal{F}'(B)}"]
      \\
      (\mathcal{G}' \circ \mathcal{F}')(A)
      \arrow[r, "(\mathcal{G}' \circ \mathcal{F}')(f)"]
      & (\mathcal{G}' \circ \mathcal{F}')(B)
    \end{tikzcd}
  \end{equation*}
  The outside rectangle is nothing else but the commuting square for a natural transformation 
  \begin{equation*}
    (\Psi * \Phi)\colon \mathcal{G} \circ \mathcal{F} \Rightarrow \mathcal{G'} \circ \mathcal{F}' 
  \end{equation*}
  with components $(\Psi * \Phi)_{A} = \Psi_{\mathcal{F}(A)} \circ \mathcal{G}(\Phi_{A})$.
\end{proof}

\begin{definition}[horizontal composition]
  \label{def:horizontalcomposition}
  The natural transformation $\Psi * \Phi$ defined above is called the \defn{horizontal composition} of $\Phi$ and $\Psi$.
\end{definition}

\begin{note}
  Really, the above definition of the horizontal composition is lopsided and ugly. It becomes less so if we notice the following. The first step in our construction of $\Psi * \Phi$ was to apply the functor $\mathcal{G}$ to the commutative diagram
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}(A)
      \arrow[d, swap, "\Phi_{A}"] 
      \arrow[r, "\mathcal{F}(f)"] 
      & \mathcal{F}(B)
      \arrow[d, "\Phi_{B}"] 
      \\
      \mathcal{F}'(A) 
      \arrow[r, swap, "\mathcal{F}'(f)"] 
      & \mathcal{F}'(B) 
    \end{tikzcd}.
  \end{equation*}

  We could instead have applied the functor $\mathcal{G}'$, giving us the following.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G}' \circ \mathcal{F})(A)
      \arrow[d, swap, "\mathcal{G}'(\Phi_{A})"] 
      \arrow[r, "(\mathcal{G}' \circ \mathcal{F})(f)"] 
      & (\mathcal{G}' \circ \mathcal{F})(B)
      \arrow[d, "\mathcal{G}'(\Phi_{B})"] 
      \\
      (\mathcal{G}' \circ \mathcal{F}')(A) 
      \arrow[r, "(\mathcal{G}' \circ \mathcal{F}')(f)"] 
      & (\mathcal{G}' \circ \mathcal{F}')(B) 
    \end{tikzcd}
  \end{equation*}

  Then we could have glued to it the bottom of the following commuting square.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G} \circ \mathcal{F})(A)
      \arrow[r, "(\mathcal{G} \circ \mathcal{F})(f)"]
      \arrow[d, swap, "\Psi_{\mathcal{F}(A)}"]
      & (\mathcal{G} \circ \mathcal{F})(B)
      \arrow[d, "\Psi_{\mathcal{F}(B)}"]
      \\
      (\mathcal{G}' \circ \mathcal{F})(A)
      \arrow[r, "(\mathcal{G}' \circ \mathcal{F})(f)"]
      & (\mathcal{G}' \circ \mathcal{F})(B)
    \end{tikzcd}
  \end{equation*}
  If you do this you get \emph{another} natural transformation $\mathcal{G} \circ \mathcal{F} \Rightarrow \mathcal{G}' \circ \mathcal{F}'$, with components $\mathcal{G}'(\Phi_{A}) \circ \Psi_{\mathcal{F}(A)}$. Why did we use the first definition rather than this one? 

  It turns out that $\Psi_{\mathcal{F}(A)} \circ \mathcal{G}(\Phi_{A})$ and $\mathcal{G}'(\Phi_{A}) \circ \Psi_{\mathcal{F}(A)}$ are equal. To see this, pick any $A \in \Obj(\mathsf{A})$. From the morphism
  \begin{equation*}
    \Phi_{A}\colon \mathcal{F}(A) \to \mathcal{F}'(A),
  \end{equation*}
  the natural transformation $\Psi$ gives us a commuting square
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (\mathcal{G} \circ \mathcal{F})(A)
      \arrow[r, "\mathcal{G}(\Phi_{A})"]
      \arrow[d, swap, "\Psi_{\mathcal{F}(A)}"]
      & (\mathcal{G} \circ \mathcal{F}')(A)
      \arrow[d, "\Psi_{\mathcal{F}'(A)}"]
      \\
      (\mathcal{G}' \circ \mathcal{F})(A)
      \arrow[r, "\mathcal{G}'(\Phi_{A})"]
      & (\mathcal{G}' \circ \mathcal{F}')(A)
    \end{tikzcd};
  \end{equation*}
  the two ways of going from top left to bottom right are nothing else but $\Psi_{\mathcal{F}(A)} \circ \mathcal{G}(\Phi_{A})$ and $\mathcal{G}'(\Phi_{A}) \circ \Psi_{\mathcal{F}(A)}$.
\end{note}

\begin{example}[whiskering]
  \label{eg:whiskering}
  Consider the following assemblage of categories, functors, and natural transformations.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathsf{C}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{F}"{name=U1}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{F}'"{name=D1}]
      & \mathsf{D}
      \arrow[r, rightsquigarrow, "\mathcal{G}"]
      & \mathsf{E}
      \arrow[from=U1, to=D1, Rightarrow, "\Phi"]
    \end{tikzcd}
  \end{equation*}

  The horizontal composition allows us to $\Phi$ to a natural transformation $\mathcal{G} \circ \mathcal{F}$ to $\mathcal{G} \circ \mathcal{F}'$ as follows. First, augment the diagram as follows.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathsf{C}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{F}"{name=U1}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{F}'"{name=D1}]
      & \mathsf{D}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{G}"{name=U2}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{G}"{name=D2}]
      & \mathsf{E}
      \arrow[from=U1, to=D1, Rightarrow, "\Phi"]
      \arrow[from=U2, to=D2, Rightarrow, "1_{\mathcal{G}}"]
    \end{tikzcd}
  \end{equation*}

  We can then take the horizontal composition of $\Phi$ and $1_{\mathcal{G}}$ to get a natural transformation from $\mathcal{G}\circ \mathcal{F}$ to $\mathcal{G} \circ \mathcal{F}'$ with components
  \begin{equation*}
    (1_{\mathcal{G}} * \Phi)_{A} = \mathcal{G}(\Phi_{A}).
  \end{equation*}
  This natural transformation is called the \emph{right whiskering} of $\Phi$ with $\mathcal{G}$, and is denoted $\mathcal{G}\Phi$. That is to say, $(\mathcal{G}\Phi)_{A} = \mathcal{G}(\Phi_{A})$. 
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C}
      \arrow[r, rightsquigarrow, bend left, "\mathcal{G} \circ \mathcal{F}"{name=U}]
      \arrow[r, swap, rightsquigarrow, bend right, "\mathcal{G} \circ \mathcal{F}'"{name=D}]
      & \mathsf{E}
      \arrow[from=U, to=D, Rightarrow, "\mathcal{G}\Phi"]
    \end{tikzcd}
  \end{equation*}
  The reason for the name is clear: we removed a whisker from the RHS of our diagram.

  We can also remove a whisker from the LHS. Given this:
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C}
      \arrow[r, rightsquigarrow, "\mathcal{F}"]
      & \mathsf{D}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{G}"{name=U}]
      \arrow[r, bend right, rightsquigarrow, swap, "\mathcal{G}'"{name=D}]
      & \mathsf{E}
      \arrow[from=U, to=D, Rightarrow, "\Psi"]
    \end{tikzcd}
  \end{equation*}
  we can build a natural transformation (denoted $\Psi\mathcal{F}$) with components
  \begin{equation*}
    (\Psi\mathcal{F})_{A} = \Psi_{\mathcal{F}(A)},
  \end{equation*}
  making this:
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C}
      \arrow[r, rightsquigarrow, bend left, "\mathcal{G} \circ \mathcal{F}"{name=U}]
      \arrow[r, swap, rightsquigarrow, bend right, "\mathcal{G}' \circ \mathcal{F}"{name=D}]
      & \mathsf{E}
      \arrow[from=U, to=D, Rightarrow, "\Psi\mathcal{F}"]
    \end{tikzcd}.
  \end{equation*}
  This is called the \emph{left whiskering} of $\Psi$ with $\mathcal{F}$
\end{example}

\begin{lemma}
  \label{lemma:naturalisomorphismshaveinverses}
  If we have a natural isomorphism $\eta\colon \mathcal{F} \Rightarrow \mathcal{G}$, we can construct a natural isomorphism $\eta^{-1}\colon \mathcal{G} \Rightarrow \mathcal{F}$.
\end{lemma}
\begin{proof}
  The natural transformation gives us for any two objects $A$ and $B$ and morphism $f\colon A \to B$ a naturality square
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}(A)
      \arrow[r, "\mathcal{F}(f)"]
      \arrow[d, swap, "\eta_{A}"]
      & \mathcal{F}(B)
      \arrow[d, "\eta_{B}"]
      \\
      \mathcal{G}(A)
      \arrow[r, "\mathcal{G}(f)"]
      & \mathcal{G}(B)
    \end{tikzcd}
  \end{equation*}
  which tells us that 
  \begin{equation*}
    \eta_{B} \circ \mathcal{F}(f) = \mathcal{G}(f) \circ \eta_{A}.
  \end{equation*}

  Since $\eta$ is a natural isomorphism, its components $\eta_{A}$ are isomorphisms, so they have inverses $\eta_{A}^{-1}$. Acting on the above equation with $\eta_{A}^{-1}$ from the right and $\eta_{B}^{-1}$ from the left, we find
  \begin{equation*}
    \mathcal{F}(f) \circ \eta_{A}^{-1} = \eta_{B}^{-1} \circ \mathcal{G}(f),
  \end{equation*}
  i.e. the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{G}(A)
      \arrow[r, "\mathcal{G}(f)"]
      \arrow[d, swap, "\eta_{A}^{-1}"]
      & \mathcal{G}(B)
      \arrow[d, "\eta_{B}^{-1}"]
      \\
      \mathcal{F}(A)
      \arrow[r, "\mathcal{F}(f)"]
      & \mathcal{F}(B)
    \end{tikzcd}
  \end{equation*}

  But this is just the naturality square for a natural isomorphism $\eta^{-1}$ with components $(\eta^{-1})_{A} = \eta_{A}^{-1}$.
\end{proof}

We would like to be able to express when two categories are the `same.' The correct notion of sameness is provided by the following definition.

\begin{definition}[categorical equivalence]
  \label{def:categoricalequivalence}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories. We say that $\mathsf{C}$ and $\mathsf{D}$ are \defn{equivalent} if there is a pair of functors
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C} \arrow[r, rightsquigarrow, shift left, "\mathcal{F}"] & \arrow[l, shift left, rightsquigarrow, "\mathcal{G}"] \mathsf{D}
    \end{tikzcd}
  \end{equation*}
  and natural isomorphisms $\eta\colon \mathcal{F} \circ \mathcal{G} \Rightarrow 1_{\mathsf{C}}$ and $\varphi\colon 1_{\mathsf{C}} \Rightarrow \mathcal{G} \circ \mathcal{F}$.
\end{definition} 

\begin{note}
  The above definition of categorical equivalence is equivalent to the following: $\mathsf{C}$ and $\mathsf{D}$ are equivalent if there is a functor $\mathcal{F}: \mathsf{C} \rightsquigarrow \mathsf{D}$ which is fully faithful (\hyperref[def:fullfaithfulfunctor]{Definition \ref*{def:fullfaithfulfunctor}}) and essentially surjective (\hyperref[def:essentiallysurjective]{Definition \ref*{def:essentiallysurjective}}). That is to say, if the equivalence $\mathcal{F}$ is `bijective up to isomorphism.'
\end{note}

\begin{definition}[essentially small category]
  \label{def:essentiallysmall}
  A category $\mathsf{C}$ is said to be \defn{essentially small} if it is equivalent to a small category (\hyperref[def:smalllocallysmallcategoryhomset]{Definition \ref*{def:smalllocallysmallcategoryhomset}}).
\end{definition}

\begin{example}
  \label{eg:intertwinersarenaturaltransformations}
  Let $G$ be a group, $\mathsf{G}$ the category which mimics it, and let $\rho$ and $\rho'\colon \mathsf{G} \rightsquigarrow \mathsf{Vect}$ be representations (recall \hyperref[eg:functorscanbegrouprepresentation]{Example \ref*{eg:functorscanbegrouprepresentation}}.)

  A natural transformation $\eta\colon \rho \Rightarrow \rho'$ is called an \emph{intertwiner}. So what \emph{is} an intertwiner?

  Well, $\eta$ has only one component, $\eta_{*}\colon \rho(*) \to \rho'(*)$, which is subject to the condition that for any $g \in \Hom_{\mathsf{G}}(*,*)$, the diagram below commutes.
  \begin{equation*}
    \begin{tikzcd}
      \rho(*)
      \arrow[r, "\rho(g)"]
      \arrow[d, swap, "\eta_{*}"]
      & \rho(*)
      \arrow[d, "\eta_{*}"]
      \\
      \rho'(*)
      \arrow[r, "\rho'(g)"]
      & \rho'(*)
    \end{tikzcd}
  \end{equation*}

  That is, an intertwiner is a linear map $\rho(*) \to \rho'(*)$ such that for all $g$,
  \begin{equation*}
    \eta_{*} \circ \rho(g) = \rho'(g) \circ \eta_{*}.
  \end{equation*}
\end{example}

\section{Some special categories} \label{sec:specialcategories}
\subsection{Comma categories}

\begin{definition}[comma category]
  \label{def:commacategory}
  Let $\mathsf{A}$, $\mathsf{B}$, $\mathsf{C}$ be categories, $\mathcal{S}$ and $\mathcal{T}$ functors as follows.
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{A} 
      \arrow[r, rightsquigarrow, "\mathcal{S}"]  
      & \mathsf{C} 
      & \arrow[l, rightsquigarrow, swap, "\mathcal{T}"]
      \mathsf{B}
    \end{tikzcd}
  \end{equation*}
  The \defn{comma category} $(\mathcal{S} \downarrow \mathcal{T})$ is the category whose
  \begin{itemize}
    \item objects are triples $(\alpha, \beta, f)$ where $\alpha\in\Obj(\mathsf{A})$, $\beta \in \Obj(\mathsf{B})$, and $f \in \Hom_{\mathsf{C}}(\mathcal{S}(\alpha), \mathcal{T}(\beta))$, and whose
    \item morphisms $(\alpha, \beta, f) \to (\alpha', \beta', f')$ are all pairs $(g, h)$, where $g\colon \alpha \to \alpha'$ and $h\colon \beta \to \beta'$, such that the diagram
      \begin{equation*}
        \begin{tikzcd}
          \mathcal{S}(\alpha) \arrow[r, "\mathcal{S}(g)"] \arrow[d, swap, "f"] & \mathcal{S}(\alpha') \arrow[d, "f'"]\\
          \mathcal{T}(\beta) \arrow[r, "\mathcal{T}(h)"] & \mathcal{T}(\beta') 
        \end{tikzcd}
      \end{equation*}
      commutes.
  \end{itemize}
\end{definition}

\begin{notation}
  We will often specify the comma category $(\mathcal{S} \downarrow \mathcal{T})$ by simply writing down the diagram
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{A}\arrow[r, rightsquigarrow, "\mathcal{S}"] & \mathsf{C} & \arrow[l, rightsquigarrow, swap, "\mathcal{T}"]\mathsf{B}
    \end{tikzcd}.
  \end{equation*}
\end{notation}

Let us check in some detail that a comma category really is a category. To do so, we need to check the three properties listed in \hyperref[def:category]{Definition \ref*{def:category}}
\begin{enumerate}
  \item We must be able to compose morphisms, i.e. we must have the following diagram.
    \begin{equation*}
      \begin{tikzcd}
        {(\alpha, \beta, f)} \arrow[r, "{(g,h)}"] \arrow[rr, bend left, "{(g', h') \circ (g, h)}"] & {(\alpha', \beta', f')} \arrow[r, "{(g', h')}"] & {(\alpha'', \beta'', f'')}
      \end{tikzcd}
    \end{equation*}
    But we certainly do, since by definition, each square of the following diagram commutes,
    \begin{equation*}
      \begin{tikzcd}
        \mathcal{S}(\alpha) \arrow[r, "\mathcal{S}(g)"] \arrow[d, swap, "f"] & \mathcal{S}(\alpha') \arrow[r, "\mathcal{S}(g')"] \arrow[d, swap, "f'"] & \mathcal{S}(a'') \arrow[d, "f''"] \\
        \mathcal{T}(\alpha) \arrow[r, "\mathcal{T}(h)"] & \mathcal{T}(\alpha') \arrow[r, "\mathcal{T}(h')"] & \mathcal{T}(a'') 
      \end{tikzcd}
    \end{equation*}
    so the square formed by taking the outside rectangle
    \begin{equation*}
      \begin{tikzcd}[column sep=huge]
        \mathcal{S}(\alpha) \arrow[d, swap, "f"] \arrow[r, "\mathcal{S}(g') \circ \mathcal{S}(g)"] & \mathcal{S}(\alpha'') \arrow[d, "f''"] \\
        \mathcal{T}(\alpha) \arrow[r, "\mathcal{T}(h') \circ \mathcal{T}(h)"] & \mathcal{T}(\alpha'') \\
      \end{tikzcd}
    \end{equation*}
    commutes. But $\mathcal{S}$ and $\mathcal{T}$ are functors, so 
    \begin{equation*}
      \mathcal{S}(g') \circ \mathcal{S}(g) = \mathcal{S}(g' \circ g),
    \end{equation*}
    and similarly for $\mathcal{T}(h' \circ h)$. Thus, the composition of morphisms is given via
    \begin{equation*}
      (g', h') \circ (g,h) = (g'\circ g, h' \circ h).
    \end{equation*}

  \item We can see from this definition that associativity in $(\mathcal{S}\downarrow \mathcal{T})$ follows from associativity in the underlying categories $\mathsf{A}$ and $\mathsf{B}$.

  \item The identity morphism is the pair $(1_{\mathcal{S}(\alpha)}, 1_{\mathcal{T}(\beta)})$. It is trivial from the definition of the composition of morphisms that this morphism functions as the identity morphism.
\end{enumerate}

\subsection{Slice categories}
A special case of a comma category, the so-called \emph{slice category}, occurs when $\mathsf{C} = \mathsf{A}$, $\mathcal{S}$ is the identity functor, and $\mathsf{B} = \mathsf{1}$, the category with one object and one morphism (\hyperref[eg:categorywithoneobject]{Example \ref*{eg:categorywithoneobject}}). 

\begin{definition}[slice category]
  \label{def:slicecategory}
  A \defn{slice category} is a comma category

  \begin{equation*}
    \begin{tikzcd}
      \mathsf{A} \arrow[r, rightsquigarrow, "\mathrm{id}_{A}"] & \mathsf{A} & \arrow[l, rightsquigarrow, swap, "\mathcal{T}"] \mathsf{1}
    \end{tikzcd}.
  \end{equation*}

  Let us unpack this prescription. Taking the definition literally, the objects in our category are triples $(\alpha, \beta, f)$, where $\alpha \in \Obj(\mathsf{A})$, $\beta \in \Obj(\mathsf{1})$, and $f \in \Hom_{\mathsf{A}}(\mathrm{id}_{\mathsf{A}}(\alpha), \mathcal{T}(\beta))$.

  There's a lot of extraneous information here, and our definition can be consolidated considerably. Since the functor $\mathcal{T}$ is given and $\mathsf{1}$ has only one object (call it $*$), the object $\mathcal{T}(*)$ (call it $X$) is singled out in $\mathsf{A}$. We can think of $\mathcal{T}$ as $\mathcal{F}_{X}$ (\hyperref[eg:functorfrom1category]{Example \ref*{eg:functorfrom1category}}). Similarly, since the identity morphism doesn't do anything interesting, Therefore, we can collapse the following diagram considerably. 
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}_{X}(*)
      \arrow[r, "\mathcal{F}_{X}(1_{*})"]
      \arrow[d, swap, "f"]
      & \mathcal{F}_{X}(*)
      \arrow[d, "f'"]
      \\
      \mathrm{id}_{A}(\alpha)
      \arrow[r, "\mathrm{id}_{A}(g)"]
      & \mathrm{id}_{A}(\alpha')
    \end{tikzcd}
  \end{equation*}
  The objects of a slice category therefore consist of pairs $(\alpha, f)$, where $\alpha \in \Obj(A)$ and 
  \begin{equation*}
    f: \alpha \to X;
  \end{equation*}
  the morphisms $(\alpha, f) \to (\alpha', f')$ consist of maps $g\colon \alpha \to \alpha'$. This allows us to define a slice category more neatly.

  Let $\mathsf{A}$ be a category, $X \in \Obj(\mathsf{A})$. The \defn{slice category} $(\mathsf{A}\downarrow X)$ is the category whose objects are pairs $(\alpha, f)$, where $\alpha \in \Obj(A)$ and $f\colon \alpha \to X$, and whose morphisms $(\alpha, f) \to (\alpha', f')$ are maps $g:\alpha \to \alpha'$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      \alpha \arrow[rr, "g"] \arrow[rd, swap, "f"] & & \alpha' \arrow[dl, "f'"] \\
      & X
    \end{tikzcd}
  \end{equation*}
  commutes.
\end{definition}

One can also define a coslice category, which is what you get when you take a slice category and turn the arrows around: coslice categories are \emph{dual} to slice categories.
\begin{definition}[coslice category]
  \label{def:coslicecategory}
  Let $\mathsf{A}$ be a category, $X \in \Obj(A)$. The \defn{coslice category} $(X \downarrow \mathsf{A})$ is the comma category given by the diagram
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{1} \arrow[r, rightsquigarrow, "\mathcal{F}_{X}"] & \mathsf{A} & \arrow[l, rightsquigarrow, swap, "\mathrm{id}_{A}"] \mathsf{A}
    \end{tikzcd}.
  \end{equation*}
  The objects are morphisms $f\colon X \to \alpha$ and the morphisms are morphisms $g\colon \alpha \to \alpha'$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      & X \arrow[dl, swap, "f"] \arrow[dr, "f'"] & \\
      \alpha \arrow[rr, swap, "g"] & & \alpha'
    \end{tikzcd}
  \end{equation*}
  commutes.
\end{definition}

\section{Universal properties} \label{sec:universalproperties}
\begin{note}
  It is assumed that the reader is familar with the basic idea of a universal property and has seen (but not necessarily understood) a few examples, such as the universal properties for products and tensor algebras.
\end{note}

In general, the `nicest' definition of a structure uses only the category-theoretic information about the category in which the structure lives; the definition of a product (of sets, for example) is best given without making use of hand-wavy statements like ``ordered pairs of an element here and an element there.'' The idea is similar to the situation in linear algebra, where it is aesthetically preferrable to avoid introducing an arbitrary basis every time one needs to prove something, instead using properties intrinsic to the vector space itself.

The easiest way to do this in general is by using the idea of a \emph{universal property}.

\begin{definition}[initial objects, final objects, zero objects]
  Let $\mathsf{C}$ be a category. An object $A \in \Obj(\mathsf{C})$ is said to be an
  \begin{itemize}
    \item \defn{initial object} if $\Hom(A,B)$ has exactly one element for all $B\in \Obj(\mathsf{C})$, i.e. if there is exactly one arrow from $A$ to every object in $\mathsf{C}$.

    \item \defn{final object} (or \emph{terminal object}) if $\Hom(B,A)$ has exactly one element for all $B\in \Obj(\mathsf{C})$, i.e. if there is exactly one arrow from every object in $\mathsf{C}$ to $A$.

    \item \defn{zero object} if it is both initial and final.
  \end{itemize}
  \label{def:initialfinalzeroobject}
\end{definition}

\begin{example}
  In $\mathsf{Set}$, there is exactly one map from any set $S$ to any one-element set $\{*\}$. Thus $\{*\}$ is a terminal object in $\mathsf{Set}$.

  Furthermore, it is conventional that there is exactly one map from the empty set $\emptyset$ to any set $B$. Thus the $\emptyset$ is initial in $\mathsf{Set}$.
\end{example}

\begin{example}
  The trivial group is a zero object in $\mathsf{Grp}$.
\end{example}

\begin{theorem}
  Let $\mathsf{C}$ be a category, let $I$ and $I'$ be two initial objects in $\mathsf{C}$. Then there exists a unique isomorphism between $I$ and $I'$.
  \label{thm:allinitialobjectsareuniquelyisomorphic}
\end{theorem}
\begin{proof}
  Since $I$ is initial, there exists exactly one morphism from $I$ to \emph{any} object, including $I$ itself. By \hyperref[item:existenceofidentitymorphism]{Definition \ref*{def:category}, Part \ref*{item:existenceofidentitymorphism}}, $I$ must have at least one map to itself: the identity morphism $1_{I}$. Thus, the only morphism from $I$ to itself is the identity morphism. Similarly, the only morphism from $I'$ to itself is also the identity morphism.

  But since $I$ is initial, there exists a unique morphism from $I$ to $I'$; call it $f$. Similarly, there exists a unique morphism from $I'$ to $I$; call it $g$. By \hyperref[item:compositionofmorphisms]{Definition \ref*{def:category}, Part \ref*{item:compositionofmorphisms}}, we can take the composition $g \circ f$ to get a morphism $I \to I$.

  But there is only one isomorphism $I \to I$: the identity morphism! Thus
  \begin{equation*}
    g \circ f = 1_{I}.
  \end{equation*}
  Similarly, 
  \begin{equation*}
    f \circ g  = 1_{I'}.
  \end{equation*}
  This means that, by \hyperref[def:isomorphism]{Definition \ref*{def:isomorphism}}, $f$ and $g$ are isomorphisms. They are clearly unique because of the uniqueness condition in the definition of an initial object. Thus, between any two initial objects there is a unique isomorphism, and we are done.

  Here is a bad picture of this proof.
  \begin{equation*}
    \begin{tikzcd}
      I \arrow[loop left, "1_{I} \,=\, g \circ f"] \arrow[r, bend left, "f"] & I' \arrow[l, bend left, "g"] \arrow[loop right, "1_{I'} \,=\, f \circ g"]
    \end{tikzcd}
  \end{equation*}
\end{proof}

\begin{definition}[category of morphisms from an object to a functor]
  \label{def:categoryofmorphismsfromanobjecttoafunctor}
  Let $\mathsf{C}$, $\mathsf{D}$ be categories, let $\mathcal{U}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ be a functor. Further let $X \in \Obj(C)$. The \defn{category of morphisms $(X \downarrow \mathcal{U})$} is the following comma category (see \hyperref[def:commacategory]{Definition \ref*{def:commacategory}}):
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{1} \arrow[r, rightsquigarrow, "\mathcal{F}_{X}"] & \mathsf{C} & \arrow[l, swap, rightsquigarrow, "\mathcal{U}"] \mathsf{D}
    \end{tikzcd}.
  \end{equation*}
  Just as for (co)slice categories (\hyperref[def:slicecategory]{Definitions \ref*{def:slicecategory} and \ref*{def:coslicecategory}}), there is some unpacking to be done. In fact, the unpacking is very similar to that of coslice categories. The LHS of the commutative square diagram collapses because the functor $\mathcal{F}_{X}$ picks out a single element $X$; therefore, the objects of $(X \downarrow \mathcal{U})$ are ordered pairs
  \begin{equation*}
    (\alpha, f);\qquad\alpha \in \Obj(\mathsf{D}),\quad f\colon X \to \mathcal{U}(\alpha),
  \end{equation*}
  and the morphisms $(\alpha, f) \to (\alpha', f')$ are morphisms $g\colon \alpha \to \alpha'$ such that the diagram

  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      & X \arrow[dl, swap, "f"] \arrow[dr, "f'"] & \\
      \mathcal{U}(\alpha) \arrow[rr, swap, "\mathcal{U}(g)"] & & \mathcal{U}(\alpha')
    \end{tikzcd}
  \end{equation*}
  commutes.

\end{definition}

Just as slice categories are dual to coslice categories, we can take the dual of the previous definition.

\begin{definition}[category of morphisms from a functor to an object]
  \label{def:categoryofmorphismsfromafunctortoanobject}
  Let $\mathsf{C}$, $\mathsf{D}$ be categories, let $\mathcal{U}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ be a functor. The \defn{category of morphisms $(\mathcal{U} \downarrow X)$} is the comma category
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{D} \arrow[r, rightsquigarrow, "\mathcal{U}"] & \mathsf{C} & \arrow[l, rightsquigarrow, swap, "\mathcal{F}_X"] \mathsf{1}
    \end{tikzcd}.
  \end{equation*}
  The objects in this category are pairs $(\alpha, f)$, where $\alpha \in \Obj(\mathsf{D})$ and $f\colon \mathcal{U}(\alpha) \to X$. The morphisms $(\alpha, f) \to (\alpha', f')$ are morphisms $g\colon \alpha \to \alpha'$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      \mathcal{U}(\alpha) \arrow[rr, "\mathcal{U}(g)"] \arrow[dr, swap, "f"] & & \mathcal{U}(\alpha') \arrow[dl, "f'"] \\
      & X &
    \end{tikzcd}
  \end{equation*}
  commutes.

\end{definition}

\begin{definition}[initial morphism]
  \label{def:initialmorphism}
  Let $\mathsf{C}$, $\mathsf{D}$ be categories, let $\mathcal{U}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ be a functor, and let $X \in \Obj(\mathsf{C})$. An \defn{initial morphism} (called a \emph{universal arrow} in \cite{maclane-categories}) is an initial object in the category $(X \downarrow \mathcal{U})$, i.e. the comma category which has the diagram
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{1} \arrow[r, rightsquigarrow, "\mathcal{F}_{X}"] & \mathsf{C} & \mathsf{D} \arrow[l, swap, rightsquigarrow, "\mathcal{U}"]
    \end{tikzcd}
  \end{equation*}
\end{definition}
This is not by any stretch of the imagination a transparent definition, but decoding it will be good practice. 

The definition tells us that an initial morphism is an object in $(X \downarrow \mathcal{U})$, i.e. a pair $(I, \varphi)$ for $I \in \Obj(\mathsf{D})$ and $\varphi\colon X \to \mathcal{U}(\alpha)$. But it is not just any object: it is an initial object. This means that for any other object $(\alpha, f)$, there exists a unique morphism $(I, \varphi) \to (\alpha, f)$.
But such morphisms are simply maps $g\colon I \to \alpha$ such that the diagram
\begin{equation*}
  \begin{tikzcd}[column sep=tiny, row sep=6ex]
    & X \arrow[dl, swap, "\varphi"] \arrow[dr, "f"] & \\
    \mathcal{U}(I) \arrow[rr, swap, "\mathcal{U}(g)"] & & \mathcal{U}(\alpha)
  \end{tikzcd}
\end{equation*}
commutes.

We can express this schematically via the following diagram (which is essentially the above diagram, rotated to agree with the literature).
\begin{equation*}
  \begin{tikzcd}[row sep=large]
    X \arrow[r, "\varphi"] \arrow[dr, swap, "f"] & \mathcal{U}(I) \arrow[d, "\mathcal{U}(g)"] & & I \arrow[d, "\exists!g"]\\
    & \mathcal{U}(\alpha) & & \alpha
  \end{tikzcd}
\end{equation*}

As always, there is a dual notion.
\begin{definition}[terminal morphism]
  \label{def:terminalmorphism}
  Let $\mathsf{C}$, $\mathsf{D}$ be categories, let $\mathcal{U}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ be a functor, and let $X \in \Obj(\mathsf{C})$. A \defn{terminal morphism} is a terminal object in the category $(\mathcal{U} \downarrow X)$.
\end{definition}

This time ``terminal object'' means a pair $(I, \varphi)$ such that for any other object $(\alpha, f)$ there is a unique morphism $g\colon \alpha \to I$ such that the diagram
\begin{equation*}
  \begin{tikzcd}[column sep=tiny, row sep=6ex]
    \mathcal{U}(\alpha) \arrow[dr, swap, "f"] \arrow[rr, "\mathcal{U}(g)"] & & \mathcal{U}(I)\arrow[dl, "\varphi"] \\
    & X 
  \end{tikzcd}
\end{equation*}
commutes.

Again, with the diagram helpfully rotated, we have the following.
\begin{equation*}
  \begin{tikzcd}[row sep=large]
    \mathcal{U}(\alpha) \arrow[d, swap, "\mathcal{U}(g)"] \arrow[rd, "f"] & & & \alpha \arrow[d, "\exists!g"]\\
    \mathcal{U}(I) \arrow[r, swap, "\varphi"] & X & & I
  \end{tikzcd}
\end{equation*}
\begin{definition}[universal property]
  \label{def:universalproperty}
  There is no hard and fast definition of a universal property. In complete generality, an object with a universal property is just an object which is initial or terminal in some category. However, quite a few interesting universal properties are given in terms of initial and terminal morphisms, and it will pay to study a few examples.
\end{definition}

\begin{note}
  One often hand-wavily says that an object $I$ \defn{satisfies a universal property} if $(I, \varphi)$ is an initial or terminal morphism. This is actually rather annoying; one has to remember that when one states a universal property in terms of a universal morphism, one is defining not only an object $I$ but also a morphism $\varphi$, which is often left implicit.
\end{note}

\begin{example}[tensor algebra]
  \label{eg:tensoralgebra}
  One often sees some variation of the following universal characterization of the tensor algebra, which was taken (almost) verbatim from Wikipedia. We will try to stretch it to fit our definition, following the logic through in some detail.
  \begin{quote}
    Let $V$ be a vector space over a field $k$, and let $A$ be an algebra over $k$. The tensor algebra $T(V)$ satisfies the following universal property.

    Any linear transformation $f\colon V \to A$ from $V$ to $A$ can be uniquely extended to an algebra homomorphism $T(V) \to A$ as indicated by the following commutative diagram.
    \begin{equation*}
      \begin{tikzcd}[row sep=large]
        V \arrow[r, "i"] \arrow[dr, swap, "f"] & T(V) \arrow[d, "\tilde{f}"] \\
        & A
      \end{tikzcd}
    \end{equation*}
  \end{quote}

  As it turns out, it will take rather a lot of stretching.

  Let $\mathcal{U}\colon k\mhyp\mathsf{Alg} \rightsquigarrow \mathsf{Vect}_{k}$ be the forgetful functor which assigns to each algebra over a field $k$ its underlying vector space. Pick some $k$-vector space $V$. We consider the category $(V \downarrow \mathcal{U})$, which is given by the following diagram.
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{1} \arrow[r, rightsquigarrow, "\mathcal{F}_{V}"] & \mathsf{Vect}_{k} &\arrow[l, swap, rightsquigarrow, "\mathcal{U}"] k\mhyp\mathsf{Alg} 
    \end{tikzcd}
  \end{equation*}

  By \hyperref[def:initialmorphism]{Definition \ref*{def:initialmorphism}}, the objects of $(V \downarrow \mathcal{U})$ are pairs $(A, L)$, where $A$ is a $k$-algebra and $L$ is a linear map $V \to \mathcal{U}(A)$. The morphisms are algebra homomorphisms $\rho\colon A \to A'$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      & V \arrow[dl, swap, "L"] \arrow[dr, "L'"] & \\
      \mathcal{U}(A) \arrow[rr, swap, "\mathcal{U}(\rho)"] & & \mathcal{U}(A')
    \end{tikzcd}
  \end{equation*}
  commutes. An object $(T(V), i)$ is initial if for any object $(A, f)$ there exists a unique morphism $g\colon T(V) \to A$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}[row sep=large]
      V \arrow[r, "i"] \arrow[dr, swap, "L"] & \mathcal{U}(T(V)) \arrow[d, "\mathcal{U}(g)"] & & T(V) \arrow[d, "\exists!g"] \\
      & \mathcal{U}(A) & & A
    \end{tikzcd}
  \end{equation*}
  commutes.

  Thus, the pair $(i, T(V))$ is the initial object in the category $(V \downarrow \mathcal{U})$. We called $T(V)$ the \emph{tensor algebra} over $V$. 

  But what is $i$? Notice that in the Wikipedia definition above, the map $i$ is from $V$ to $T(V)$, but in the diagram above, it is from $V$ to $\mathcal{U}(T(V))$. What gives?

  The answer that the diagram in Wikipedia's definition does not take place in a specific category. Instead, it implicitly treats $T(V)$ only as a vector space. But this is exactly what the functor $\mathcal{U}$ does.
\end{example}

\begin{example}[tensor product]
  \label{eg:universalpropertyoftensorproduct}
  According to the excellent book \cite{sontz-principal-bundles-classical}, the tensor product satisfies the following universal property.
  \begin{quote}
    Let $V_{1}$ and $V_{2}$ be vector spaces. Then we say that a vector space $V_{3}$ together with a bilinear map $\iota\colon V_{1} \times V_{2} \to V_{3}$ has the \emph{universal property} provided that for any bilinear map $B\colon V_{1} \times V_{2} \to W$, where $W$ is also a vector space, there exists a unique linear map $L\colon V_{3} \to W$ such that $B = L\iota$. Here is a diagram describing this `factorization' of $B$ through $\iota$:
    \begin{equation*}
      \begin{tikzcd}
        V_{1} \times V_{2} \arrow[r, "\iota"] \arrow[rd, swap, "B"] & V_{3} \arrow[d, "L"] \\
        & W
      \end{tikzcd}
    \end{equation*}
  \end{quote}

  It turns out that the tensor product defined in this way is neither an initial or final morphism. This is because in the category $\mathsf{Vect}_{k}$, there is no way of making sense of a bilinear map. 
\end{example}

\begin{example}[categorical product]
  \label{eg:universalpropertyofproducts}
  Here is the universal property for a product, taken verbatim from Wikipedia (\cite{wikipedia-product}).
  \begin{quote}
    Let $\mathsf{C}$ be a category with some objects $X_{1}$ and $X_{2}$. A product of $X_{1}$ and $X_{2}$ is an object $X$ (often denoted $X_{1}\times X_{2}$) together with a pair of morphisms $\pi_{1}\colon X \to X_{1}$ and $\pi_{2}\colon X \to X_{2}$ such that for every object $Y$ and pair of morphisms $f_{1}\colon Y \to X_{1}$, $f_{2}\colon Y \to X_{2}$, there exists a unique morphism $f\colon Y \to X_{1} \times X_{2}$ such that the following diagram commutes.
    \begin{equation*}
      \begin{tikzcd}
        & Y \arrow[dl, swap, "f_{1}"] \arrow[d, "f"] \arrow[dr, "f_{2}"] & \\
        X_{1} & X_{1} \times X_{2} \arrow[l, "\pi_{1}"] \arrow[r, swap, "\pi_{2}"] & X_{2}
      \end{tikzcd}
    \end{equation*}
  \end{quote}
  Consider the comma category $(\Delta \downarrow (X_{1},X_{2}))$ (where $\Delta$ is the diagonal functor, see \hyperref[eg:diagonalfunctor]{Example \ref*{eg:diagonalfunctor}}) given by the following diagram.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C} \arrow[r, rightsquigarrow, "\Delta"] & \mathsf{C}\times\mathsf{C} & \mathsf{1} \arrow[l, swap, rightsquigarrow, "\mathcal{F}_{(X_{1},X_{2})}"]
    \end{tikzcd}
  \end{equation*}
  The objects of this category are pairs $(A, (s,t))$, where $A \in \Obj(\mathsf{C})$ and 
  \begin{equation*}
    (s,t)\colon \Delta(A) = (A,A) \to (X_{1}, X_{2}).
  \end{equation*}

  The morphisms $(A, (s,t)) \to (B, (u,v))$ are morphisms $r\colon A \to B$ such that the diagram 
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny, row sep=6ex]
      (A,A) \arrow[rr, "{(r, r)}"] \arrow[rd, swap, "{(s,t)}"] & & (B,B) \arrow[dl, "{(u,v)}"] \\
      & (X_{1}, X_{2}) & 
    \end{tikzcd}
  \end{equation*}
  commutes.

  An object $(X_{1}\times X_{2}, (\pi_{1}, \pi_{2}))$ is final if for any other object $(Y, (f_{1},f_{2}))$, there exists a unique morphism $f\colon Y \to X_{1} \times X_{2}$ such that the diagram 
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      (Y, Y) \arrow[d, swap, "{(f, f)}"] \arrow[rd, "{(f_{1}, f_{2})}"] & & Y \arrow[d, "\exists!f"]\\
      (X_{1}\times X_{2}, X_{1}\times X_{2}) \arrow[r, swap, "{(\pi_{1}, \pi_{2})}"] &  (X_{1}, X_{2})& X_{1}\times X_{2}
    \end{tikzcd}
  \end{equation*}
  commutes. If we re-arrange our diagram a bit, it is not too hard to see that it is equivalent to the one given above. Thus, we can say: a product of two sets $X_{1}$ and $X_{2}$ is a final object in the category $(\Delta \downarrow (X_{1}, X_{2}))$
\end{example}

\section{Cartesian closed categories} \label{sec:cartesianclosedcategories}
This section loosely follows \cite{awodey-category-theory-foundations-videos}. 

Cartesian closed categories are the prototype for many of the structures possessed by monoidal categories. They are interesting in their own right, but will not be essential in what follows.

\subsection{Products}
We saw in \hyperref[eg:universalpropertyofproducts]{Example \ref*{eg:universalpropertyofproducts}} the definition for a categorical product. In some categories, we can naturally take the product of any two objects; one generally says that such a category \emph{has products}. We formalize that in the following.

\begin{definition}[category with products]
  \label{def:categorywithproducts}
  Let $\mathsf{C}$ be a category such that for every two objects $A$, $B \in \Obj(\mathsf{C})$, there exists an object $A \times B$ which satisfies the universal property (\hyperref[eg:universalpropertyofproducts]{Example \ref*{eg:universalpropertyofproducts}}). Then we say that $\mathsf{C}$ \defn{has products}.
\end{definition}
\begin{note}
  Sometimes people call a category with products a \emph{Cartesian category}, but others use this terminology to mean a category with all finite limits.\footnote{We will see later that any category with both products and equalizers has all finite limits.} We will avoid it altogether. 
\end{note}

\begin{theorem}
  \label{thm:productisafunctor}
  Let $\mathsf{C}$ be a category with products. Then the product can be extended to a bifunctor (\hyperref[def:bifunctor]{Definition \ref*{def:bifunctor}}) $\mathsf{C} \times \mathsf{C} \to \mathsf{C}$.
\end{theorem}
\begin{proof}
  Let $X$, $Y \in \Obj(C)$. We need to check that the assignment $(X,Y) \mapsto \times(X,Y) \equiv X \times Y$ is functorial, i.e. that $\times$ assigns
  \begin{itemize}
    \item to each pair $(X,Y) \in \Obj(\mathsf{C}\times\mathsf{C})$ an object $X\times Y \in \Obj(\mathsf{C})$ and 
    \item to each pair of morphisms $(f,g)\colon (X,Y) \to (X',Y')$ a morphism
      \begin{equation*}
        \times(f,g) = f \times g\colon X\times Y \to X' \times Y'
      \end{equation*}
      such that
      \begin{itemize}
        \item $1_{X} \times 1_{Y} = 1_{X \times Y}$, and
        \item $\times$ respects composition as follows.
          \begin{equation*}
            \begin{tikzcd}[column sep=large, row sep=small]
              {(X,Y)} \arrow[r, "{(f,g)}"] \arrow[rr, bend left, "{(f',g') \circ (f,g) = (f'\circ f, g'\circ g)}"] & {(X',Y')} \arrow[r, "{(f',g')}"] &  {(X'',Y'')} \\
              & \,\arrow[dd, rightsquigarrow, "\times"] & \\
              & \, & \\
              & \, & \\
              X\times Y \arrow[r, "f\times g"] \arrow[rr, bend right, swap, "f'\times g' \circ f \times g = (f'\circ f)\times(g'\circ g)"] & X' \times Y' \arrow[r, "f'\times g'"]& X''\times Y''
            \end{tikzcd}
          \end{equation*}
      \end{itemize}
  \end{itemize}
\end{proof}
We know how $\times$ assigns objects in $\mathsf{C} \times \mathsf{C}$ to objects in $\mathsf{C}$. We need to figure out how $\times$ should assign to a pair of $(f,g)$ a morphism $f\times g$. We do this by diagram chasing.

Suppose we are given two maps $f\colon X_{1} \to X_{2}$ and $g\colon Y_{1} \to Y_{2}$. We can view this as a morphism $(f,g)$ in $\mathsf{C} \times \mathsf{C}$.

Recall the universal property for products: a product $X_{1}\times Y_{1}$ is a final object in the category $(\Delta \downarrow (X_{1}, Y_{1}))$. Objects in this category can be thought of as diagrams in $\mathsf{C} \times \mathsf{C}$.
\begin{equation*}
  \begin{tikzcd}[column sep=huge]
    {(X_{1}\times Y_{1}, X_{1}\times Y_{1})} \arrow[r, "{\pi_{1}, \pi_{2}}"] & {(X_{1}, Y_{1})}
  \end{tikzcd}.
\end{equation*}
By assumption, we can take the product of both $X_{1}$ and $Y_{1}$, and $X_{2}$ and $Y_{2}$. This gives us two diagrams living in $\mathsf{C} \times \mathsf{C}$, which we can put next to each other. 
\begin{equation*}
  \begin{tikzcd}[column sep=huge]
    {(X_{1} \times Y_{1}, X_{1} \times Y_{1})} \arrow[r, "{(\pi_{1}, \pi_{2})}"] & (X_{1}, Y_{1}) \\
    {(X_{2} \times Y_{2}, X_{2} \times Y_{2})} \arrow[r, "{(\rho_{1}, \rho_{2})}"] & (X_{2}, Y_{2}) \\
  \end{tikzcd}
\end{equation*}
We can draw in our morphism $(f,g)$, and take its composition with $(\pi_{1}, \pi_{2})$.
\begin{equation*}
  \begin{tikzcd}[row sep=huge, column sep=huge]
    {(X_{1} \times Y_{1}, X_{1} \times Y_{1})} \arrow[r, "{(\pi_{1}, \pi_{2})}"] \arrow[dr, swap, "{(f\circ \pi_{1}, g \circ \pi_{2})}"] & (X_{1}, Y_{1}) \arrow[d, "{(f,g)}"] \\
    {(X_{2} \times Y_{2}, X_{2} \times Y_{2})} \arrow[r, swap, "{(\rho_{1}, \rho_{2})}"] & (X_{2}, Y_{2}) 
  \end{tikzcd}
\end{equation*}
Now forget about the top right of the diagram. I'll erase it to make this easier.
\begin{equation*}
  \begin{tikzcd}[row sep=huge, column sep=huge]
    {(X_{1} \times Y_{1}, X_{1} \times Y_{1})}  \arrow[dr, "{(f\circ \pi_{1}, g \circ \pi_{2})}"] & \\
    {(X_{2} \times Y_{2}, X_{2} \times Y_{2})} \arrow[r, swap, "{(\rho_{1}, \rho_{2})}"] & (X_{2}, Y_{2})
  \end{tikzcd}
\end{equation*}
The universal property for products says that there exists a unique map $h\colon X_{1}\times Y_{1} \to X_{2}\times Y_{2}$ such that the diagram below commutes.
\begin{equation*}
  \begin{tikzcd}[row sep=huge, column sep=huge]
    (X_{1} \times Y_{1}, X_{1} \times Y_{1}) \arrow[d, swap, "{(h, h)}"] \arrow[rd, "{(f \circ \pi_{1}, g \circ \pi_{2})}"] & & X_{1}\times Y_{1} \arrow[d, "\exists!h"]\\
    (X_{2}\times Y_{2}, X_{2}\times Y_{2}) \arrow[r, swap, "{(\rho_{1}, \rho_{2})}"] &  (X_{1}, X_{2})& X_{2}\times Y_{2}
  \end{tikzcd}
\end{equation*}
And $h$ is what we will use for the product $f \times g$.

Of course, we must also check that $h$ behaves appropriately. Draw two copies of the diagram for the terminal object in $(\Delta\downarrow (X,Y))$ and identity arrows between them.
\begin{equation*}
  \begin{tikzcd}[row sep=huge, column sep=huge]
    {(X\times Y, X \times Y)} \arrow[r, "{(\pi_{1}, \pi_{2})}"] \arrow[d, swap, "{(1_{X \times Y}, 1_{X \times Y})}"] & (X, Y) \arrow[d, "{(1_{X}, 1_{Y})}"] \\
    {(X\times Y, X \times Y)} \arrow[r, swap, "{(\pi_{1}, \pi_{2})}"] & (X, Y)
  \end{tikzcd}
\end{equation*}
Just as before, we compose the morphisms to and from the top right to draw a diagonal arrow, and then erase the top right object and the arrows to and from it.
\begin{equation*}
  \begin{tikzcd}[row sep=huge, column sep=huge]
    {(X\times Y, X \times Y)}  \arrow[d, swap, "{(1_{X \times Y},\, 1_{X \times Y}) = (1_{X}\times 1_{Y},\, 1_{X}\times 1_{Y})}"] \arrow[dr, "{(1_{X} \circ \pi_{1},\, 1_{Y} \circ \pi_{2})}"] & & X\times Y \arrow[d, "1_{X}\times1_{Y} = 1_{X}\times1_{Y}"] \\
    {(X\times Y, X \times Y)} \arrow[r, swap, "{(\pi_{1}, \pi_{2})}"] & (X, Y) & X\times Y
  \end{tikzcd}
\end{equation*}
By definition, the arrow on the right is $1_{X}\times1_{Y}$. It is also $1_{X\times Y}$, and by the universal property it is unique. Therefore $1_{X\times Y} = 1_{X}\times 1_{Y}$.

Next, put three of these objects together. The proof of the last part is immediate.
\begin{equation*}
  \begin{tikzcd}[column sep=huge, row sep=huge]
    {(X_{1} \times Y_{1}, X_{1} \times Y_{1})}  \arrow[d, "{(f\times g, f\times g)}"] \arrow[dd, bend right=70, swap, "{(f'\times g') \circ (f\times g)} = (g' \circ g) \times (f' \circ f)"]  & (X_{1}, Y_{1}) \arrow[d, swap, "{(f,g)}"] \arrow[dd, bend left=70, "{(g'\circ g, f' \circ f)}"] \\
    {(X_{2} \times Y_{2}, X_{2} \times Y_{2})}\arrow[d, "{(f'\times g', f'\times g')}"]  & (X_{2}, Y_{2}) \arrow[d, swap, "{(f',g')}"] \\
    {(X_{3} \times Y_{3}, X_{3} \times Y_{3})}  & (X_{3}, Y_{3})
  \end{tikzcd}
\end{equation*}
The meaning of this theorem is that in any category where you can take products of the objects, you can also take products of the morphisms.

\begin{example}
  The category $\mathsf{Vect}_{k}$ of vector spaces over a field $k$ has the direct sum $\oplus$ as a product. 
\end{example}

The product is, in an appropriate way, commutative.
\begin{lemma}
  There is a natural isomorphism between the following functors $\mathsf{C} \times \mathsf{C} \to \mathsf{C}$
  \begin{equation*}
    \times\colon (A, B) \to A \times B\qquad\text{and}\qquad \tilde{\times}\colon (A, B) \to B \times A.
  \end{equation*}
\end{lemma}
\begin{proof}
  We define a natural transformation $\Phi\colon \times \Rightarrow \tilde{times}$ whose components are 
  \begin{equation*}
    \Phi_{A, B}\colon A \times B \to B \times A
  \end{equation*}
  as follows. Denote the canonical projections for the product $A \times B$ by $\pi_{A}$ and $\pi_{B}$. Then $(\pi_{B}, \pi_{A})$ is a map $A \times B \to (B, A)$, and the universal property for products gives us a map $A \times B \to B \times A$. We can pull the same trick to go from $B \times A$ to $A \times B$, using the pair $(\pi_{A}, \pi_{B})$ and the universal property. Furthermore, these maps are inverse to each other, so $\Phi_{A, B}$ is an isomorphism.

  We need only check naturality, i.e. that for $f\colon A \to A'$, $g\colon B \to B'$, the following square commutes.
  \begin{equation*}
    \begin{tikzcd}
      A \times B
      \arrow[r]
      \arrow[d, swap]
      & A' \times B'
      \arrow[d]
      \\
      B \times A
      \arrow[r]
      & B' \times A'
    \end{tikzcd}
  \end{equation*}
\end{proof}

\begin{note}
  It is an important fact that the product is associative. We shall see this in \hyperref[thm:categoricalproductisassociative]{Theorem \ref*{thm:categoricalproductisassociative}}, after we have developed the machinery to do it cleanly.
\end{note}

\subsection{Coproducts}
\begin{definition}
  Let $\mathsf{C}$ be a category, $X_{1}$ and $X_{2} \in \Obj(\mathsf{C})$. The \defn{coproduct} of $X_{1}$ and $X_{2}$, denoted $X_{1} \amalg X_{2}$, is the initial object in the category $((X_{1},X_{2})\downarrow \Delta)$. In everyday language, we have the following.
  \begin{quote}
    An object $X_{1} \amalg X_{2}$ is called the coproduct of $X_{1}$ and $X_{2}$ if 
    \begin{enumerate}
      \item there exist morphisms $i_{1}\colon X_{1} \to X_{1} \amalg X_{2}$ and $i_{2}\colon X_{2} \to X_{1} \amalg X_{2}$ called \emph{canonical injections} such that

      \item for any object $Y$ and morphisms $f_{1}\colon X_{1} \to Y$ and $f_{2}\colon X_{2} \to Y$ there exists a unique morphism $f\colon X_{1} \amalg X_{2} \to Y$ such that $f_{1} = f \circ i_{1}$ and $f = f_{2} = f \circ i_{2}$, i.e. the following diagram commutes.
        \begin{equation*}
          \begin{tikzcd}
            & Y & \\
            X_{1} \arrow[ur, "f_{1}"] \arrow[r, swap, "i_{1}"] & X_{1} \arrow[u, "f"] \amalg X_{2} & X_{2} \arrow[l, "i_{2}"] \arrow[ul, swap, "f_{2}"]
          \end{tikzcd}
        \end{equation*}
    \end{enumerate}
  \end{quote}
\end{definition}

\begin{definition}[category with coproducts]
  \label{def:categorywithcoproducts}
  We say that a category $\mathsf{C}$ \defn{has coproducts} if for all $A$, $B \in \Obj(C)$, the coproduct $A \amalg B$ is in $\Obj(C)$.
\end{definition} 

We have the following analog of \hyperref[thm:productisafunctor]{Theorem \ref*{thm:productisafunctor}}.
\begin{theorem}
  \label{thm:coproductisafunctor}
  Let $C$ be a category with coproducts. Then we have a functor $\amalg\colon C \times C \rightsquigarrow C$.
\end{theorem}
\begin{proof}
  We verify that $\amalg$ allows us to define canonically a coproduct of morphisms. Let $X_{1}$, $X_{2}$, $Y_{1}$, $Y_{2} \in \Obj(C)$, and let $f\colon X_{1} \to Y_{1}$ and $g\colon X_{2} \to Y_{2}$. We have the following diagram.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      X_{1} \arrow[d, swap, "f"] \arrow[dr, "{i_{i,Y} \circ f}"] \arrow[r, "{i_{1, X}}"]& X_{1} \amalg X_{2} & X_{2} \arrow[dl, swap, "{i_{2, Y} \circ g}"] \arrow[ d, "g"] \arrow[l, swap, "{i_{2, X}}"] \\
      Y_{1} \arrow[r, "{i_{1, Y}}"]& Y_{1} \amalg Y_{2} & Y_{2} \arrow[l, swap, "{i_{2, Y}}"]
    \end{tikzcd}
  \end{equation*}
  But by the universal property of coproducts, the diagonal morphisms induce a map $X_{1} \amalg X_{2} \to Y_{1} \amalg Y_{2}$, which we define to be $f \amalg g$.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      X_{1} \arrow[dr, swap, "{i_{i,Y} \circ f}"] \arrow[r, "{i_{1, X}}"]& X_{1} \arrow[d, "f \amalg g"] \amalg X_{2} & X_{2} \arrow[dl, "{i_{2, Y} \circ g}"] \arrow[l, swap, "{i_{2, X}}"] \\
      & Y_{1} \amalg Y_{2} &
    \end{tikzcd}
  \end{equation*}
  The rest of the verification that $\amalg$ really is a functor is identical to that in \hyperref[thm:productisafunctor]{Theorem \ref*{thm:productisafunctor}}.
\end{proof}

\begin{example}
  In $\mathsf{Set}$, the coproduct is the disjoint union. 
\end{example}

\begin{example}
  In $\mathsf{Vect}_{k}$, the coproduct $\oplus$ is the direct sum.
\end{example}

\begin{example}
  In $\mathsf{Alg}_{k}$ the tensor product is the coproduct. To see this, let $A$, $B$, and $R$ be $k$-algebras. The tensor product $A \otimes_{k} B$ has canonical injections $\iota_{1}\colon A \to A \otimes B$ and $\iota_{2}\colon B \to A \otimes B$ given by
  \begin{equation*}
    \iota_{A}\colon v \to v \otimes 1_{B}\qquad\text{and}\qquad \iota_{B}\colon v \mapsto 1 \otimes v.
  \end{equation*}

  Let $f_{1}\colon A \to R$ and $f_{2}\colon B \to R$ be $k$-algebra homomorphisms. Then there is a unique homomorphism $f\colon A \otimes B \to R$ which makes the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[r, "\iota_{A}"]
      \arrow[dr, swap, "f_{1}"]
      & A \otimes B
      \arrow[d, "f"]
      & B
      \arrow[l, swap, "\iota_{B}"]
      \arrow[dl, "f_{2}"]
      \\
      & R
    \end{tikzcd}
  \end{equation*}
\end{example}

\subsection{Biproducts} \label{sec:biproducts}
A lot of the stuff in this section was adapted and expanded from \cite{annoying-precision-meditation}.

One often says that a biproduct is an object which is both a product and a coproduct, but this is both misleading and unnecessarily vague. Objects satisfying universal properties are only defined up to isomorphism, so it doesn't make much sense to demand that products \emph{coincide} with coproducts. However, demanding only that they be isomorphic (or even naturally isomorphic) is too weak, and does not determine a biproduct uniquely. In this section we will give the correct definition which encapsulates all of the properties of the product and the coproduct we know and love.

In order to talk about biproducts, we need to be aware of the following result, which we will consider in much more detail in \hyperref[sec:preabeliancategories]{Definition \ref*{sec:preabeliancategories}}.

\begin{definition}[zero morphism]
  Let $\mathsf{C}$ be a category with zero object $0$. For any two objects $A$, $B \in \Obj(\mathsf{C})$, the \defn{zero morphism} $0_{A,B}$ is the unique morphism $A \to B$ which factors through $0$.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[rr, bend left, "0_{A, B}"]
      \arrow[r]
      & 0
      \arrow[r]
      & B
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{notation}
  It will often be clear what the source and destination of the zero morphism are; in this case we will drop the subscripts, writing $0$ instead of $0_{AB}$. With this notation, for all morphisms $f$ and $g$ we have $f \circ 0 = 0$ and $0 \circ g = 0$.
\end{notation}

\begin{definition}[category with biproducts]
  \label{def:categorywithbiproducts}
  Let $\mathsf{C}$ be a category with zero object $0$, products $\times$ (with canonical projections $\pi$), and coproducts $\amalg$ (with natural injections $\iota$). We say that $\mathsf{C}$ \defn{has biproducts} if for each two objects $A_{1}$, $A_{2} \in \Obj(\mathsf{C})$, there exists an isomorphism
  \begin{equation*}
    \Phi_{A_{1}, A_{2}}\colon A_{1} \amalg A_{2} \to A_{1} \times A_{2}
  \end{equation*}
  such that 
  \begin{equation*}
    \begin{tikzcd}[column sep=large]
      A_{i}
      \arrow[r, "\iota_{i}"]
      & A_{1} \amalg A_{2} 
      \arrow[r, "{\Phi_{A_{1}, A_{2}}}"]
      & A_{1} \times A_{2}
      \arrow[r, "\pi_{j}"]
      & A_{j}
    \end{tikzcd}
    =
    \begin{cases}
      1_{A_{1}}, & i = j \\
      0, & i \neq j.
    \end{cases}
  \end{equation*}
  Here is a picture.
  \begin{equation*}
    \begin{tikzcd}
      & A_{1} \amalg A_{2} 
      \arrow[dd, "\Phi_{A_{1},A_{2}}"]
      \\
      A_{1} 
      \arrow[ur, "\iota_{1}"]
      & & A_{2}
      \arrow[ul, swap, "\iota_{2}"]
      \\
      & A_{1} \times A_{2}
      \arrow[ul, "\pi_{1}"]
      \arrow[ur, swap, "\pi_{2}"]
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{lemma}
  The isomorphisms $\Phi_{A, B}$ are unique if they exist. 
\end{lemma}
\begin{proof}
  We can compose $\Phi_{A_{1}, A_{2}}$ with $\pi_{1}$ and $\pi_{2}$ to get maps $A_{1} \amalg A_{2} \to A_{1}$ and $A_{1} \amalg A_{2} \to A_{2}$. The universal property for products tells us that there is a unique map $\varphi\colon A_{1} \amalg A_{2} \to A_{1} \times A_{2}$ such that $\pi_{1} \circ \varphi = \pi_{1} \circ \Phi_{A_{1}, A_{2}}$ and $\pi_{2} \circ \varphi = \pi_{2} \circ \Phi_{A_{1}, A_{2}}$; but $\varphi$ is just $\Phi_{A_{1}, A_{2}}$. Hence $\Phi_{A_{1}, A_{2}}$ is unique.
\end{proof}

\begin{lemma}
  \label{lemma:mapcoprodtoproddeterminedbycomponents}
  In any category $\mathsf{C}$ with biproducts $\Phi_{A, B}\colon A \amalg B \to A \times B$, any map $A \amalg B \to A' \times B'$ is completely determined by its components $A \to A'$, $A \to B'$, $B \to A'$, and $B \to B'$. That is, given any map $f\colon A \amalg B \to A' \times B'$, we can create four maps
  \begin{equation*}
    \pi_{A'} \circ f \circ \iota_{A}\colon A \to A', \qquad \pi_{B'} \circ f \circ \iota_{A}\colon A \to B',\qquad\text{etc},
  \end{equation*}
  and given four maps
  \begin{equation*}
    \psi_{A, A'}\colon A \to A';\qquad \psi_{A, B'}\colon A \to B';\qquad \psi_{B,A'}\colon B \to A',\qquad\text{and } \psi_{B,B'}\colon B \to B',
  \end{equation*}
  we can construct a unique map $\psi\colon A \amalg B \to A' \times B'$ such that
  \begin{equation*}
    \psi_{A, A'} = \pi_{A'} \circ \psi \circ \iota_{A},\qquad \psi_{A,B'} = \pi_{B'} \circ \psi \circ \iota_{A},\qquad\text{etc}.
  \end{equation*}
\end{lemma}
\begin{proof}
  The universal property for coproducts give allows us to turn the morphisms $\psi_{A,A'}$ and $\psi_{A, B'}$ into a map $\psi_{A}\colon A \to B \times B'$, the unique map such that 
  \begin{equation*}
    \pi_{A'} \circ \psi_{A} = \psi_{A,A'} \qquad\text{and}\qquad\pi_{B'} \circ \psi_{A} = \psi_{A,B'}.
  \end{equation*}

  The morphisms $\psi_{B,A'}$ and $\psi_{B,B'}$ give us a map $\psi_{B}$ which is unique in the same way.

  But now the universal property for coproducts gives us a map $\psi\colon A \amalg B \to A' \times B'$, the unique map such that $\psi \circ \iota_{A} = \psi_{A}$ and $\psi \circ \iota_{B} = \psi_{B}$.
\end{proof}

\begin{theorem}
  The $\Phi_{A, B}$ defined above form the components of a natural isomorphism.
\end{theorem}
\begin{proof}
  Let $A$, $B$, $A'$, and $B' \in \Obj(\mathsf{C})$, and let $f\colon A \to A'$ and $g\colon B \to B'$. To check that $\Phi_{A, B}$ are the components of a natural isomorphism, we have to check that the following naturality square commutes.
  \begin{equation*}
    \begin{tikzcd}[column sep=large, row sep=large]
      A \amalg B
      \arrow[r, "f \amalg g"]
      \arrow[d, swap, "{\Phi_{A, B}}"]
      & A' \amalg B'
      \arrow[d, "{\Phi_{A', B'}}"]
      \\
      A \times B
      \arrow[r, swap, "f \times g"]
      & A' \times B'
    \end{tikzcd}
  \end{equation*}
  That is, we need to check that $(f \times g) \circ \Phi_{A, B} = \Phi_{A', B'} \circ (f \amalg g)$.

  Both of these are morphisms $A \amalg B \to A' \times B'$, and by \hyperref[lemma:mapcoprodtoproddeterminedbycomponents]{Lemma \ref*{lemma:mapcoprodtoproddeterminedbycomponents}}, it suffices to check that their components agree, i.e. that 
\end{proof}


\begin{example}
  In the category $\mathsf{Vect}_{k}$ of vector spaces over a field $k$, the direct sum $\oplus$ is a biproduct.
\end{example}

\begin{example}
  In the category $\mathsf{Ab}$ of abelian groups, the direct sum $\oplus$ is both a product and a coproduct. Hence $\mathsf{Ab}$ has biproducts. 
\end{example} 

\subsection{Exponentials}
Astonishingly, we can talk about functional evaluation purely in terms of products and universal properties.

\begin{definition}[exponential]
  \label{def:exponential}
  Let $\mathsf{C}$ be a category with products, $A$, $B \in \Obj(\mathsf{C})$. The \defn{exponential} of $A$ and $B$ is an object $B^{A} \in \Obj(\mathsf{C})$ together with a morphism $\varepsilon\colon B^{A} \times A \to B$, called the \emph{evaluation morphism}, which satisfies the following universal property.

  \begin{quote}
    For any object $X \in \Obj(\mathsf{C})$ and morphism $f\colon X \times A \to B$, there exists a unique morphism $\bar{f}\colon X \to B^{A}$ which makes the following diagram commute.
    \begin{equation*}
      \begin{tikzcd}
        B^{A}
        & & B^{A} \times A
        \arrow[r, "\varepsilon"]
        & B
        \\
        X 
        \arrow[u, "\exists!\bar{f}"]
        & & X \times A
        \arrow[ur, swap, "f"]
        \arrow[u, "\bar{f} \times 1_{A}"]
      \end{tikzcd}
    \end{equation*}
  \end{quote}
\end{definition}

\begin{example}
  In $\mathsf{Set}$, the exponential object $B^{A}$ is the set of functions $A \to B$, and the evaluation morphism $\varepsilon$ is the map which assigns $(f, a) \mapsto f(a)$. Let us check that these objects indeed satisfy the universal property given.

  Suppose we are given a function $f\colon X \times A \to B$. We want to construct from this a function $\bar{f}\colon X \to B^{A}$. i.e. a function which takes an element $x \in X$ and returns a function $\bar{f}(x)\colon A \to B$. There is a natural way to do this: fill the first slot of $f$ with $x$! That is to say, define $\bar{f}(x) = f(x, -)$. It is easy to see that this makes the diagram commute:
  \begin{equation*}
    \begin{tikzcd}
      {(f(x,-), a)}
      \arrow[r, mapsto, "\varepsilon"]
      &{f(x, a)}
      \\
      {(x, a)}
      \arrow[u, mapsto, "\bar{f} \times 1_{A}"]
      \arrow[ur, mapsto, swap, "f"]
    \end{tikzcd}
  \end{equation*}
  Furthermore, $\bar{f}$ is unique since if it sent $x$ to any function other than $f$ the diagram would not commute.
\end{example}

\begin{example}
  One might suspect that the above generalizes to all sorts of categories. For example, one might hope that in $\mathsf{Grp}$, the exponential $H^{G}$ of two groups $G$ and $H$ would somehow capture all homomorphisms $G \to H$. This turns out to be impossible; we will see why in REF.
\end{example}

\subsection{Cartesian closed categories}
\begin{definition}[cartesian closed category]
  \label{def:cartesianclosedcategory}
  A category $\mathsf{C}$ is \defn{cartesian closed} if it has
  \begin{enumerate}
    \item products: for all $A$, $B \in \Obj(\mathsf{C})$, $A \times B \in \Obj(\mathsf{C})$;
    \item exponentials: for all $A$, $B \in \Obj(\mathsf{C})$, $B^{A} \in \Obj(\mathsf{C})$;
    \item a terminal element (\hyperref[def:initialfinalzeroobject]{Definition \ref*{def:initialfinalzeroobject}}) $1 \in \mathsf{C}$.
  \end{enumerate}
\end{definition}

Cartesian closed categories have many inveresting properties. For example, they replicate many properties of the integers: we have the following familiar formulae for any objects $A$, $B$, and $C$ in a Cartesian closed category with coproducts $+$.
\begin{enumerate} 
  \item $A \times (B + C) \simeq (A \times B) + (A \times C)$
  \item $C^{A + B} \simeq C^{A} \times C^{B}$.
  \item $(C^{A})^{B} \simeq C^{A \times B}$
  \item $(A \times B)^{C} \simeq A^{C} \times B^{C}$
  \item $C^{1} \simeq C$
\end{enumerate}

We could prove these immediately, by writing down diagrams and appealing to the universal properties. However, we will soon have a powerful tool, the Yoneda lemma, that makes their proof completely routine.

\section{Hom functors and the Yoneda lemma} \label{sec:homfunctorsandyoneda}
\subsection{The Hom functor} \label{section:homfunctor}
Given any locally small category (\hyperref[def:smalllocallysmallcategoryhomset]{Definition \ref*{def:smalllocallysmallcategoryhomset}}) $\mathsf{C}$ and two objects $A$, $B \in \Obj(\mathsf{C})$, we have thus far notated the set of all morphisms $A \to B$ by $\Hom_{\mathsf{C}}(A, B)$. This may have seemed a slightly odd notation, but there was a good reason for it: $\Hom_{\mathsf{C}}$ is really a functor.

To be more explicit, we can view $\Hom_{\mathsf{C}}$ in the following way: it takes two objects, say $A$ and $B$, and returns the set of morphisms $A \to B$. It's a functor $\mathsf{C} \times \mathsf{C}$ to set!

(Actually, as we'll see, this isn't quite right: it is actually a functor $\mathsf{C}^{\text{op}} \times \mathsf{C} \to \mathsf{Set}$. But this is easier to understand if it comes about in a natural way, so we'll keep writing $\mathsf{C} \times \mathsf{C}$ for the time being.)

That takes care of how it sends objects to objects, but any functor has to send morphisms to morphisms. How does it do that?

A morphism in $\mathsf{C} \times \mathsf{C}$ between two objects $(A', A)$ and $(B', B)$ is an ordered pair $(f', f)$ of two morphisms:
\begin{equation*}
  f\colon A \to B'\qquad\text{and}\qquad f\colon A' \to B',
\end{equation*}
where $A$, $B$, $A'$, and $B' \in \Obj(\mathsf{C})$. We can draw this as follows.
\begin{equation*}
  \begin{tikzcd}[row sep=tiny]
    A' 
    \arrow[r, "f'"]
    & B'
    \\
    A
    \arrow[r, "f"]
    & B
  \end{tikzcd}
\end{equation*}
We have to send this to a set-function $\Hom_{\mathsf{C}}(A', A) \to \Hom_{\mathsf{C}}(B', B)$. So let $m$ be a morphism in $\Hom_{\mathsf{C}}(A, A')$. We can draw this into our little diagram above like so.
\begin{equation*}
  \begin{tikzcd}
    A' 
    \arrow[r, "f'"]
    \arrow[d, swap, "m"]
    & B'
    \\
    A
    \arrow[r, "f"]
    & B
  \end{tikzcd}
\end{equation*}

Notice: we want to build from $m$, $f$, and $f'$ a morphism $B' \to B$. Suppose $f'$ were going the other way.
\begin{equation*}
  \begin{tikzcd}
    A' 
    \arrow[d, swap, "m"]
    & B'
    \arrow[l, swap, "f'"]
    \\
    A
    \arrow[r, "f"]
    & B
  \end{tikzcd}
\end{equation*}
Then we could get what we want simply by taking the composition $f \circ m \circ f'$.
\begin{equation*}
  \begin{tikzcd}
    A' 
    \arrow[d, swap, "m"]
    & B'
    \arrow[l, swap, "f'"]
    \arrow[d, "f \circ m \circ f'"]
    \\
    A
    \arrow[r, "f"]
    & B
  \end{tikzcd}
\end{equation*}
But we can make $f'$ go the other way simply by making the first argument of $\Hom_{\mathsf{C}}$ come from the opposite category: that is, we want $\Hom_{\mathsf{C}}$ to be a functor $\mathsf{C}^{\text{op}} \times \mathsf{C} \to \mathsf{Set}$.

As the function $m \mapsto f \circ m \circ f'$ is the action of the functor $\Hom_{\mathsf{C}}$ on the morphism $(f', f)$, it is natural to call it $\mathrm{Hom}_{\mathsf{C}}(f', f)$.

Of course, we should check that this \emph{really is} a functor, i.e. that it treats identities and compositions correctly. This is completely routine, and you should skip it if you read the last sentence with annoyance. 

First, we need to show that $\Hom_{\mathsf{C}}(1_{A'}, 1_{A})$ is the identity function $1_{\Hom_{\mathsf{C}}(A', A)}$. It is, since if $m \in \Hom_{\mathsf{C}}(A', A)$,
\begin{equation*}
  \Hom_{\mathsf{C}}(1_{A'}, 1_{A})\colon m \mapsto 1_{A} \circ m \circ 1_{A'} = m.
\end{equation*}

Next, we have to check that compositions work the way they're supposed to. Suppose we have objects and morphisms like this:
\begin{equation*}
  \begin{tikzcd}[row sep=tiny]
    A'
    & B'
    \arrow[l, swap, "f'"]
    & C'
    \arrow[l, swap, "g'"]
    \\
    A
    \arrow[r, "f"]
    & B
    \arrow[r, "g"]
    & C
  \end{tikzcd}
\end{equation*}
where the primed stuff is in $\mathsf{C}^{\mathrm{op}}$ and the unprimed stuff is in $\mathsf{C}$. This can be viewed as three objects $(A', A)$, etc. in $\mathsf{C}^{\mathrm{op}} \times \mathsf{C}$ and two morphisms  $(f', f)$ and $(g', g)$ between them.

Okay, so we can compose $(f', f)$ and $(g', g)$ to get a morphism 
\begin{equation*}
  (g', g) \circ (f', f) = (f' \circ g', g \circ f)\colon (A', A) \to (C', C).
\end{equation*}
Note that the order of the composition in the first argument has been turned around: this is expected since the first argument lives in $\mathsf{C}^{\mathrm{op}}$. To check that $\Hom_{\mathsf{C}}$ handles compositions correctly, we need to verify that 
\begin{equation*}
  \Hom_{\mathsf{C}}(g, g') \circ \Hom_{\mathsf{C}}(f, f') = \Hom_{\mathsf{C}}(f' \circ g', g \circ f).
\end{equation*}

Let $m \in \Hom_{\mathsf{C}}(A', A)$. We victoriously compute
\begin{align*}
  [\Hom_{\mathsf{C}}(g, g') \circ \Hom_{\mathsf{C}}(f, f')](m) &= \Hom_{\mathsf{C}}(g, g')(f \circ m \circ f') \\
  &= g \circ f \circ m \circ f' \circ g' \\
  &= \Hom_{\mathsf{C}}(f' \circ g', g \circ f).
\end{align*}

Let's formalize this in a definition.
\begin{definition}[hom functor]
  \label{def:homfunctor}
  Let $\mathsf{C}$ be a locally small category. The \defn{hom functor} $\Hom_{\mathsf{C}}$ is the functor 
  \begin{equation*}
    \mathsf{C}^{\mathrm{op}} \times \mathsf{C} \rightsquigarrow \mathsf{Set}
  \end{equation*}
  which sends the object $(A', A)$ to the set $\Hom_{\mathsf{C}}(A', A)$ of morphisms $A' \to A$, and the morphism $(f', f)\colon (A', A) \to (B', B)$ to the function
  \begin{equation*}
    \Hom_{\mathsf{C}}(f', f)\colon \Hom_{\mathsf{C}}(A', A) \to \Hom_{\mathsf{C}}(B', B);\qquad m \mapsto f \circ m \circ f'.
  \end{equation*}
\end{definition}

\begin{example}
  \label{eg:naturaltransformationsforcccs}
  Hom functors give us a new way of looking at the universal property for products, coproducts, and exponentials.

  Let $\mathsf{C}$ be a locally small category with products, and let $X$, $A$, $B \in \Obj(\mathsf{C})$. Recall that the universal property for the product $A \times B$ allows us to exchange two morphisms
  \begin{equation*}
    f_{1}\colon X \to A\qquad\text{and}\qquad f_{2}\colon X \to B
  \end{equation*}
  for a morphism
  \begin{equation*}
    f\colon X \to A \times B.
  \end{equation*}

  We can also compose a morphism $g\colon X \to A \times B$ with the canonical projections
  \begin{equation*}
    \pi_{A}\colon A \times B \to A \qquad\text{and}\qquad \pi_{B}\colon A \times B \to B
  \end{equation*}
  to get two morphisms
  \begin{equation*}
    \pi_{A} \circ f\colon X \to A\qquad\text{and}\qquad \pi_{B} \circ f\colon X \to B.
  \end{equation*}

  This means that there is a bijection 
  \begin{equation*}
    \Hom_{\mathsf{C}}(X, A \times B) \simeq \Hom_{\mathsf{C}}(X, A) \times \Hom_{\mathsf{C}}(X, B).
  \end{equation*}

  In fact this bijection is natural in $X$. That is to say, there is a natural bijection between the following functors $\mathsf{C}^{\mathrm{op}} \to \mathsf{Set}$:
  \begin{equation*}
    h_{A \times B}\colon X \to \Hom_{\mathsf{C}}(X, A \times B)\qquad\text{and}\qquad h_{A} \times h_{B}\colon X \to \Hom_{\mathsf{C}}(X, A) \times \Hom_{\mathsf{C}}(X, B).
  \end{equation*}

  Let's prove naturality. Let
  \begin{equation*}
    \Phi_{X}\colon \Hom(X, A \times B) \to \Hom(X, A) \times \Hom(X, B);\qquad f \mapsto (\pi_{A} \circ f, \pi_{B} \circ f)
  \end{equation*}
  be the components of the above transformation, and let $g\colon X' \to X$. Naturality follows from the fact that the diagram below commutes.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \Hom(X, A \times B)
      \arrow[rr, "{\Hom(g, 1_{A \times B})}"]
      \arrow[d, swap, "\Phi_{X}"]
      & & \Hom(X', A \times B)
      \arrow[d, "\Phi_{X'}"]
      \\
      \Hom(X, A) \times \Hom(X, B)
      \arrow[rr, "{\Hom(g, 1_{A}) \times \Hom(g, 1_{B})}"]
      & & \Hom(X', A) \times \Hom(X', B)
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      f
      \arrow[r, mapsto]
      \arrow[d, mapsto]
      & f \circ g
      \arrow[d, mapsto]
      \\
      (\pi_{A} \circ f, \pi_{B} \circ g)
      \arrow[r, mapsto]
      & (\pi_{A} \circ f \circ g, \pi_{B} \circ f \circ g)
    \end{tikzcd}
  \end{equation*}

  The coproduct does a similar thing: it allows us to trade two morphisms
  \begin{equation*}
    A \to X\qquad\text{and}\qquad B \to X
  \end{equation*}
  for a morphism
  \begin{equation*}
    A \amalg B \to X,
  \end{equation*}
  and vice versa. Similar reasoning yields a natural bijection between the following functors $\mathsf{C} \rightsquigarrow \mathsf{Set}$:
  \begin{equation*}
    h^{A \amalg B} \Rightarrow h^{A} \times h^{B}.
  \end{equation*}

  The exponential is a little bit more complicated: it allows us to trade a morphism
  \begin{equation*}
    X \times A \to B
  \end{equation*}
  for a morphism 
  \begin{equation*}
    X \to B^{A},
  \end{equation*}
  and vice versa.  That is to say, we have a bijection between two functors $\mathsf{C}^{\mathrm{op}} \rightsquigarrow \mathsf{Set}$
  \begin{equation*}
    X \mapsto \Hom_{\mathsf{C}}(X \times A, B)\qquad\text{and}\qquad X \mapsto \Hom_{\mathsf{C}}(X, B^{A}).
  \end{equation*}
  The fact that the functors are more complicated does not hurt us: the above bijection is still natural.
\end{example}

The hom functor as defined above is cute, but not a whole lot else. Things get interesting when we curry it.

Recall from computer science the concept of currying. Suppose we are given a function of two arguments, say $f(x, y)$. The idea of currying is this: if we like, we can view $f$ as a family of functions of only one variable $y$, indexed by $x$:
\begin{equation*}
  h_{x}(y) = f(x, y).
\end{equation*}

We can even view $h$ as a function which takes one argument $x$, and which returns a \emph{function} $h_{x}$ of one variable $y$.

This sets up a correspondence between functions $f\colon A \times B \to C$ and functions $h\colon A \to C^{B}$, where $C^{B}$ is the set of functions $B \to C$. The map which replaces $f$ by $h$ is called \emph{currying}. We can also go the other way (i.e. $h \mapsto f$), which is called \emph{uncurrying}.

We have been intentionally vague about the nature of our function $f$, and what sort of arguments it might take; everything we have said also holds for, say, bifunctors.

In particular, it gives us two more ways to view our bifunctor $\Hom_{\mathsf{C}}$. We can fix any $A \in \Obj(\mathsf{C})$ and curry either argument. This gives us the following.

\begin{definition}[curried hom functor]
  \label{def:curriedhomfunctor}
  Let $\mathsf{C}$ be a locally small category, $A \in \Obj(\mathsf{C})$. We can construct from the hom functor $\Hom_{\mathsf{C}}$
  \begin{itemize}
    \item a functor $h^{A}\colon \mathsf{C} \rightsquigarrow \mathsf{Set}$ which maps
      \begin{itemize}
        \item an object $B \in \Obj(\mathsf{C})$ to the set $\Hom_{\mathsf{C}}(A, B)$, and

        \item a morphism $f\colon B \to B'$ to a $\mathsf{Set}$-function
          \begin{equation*}
            h^{A}(f)\colon \Hom_{\mathsf{C}}(A, B) \to \Hom_{\mathsf{C}}(A, B'); \qquad m \mapsto f \circ m.
          \end{equation*} 
      \end{itemize}

    \item a functor $h_{A}\colon \mathsf{C}^{\mathrm{op}} \rightsquigarrow \mathsf{Set}$ which maps
      \begin{itemize}
        \item an object $B \in \Obj(\mathsf{C})$ to the set $\Hom_{\mathsf{C}}(B, A)$, and

        \item a morphism $f\colon B \to B'$ to a $\mathsf{Set}$-function
          \begin{equation*}
            h_{A}(f)\colon \Hom_{\mathsf{C}}(B', A) \to \Hom_{\mathsf{C}}(B, A); \qquad m \mapsto m \circ f.
          \end{equation*} 
      \end{itemize}
  \end{itemize}
\end{definition}

\subsection{Representable functors}
Roughly speaking, a functor $\mathsf{C} \rightsquigarrow \mathsf{Set}$ is representable if it is 

\begin{definition}[representable functor]
  \label{def:representablefunctor}
  Let $\mathsf{C}$ be a category. A functor $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{Set}$ is \defn{representable} if there is an object $A \in \Obj(\mathsf{C})$ and a natural isomorphism (\hyperref[def:naturalisomorphism]{Definition \ref*{def:naturalisomorphism}})
  \begin{equation*}
    \eta\colon \mathcal{F} \Rightarrow 
    \begin{cases}
      h^{A}, & \text{if $\mathcal{F}$ is covariant} \\
      h_{A}, & \text{if $\mathcal{F}$ is contravariant.}
    \end{cases}
  \end{equation*}
\end{definition}

\begin{note}
  Since natural isomorphisms are invertible, we could equivalently define a representable functor with the natural isomorphism going the other way.
\end{note}

\begin{example}
  Consider the forgetful functor $\mathcal{U}\colon \mathsf{Grp} \rightsquigarrow \mathsf{Set}$ which sends a group to its underlying set, and a group homomorphism to its underlying function. This functor is represented by the group $(\Z, +)$.

  To see this, we have to check that there is a natural isomorphism $\eta$ between $\mathcal{U}$ and $h^{\Z} = \Hom_{\mathsf{Grp}}(\Z, -)$. That is to say, for each group $G$, there a $\mathsf{Set}$-isomorphism (i.e. a bijection)
  \begin{equation*}
    \eta_{G}\colon \mathcal{U}(G) \to \Hom_{\mathsf{Grp}}(\Z, G)
  \end{equation*}
  satisfying the naturality conditions in \hyperref[def:naturaltransformation]{Definition \ref*{def:naturaltransformation}}.

  First, let's show that there's a bijection by providing an injection in both directions. Pick some $g \in G$. Then there is a unique group homomorphism which sends $1 \mapsto g$, so we have an injection $\mathcal{U}(G) \hookrightarrow \Hom_{\mathsf{Grp}}(\Z, G)$.

  Now suppose we are given a group homomorphism $\Z \to G$. This sends $1$ to some element $g \in G$, and this completely determines the rest of the homomorphism. Thus, we have an injection $\Hom_{\mathsf{Grp}(\Z, G)} \hookrightarrow \mathcal{U}(G)$.

  All that is left is to show that $\eta$ satisfies the naturality condition. Let $F$ and $H$ be groups, and $f\colon G \to H$ a homomorphism. We need to show that the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{U}(G)
      \arrow[r, "\mathcal{U}(f)"]
      \arrow[d, swap, "\eta_{G}"]
      & \mathcal{U}(H)
      \arrow[d, "\eta_{H}"]
      \\
      \Hom_{\mathsf{Grp}}(\Z, G)
      \arrow[r, "h^{\Z}(f)"]
      & \Hom_{\mathsf{Grp}}(\Z, H)
    \end{tikzcd}
  \end{equation*}
  The upper path from top left to bottom right assigns to each $g \in G$ the function $\Z \to G$ which maps $1 \mapsto f(g)$. Walking down $\eta_{G}$ from $\mathcal{U}(G)$ to $\Hom_{\mathsf{Grp}}(\Z, G)$, $g$ is mapped to the function $\Z \to G$ which maps $1 \mapsto g$. Walking right, we compose this function with $f$ to get a new function $\Z \to H$ which sends $1 \mapsto f(g)$. This function is really the image of a homomorphism and is therefore unique, so the diagram commutes.
\end{example}

\subsection{The Yoneda embedding}
The Yoneda embedding is a very powerful tool which allows us to prove things about any locally small category by embedding that category into $\mathsf{Set}$, and using the enormous amount of structure that $\mathsf{Set}$ has. 

In the thread \cite{baez-categories-usenet} on the sci.math google group, John Baez explains the importance of the Yoneda lemma in the following way.

\begin{quote}
  Category theory can be viewed in many ways, but one is as a massive generalization of set theory.  The category $\mathsf{Set}$ has sets as objects and functions between them as morphisms.  In practice, other categories are often built by taking as objects ``sets with extra structure'' and as morphisms ``functions preserving the extra structure.''  For example, $\mathsf{Vect}$ is the category whose objects are vector spaces and whose morphisms are linear functions.  The extra structure in this case is the linear structure.  One can easily list hundreds more such examples.  

  However, an abstract category needn't have as its objects ``sets with structure''---its objects are simply abstract thingamabobs!  (I would have said ``simply abstract objects'', but that would sound circular here, since it is, so I resorted to a more technical term.)  At this point the Yoneda lemma leaps to our rescue by saying that all (small enough) categories can be embedded in categories in which the objects are sets with structure and the morphisms are structure-preserving functions between these.  

  Here's the basic idea of how it works, in watered-down form.  To each object $x$, we associate the set $S(x)$ of all morphisms \textit{to} $x$ from all other objects.  This set has a certain structure which one can work out, and a morphism $f\colon x \to y$ gives rise to a structure-preserving function $S(f)$ from $S(x)$ to $S(y)$ in the obvious way: given a morphism from something to $x$, just compose it with $f$ to get a morphism to $y$.

  So we have taken our original category and embedded in a category of ``sets with structure.''
\end{quote}

\begin{definition}[Yoneda embedding]
  \label{def:yonedaembedding}
  Let $\mathsf{C}$ be a locally small category. The \defn{Yoneda embedding} is the functor
  \begin{equation*}
    \mathcal{Y}\colon \mathsf{C} \rightsquigarrow [\mathsf{C}^{\mathrm{op}}, \mathsf{Set}], \qquad A \mapsto h_{A} = \Hom_{\mathsf{C}}(-, A).
  \end{equation*}
  where $[\mathsf{C}^{\mathrm{op}}, \mathsf{Set}]$ is the category of functors $\mathsf{C}^{\mathrm{op}} \rightsquigarrow \mathsf{Set}$, defined in \hyperref[def:functorcategory]{Definition \ref*{def:functorcategory}}.

  Of course, we also need to say how $\mathcal{Y}$ behaves on morphisms. Let $f\colon A \to A'$. Then $\mathcal{Y}(f)$ will be a morphism from $h_{A}$ to $h_{A'}$, i.e. a natural transformation $h_{A} \Rightarrow h_{A'}$. We can specify how it behaves by specifying its components $(\mathcal{Y}(f))_{B}$. 

  Plugging in definitions, we find that $(\mathcal{Y}(f))_{B}$ is a map $\Hom_{\mathsf{C}}(B, A) \to \Hom_{\mathsf{C}}(B, A')$. But we know how to get such a map---we can compose all of the maps $B \to A$ with $f$ to get maps $B \to A'$. So $\mathcal{Y}(f)$, as a natural transformation $\Hom_{\mathsf{C}}(-, A) \to \Hom_{\mathsf{C}}(-, A')$, simply composes everything in sight with $f$.
\end{definition}

\begin{theorem}[Yoneda lemma]
  Let $\mathcal{F}$ be a functor from $\mathsf{C}^{\mathrm{op}}$ to $\mathsf{Set}$. Let $A \in \Obj(\mathsf{C})$. Then there is a set-isomorphism (i.e. a bijection) $\eta$ between the set $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{A}, \mathcal{F})$ of natural transformations $h_{A} \Rightarrow \mathcal{F}$ and the set $\mathcal{F}(A)$. Furthermore, this isomorphism is natural in $A$.
\end{theorem}
\begin{note}
  A quick admission before we begin: we will be sweeping a lot of issues with size under the rug in what follows by tacitly assuming that $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{A}, \mathcal{F})$ is a set. The reader will have to trust that everything works out in the end.
\end{note}
\begin{proof}
  A natural transformation $\Phi\colon h_{A} \Rightarrow \mathcal{F}$ consists of a collection of $\mathsf{Set}$-morphisms (that is to say, functions) $\Phi_{B}\colon \Hom_{\mathsf{C}}(B, A) \to \mathcal{F}(A)$, one for each $B \in \Obj(\mathsf{C})$. We need to show that to each element of $\mathcal{F}(A)$ there corresponds exactly one $\Phi$. We will do this by showing that any $\Phi$ is completely determined by where $\Phi_{A}$ sends the identity morphism $1_{A} \in \Hom_{\mathsf{C}}(A, A)$, so there is exactly one natural transformation for each place $\Phi$ can send $1_{A}$.

  The proof that this is the case can be illustrated by the following commutative diagram.
  \begin{equation*}
    \begin{tikzcd}
      \Hom(A, A)
      \arrow[rrr, "h_{A}(f)"]
      \arrow[ddd, swap, "\Phi_{A}"]
      & & & \Hom(B, A)
      \arrow[ddd, "\Phi_{B}"]
      \\
      & 1_{A}
      \arrow[r, mapsto]
      \arrow[d, mapsto]
      & f
      \arrow[d, mapsto]
      \\
      & a
      \arrow[r, mapsto]
      & (\mathcal{F}f)(a) \stackrel{!}{=} \Phi_{B}(f) 
      \\
      \mathcal{F}(A)
      \arrow[rrr, swap, "\mathcal{F}(f)"]
      & & & \mathcal{F}(B)
    \end{tikzcd}
  \end{equation*}
  Here's what the above diagram means. The natural transformation $\Phi\colon h_{A} \Rightarrow \mathcal{F}$ has a component $\Phi_{A}\colon \Hom_{\mathsf{C}}(A, A) \to \mathcal{F}(A)$, and $\Phi_{A}$ has to send the identity transformation $1_{A}$ \emph{somewhere}. It can send it to any element of $\mathcal{F}(A)$; let's call $\Phi_{A}(1_{A}) = a$.

  Now the naturality conditions force our hand. For any $B \in \Obj(\mathsf{C})$ and any $f\colon B \to A$, the naturality square above means that $\Phi_{B}(f)$ \emph{has to} be equal to $(\mathcal{F}f)(a)$. We get no choice in the matter.

  But this completely determines $\Phi$! So we have shown that there is exactly one natural transformation for every element of $\mathcal{F}(A)$. We are done!

  Well, almost. We still have to show that the bijection we constructed above is natural. To that end, let $f\colon B \to A$. We need to show that the following square commutes.
  \begin{equation*}
    \begin{tikzcd}
      \Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{A}, \mathcal{F})
      \arrow[r, "\eta_{A}"]
      \arrow[d, swap, "{\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(\mathcal{Y}(f), \mathcal{F})}"]
      & \mathcal{F}(A)
      \arrow[d, "\mathcal{F}(f)"]
      \\
      \Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{B}, \mathcal{F})
      \arrow[r, "\eta_{B}"]
      & \mathcal{F}(B)
    \end{tikzcd}
  \end{equation*}

  This is notationally dense, and deserves a lot of explanation. The natural transformation $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(\mathcal{Y}(f), \mathcal{F})$ looks complicated, but it's really not since we know what the hom functor does: it takes every natural transformation $h_{A} \Rightarrow \mathcal{F}$ and pre-composes it with $\mathcal{Y}(f)$. The natural transformation $\eta_{A}$ takes a natural transformation $\Phi\colon h_{A} \Rightarrow \mathcal{F}$ and sends it to the element $\Phi_{A}(1_{A}) \in \mathcal{F}(A)$.

  So, starting at the top left with a natural transformation $\Phi\colon h_{A} \Rightarrow \mathcal{F}$, we can go to the bottom right in two ways.
  \begin{enumerate}
    \item We can head down to $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{B}, \mathcal{F})$, mapping 
      \begin{equation*}
        \Phi \mapsto \Phi \circ \mathcal{Y}(f),
      \end{equation*}
      then use $\eta_{B}$ to map this to $\mathcal{F}(B)$:
      \begin{equation*}
        \Phi \circ \mathcal{Y}(f) \mapsto (\Phi \circ \mathcal{Y}(f))_{B}(1_{B}).
      \end{equation*}

      We can simplify this right away. The component of the composition of natural transformations is the composition of the components, i.e.
      \begin{equation*}
        (\Phi \circ \mathcal{Y}(f))_{B}(1_{B}) = (\Phi_{B} \circ \mathcal{Y}(f)_{B})(1_{B}).
      \end{equation*}
      We also know how $\mathcal{Y}(f)$ behaves: it composes everything in sight with $f$.
      \begin{equation*}
        \mathcal{Y}(f)(1_{B}) = f.
      \end{equation*}
      Thus, we have
      \begin{equation*}
        (\Phi \circ \mathcal{Y}(f))(1_{B}) = \Phi_{B}(f).
      \end{equation*}

    \item We can first head to the right using $\eta_{A}$. This sends $\Phi$ to 
      \begin{equation*}
        \Phi_{A}(1_{A}) \in \mathcal{F}(A).
      \end{equation*}
      We can then map this to $\mathcal{F}(B)$ with $f$, getting
      \begin{equation*}
        \mathcal{F}(f)(\Phi_{A}(1_{A})).
      \end{equation*}
  \end{enumerate}

  The naturality condition is thus
  \begin{equation*}
    (\mathcal{F}(f))(\Phi_{A}(1_{A})) = \Phi_{B}(f),
  \end{equation*}
  which we saw above was true for any natural transformation $\Phi$.
\end{proof}
\begin{lemma}
  The Yoneda embedding is fully faithful (\hyperref[def:fullfaithfulfunctor]{Definition \ref*{def:fullfaithfulfunctor}}).
\end{lemma}
\begin{proof}
  We have to show that for all $A$, $B \in \Obj(\mathsf{C})$, the map
  \begin{equation*}
    \mathcal{Y}_{A, B}\colon \Hom_{\mathsf{C}}(A, B) \to \Hom_{[\mathsf{C}^{\mathrm{op}}, \mathsf{Set}]}(h_{A}, h_{B})
  \end{equation*}
  is a bijection.

  Fix $B \in \Obj(\mathsf{C})$, and consider the functor $h_{B}$. By the Yoneda lemma, there is a bijection between $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{A}, h_{B})$ and $h_{B}(A)$. By definition,
  \begin{equation*}
    h_{B}(A) = \Hom_{\mathsf{C}}(A, B).
  \end{equation*}

  But $\Hom_{[\mathsf{C}^{\text{op}}, \mathsf{Set}]}(h_{A}, h_{B})$ is nothing else but $\Hom_{[\mathsf{C}^{\mathrm{op}}, \mathsf{Set}]}(h_{A}, h_{B})$, so we are done.
\end{proof}

\begin{note}
  \label{note:covariantyonedaembedding}
  We defined the Yoneda embedding to be the functor $A \mapsto h_{A}$; this is sometimes called the \emph{contravariant Yoneda embedding}. We could also have studied to map $A$ to $h^{A}$, called the \emph{covariant Yoneda embedding}, in which case a slight modification of the proof of the Yoneda lemma would have told us that the map
  \begin{equation*}
    \Hom_{\mathsf{C^{\mathrm{op}}}}(B, A) \to \Hom_{[\mathsf{C}, \mathsf{Set}]}(h^{A}, h^{B})
  \end{equation*}
  is a natural bijection. This is often called the \emph{covariant Yoneda lemma}.
\end{note}

Here is a situation in which the Yoneda lemma is commonly used.
\begin{corollary}
  \label{cor:yonedaembeddingrespectsisomorphisms}
  Let $\mathsf{C}$ be a locally small category. Suppose for all $A \in \Obj(\mathsf{C})$ there is a bijection
  \begin{equation*}
    \Hom_{\mathsf{C}}(A, B) \stackrel{\sim}{\to} \Hom_{\mathsf{C}}(A, B')
  \end{equation*}
  which is natural in $A$. Then $B \simeq B'$
\end{corollary}
\begin{proof}
  We have a natural isomorphism $h_{B} \Rightarrow h_{B'}$. Since the Yoneda embedding is fully faithful, it is injective on objects up to isomorphism (\hyperref[lemma:fullyfaithfulfunctorinjectiveuptoisomorphism]{Lemma \ref*{lemma:fullyfaithfulfunctorinjectiveuptoisomorphism}}). Thus, since $h_{B}$ and $h_{B'}$ are isomorphic, so must be $B \simeq B'$.
\end{proof}

\subsection{Applications}
We have now built up enough machinery to make the proofs of a great variety of things trivial, as long as we accept a few assertions. 

It is possible, for example, to prove that the product is associative in \emph{any} locally small category, just from the fact that it is associative in $\mathsf{Set}$.
\begin{lemma}
  The Cartesian product is associative in $\mathsf{Set}$. That is, there is a natural isomorphism $\alpha$ between $(A \times B) \times C$ and $A \times (B \times C)$ for any sets $A$, $B$, and $C$.
\end{lemma}
\begin{proof}
  The isomorphism is given by $\alpha_{A, B, C}\colon ((a, b), c) \mapsto (a, (b, c))$. This is natural because for functions like this,
  \begin{equation*}
    \begin{tikzcd}[row sep=tiny]
      A 
      \arrow[r, "f"]
      & A'
      \\
      B 
      \arrow[r, "g"]
      & B'
      \\
      C 
      \arrow[r, "h"]
      & C'
    \end{tikzcd}
  \end{equation*}
  the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}
      ((a, b), c)
      \arrow[r, mapsto, "{((f, g), h)}"]
      \arrow[d, mapsto, swap, "{\alpha_{A, B, C}}"]
      & ((f(a), g(b)), h(c))
      \arrow[d, mapsto, "{\alpha_{A', B', C'}}"]
      \\
      (a, (b, c))
      \arrow[r, mapsto, swap, "{(f, (g, h))}"]
      & (f(a), (g(b), h(c)))
    \end{tikzcd}
  \end{equation*}
\end{proof}
\begin{theorem}
  \label{thm:categoricalproductisassociative}
  In any locally small category $\mathsf{C}$ with products, there is a natural isomorphism $(A \times B) \times C \simeq A \times (B \times C)$.
\end{theorem}
\begin{proof}
  We have the following string of natural isomorphisms for any $X$, $A$, $B$, $C \in \Obj(\mathsf{C})$.
  \begin{align*}
    \Hom_{\mathsf{C}}(X, (A \times B) \times C) &\simeq \Hom_{\mathsf{C}}(X, A \times B) \times \Hom_{\mathsf{C}}(X, C) \\
    & \simeq (\Hom_{\mathsf{C}}(X, A) \times \Hom_{\mathsf{C}}(X, B)) \times \Hom_{\mathsf{C}}(X, C) \\
    & \simeq \Hom_{\mathsf{C}}(X, A) \times (\Hom_{\mathsf{C}}(X, B) \times \Hom_{\mathsf{C}}(X, C)) \\
    & \simeq \Hom_{\mathsf{C}}(X, A) \times \Hom_{\mathsf{C}}(X, B \times C) \\
    & \simeq \Hom_{\mathsf{C}}(X, A \times (B \times C)).
  \end{align*}

  Thus, by \hyperref[cor:yonedaembeddingrespectsisomorphisms]{Corollary \ref*{cor:yonedaembeddingrespectsisomorphisms}}, $(A \times B) \times C \simeq A \times (B \times C)$.
\end{proof}

\begin{note}
  By induction, all finite products are associative.
\end{note}

\begin{theorem}
  In any category with products $\times$, coproducts $+$, initial object $0$, final object $1$, and exponentials, we have the following natural isomorphisms.
  \begin{enumerate}
    \item $A \times (B + C) \simeq (A \times B) + (A \times C)$
    \item $C^{A + B} \simeq C^{A} \times C^{B}$.
    \item $(C^{A})^{B} \simeq C^{A \times B}$
    \item $(A \times B)^{C} \simeq A^{C} \times B^{C}$
    \item $C^{0} \simeq 1$
    \item $C^{1} \simeq C$
  \end{enumerate}
\end{theorem}
\begin{proof}
  $\,$
  \begin{enumerate}
    \item We have the following list of natural isomorphisms.
      \begin{align*}
        \Hom_{\mathsf{C}}(A \times (B + C), X) & \simeq \Hom_{\mathsf{C}}(B + C, X^{A}) \\
        & \simeq \Hom_{\mathsf{C}}(B, X^{A}) \times \Hom_{\mathsf{C}}(C, X^{A}) \\
        & \simeq \Hom_{\mathsf{C}}(B \times A, X) \times \Hom_{\mathsf{C}}(C \times A, X) \\
        & \simeq \Hom_{\mathsf{C}}((B \times A) + (C \times A), X).
      \end{align*}
    \item We have the following list of natural isomorphisms.
      \begin{align*}
        \Hom(X, C^{A + B}) & \simeq \Hom(X \times (A + B), C) \\
        & \simeq \Hom_{\mathsf{C}}((X \times A) + (X \times B), C) \\
        & \simeq \Hom_{\mathsf{C}}(X \times A, C) \times \Hom_{\mathsf{C}}(X \times B, C) \\
        & \simeq \Hom_{\mathsf{C}}(X, C^{A}) \times \Hom_{\mathsf{C}}(X, C^{B}) \\
        & \simeq \Hom_{\mathsf{C}}(X, C^{A} \times C^{B}).
      \end{align*}
  \end{enumerate}
  Etc.
\end{proof}

\section{Limits} \label{sec:limits}
\subsection{Limits and colimits}
As we have seen, one often gets categorical concepts by `categorifying' concepts from set theory. One example of this is the notion of a \emph{diagram}, which is the categorical generalization of an indexed family.

\begin{definition}[indexed family]
  \label{def:indexedfamily}
  Let $J$ and $X$ be sets. A \defn{family of elements in $X$ indexed by $J$} is a function
  \begin{equation*}
    x\colon J \to X;\qquad j \mapsto x_{j}.
  \end{equation*}
\end{definition}

To categorify this, one considers a functor from one category $\mathsf{J}$, called the \emph{index category}, to another category $\mathsf{C}$. Using a functor from a category instead of a function from a set allows us to index the morphisms as well as the objects. 

\begin{definition}[diagram]
  \label{def:diagram}
  Let $\mathsf{J}$ and $\mathsf{C}$ be categories. A \defn{diagram} of type $\mathsf{J}$ in $\mathsf{C}$ is a (covariant) functor
  \begin{equation*}
    \mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}.
  \end{equation*}
\end{definition}

One thinks of the functor $\mathcal{D}$ embedding the index category $\textsf{J}$ into $\mathsf{C}$.

\begin{definition}[cone]
  \label{def:cone}
  Let $\mathsf{C}$ be a category, $\mathsf{J}$ an index category, and $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ be a diagram. Let $\mathsf{1}$ be the category with one object and one morphism (\hyperref[eg:categorywithoneobject]{Example \ref*{eg:categorywithoneobject}}) and $\mathcal{F}_{X}$ the functor $\mathsf{1} \rightsquigarrow \mathsf{C}$ which picks out $X \in \Obj(\mathsf{C})$ (see \hyperref[eg:functorfrom1category]{Example \ref*{eg:functorfrom1category}}). Let $\mathcal{K}$ be the unique functor $\mathsf{J} \rightsquigarrow \mathsf{1}$.
  \begin{equation*} 
    \begin{tikzcd}
      \mathsf{1}
      \arrow[rd, rightsquigarrow, "\mathcal{F}_{X}"]
      \\
      \mathsf{J}
      \arrow[u, rightsquigarrow, "\mathcal{K}"]
      \arrow[r, rightsquigarrow, "\mathcal{D}"]
      & \mathsf{C}
    \end{tikzcd}
  \end{equation*}
  A \defn{cone} of shape $\mathsf{J}$ from $X$ is an object $X \in \Obj(\mathsf{C})$ together with a natural transformation
  \begin{equation*}
    \varepsilon\colon \mathcal{F}_{X} \circ \mathcal{K} \Rightarrow \mathcal{D}.
  \end{equation*}

  That is to say, a cone to $\mathsf{J}$ is an object $X \in \Obj(\mathsf{C})$ together with a family of morphisms $\Phi_{A}\colon X \to \mathcal{D}(A)$ (one for each $A \in \Obj(\mathsf{J})$) such that for all $A$, $B \in \Obj(\mathsf{J})$ and all $f\colon A \to B$ the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
      & X
      \arrow[dl, swap, "\Phi_{A}"]
      \arrow[dr, "\Phi_{B}"]
      \\
      \mathcal{D}(A)
      \arrow[rr, "\mathcal{D}(f)"]
      & & \mathcal{D}(B)
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{note}
  \label{note:alternatedefinitionofcone}
  Here is an alternate definition. Let $\Delta$ be the functor $\mathsf{C} \rightsquigarrow [\mathsf{J}, \mathsf{C}]$ (the category of functors $\mathsf{J} \rightsquigarrow \mathsf{C}$, see TODO) which assigns to each object $X \in \Obj(\mathsf{C})$ the constant functor $\Delta_{X}: \mathsf{J} \rightsquigarrow \mathsf{C}$, i.e. the functor which maps every object of $\mathsf{J}$ to $X$ and every morphism to $1_{X}$. A cone over $\mathcal{D}$ is then an object in the comma category (\hyperref[def:commacategory]{Definition \ref*{def:commacategory}}) $(\Delta \downarrow \mathcal{D})$ given by the diagram
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, rightsquigarrow, "\Delta"]
      & {[\mathsf{J}, \mathsf{C}]}
      & \mathsf{1}
      \arrow[l, swap, rightsquigarrow, "\mathcal{D}"]
    \end{tikzcd}.
  \end{equation*}

  The objects of this category are pairs $(X, f)$, where $X \in \Obj(\mathsf{C})$ and $f\colon \Delta(X) \to \mathcal{D}$; that is to say, $f$ is a natural transformation $\Delta_{X} \Rightarrow \mathcal{D}$.

\end{note}
This allows us to make the following definition. 

\begin{definition}[category of cones over a diagram]
  \label{def:categoryofconesoveradiagram}
  Let $\mathsf{C}$ be a category, $\mathsf{J}$ an index category, and $\mathcal{D}\colon \mathsf{J} \rightarrow \mathsf{C}$ a diagram. The \defn{category of cones over $\mathcal{D}$} is the category $(\Delta \downarrow \mathcal{D})$.
\end{definition} 

\begin{note}
  The alternate definition given in \hyperref[note:alternatedefinitionofcone]{Note \ref*{note:alternatedefinitionofcone}} not only reiterates what cones look like, but even prescribes what morphisms between cones look like. Let $(X, \Phi)$ be a cone over a diagram $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$, i.e.
  \begin{itemize}
    \item an object in the category $(\Delta \downarrow \mathcal{D})$, i.e.
    \item a pair $(X, \Phi)$, where $X \in \Obj(\mathsf{C})$ and $\Phi\colon \Delta_{X} \Rightarrow \mathcal{D}$ is a natural transformation, i.e.
    \item for each $J \in \Obj(\mathsf{J})$ a morphism $\Phi_{J}\colon X \to \mathcal{D}(J)$ such that for any other object $J' \in \Obj(\mathsf{J})$ and any morphism $f\colon J \to J'$ the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}
          & X
          \arrow[ld, swap, "\Phi_{J}"]
          \arrow[rd, "\Phi_{J'}"]
          \\
          \mathcal{D}(J)
          \arrow[rr, "\mathcal{D}(f)"]
          & & \mathcal{D}(J')
        \end{tikzcd}
      \end{equation*}
  \end{itemize}
  This agrees with our previous definition of a cone.

  Let $(Y, \Gamma)$ be another cone over $\mathcal{D}$. Then a morphism $\Xi\colon (X, \Phi) \to (Y, \Gamma)$ is 
  \begin{itemize}
    \item a morphism $\Xi \in \Hom_{(\Delta \downarrow \mathcal{D})}((X, \Phi), (Y, \Gamma))$, i.e.
    \item a natural transformation $\Xi\colon \Delta_{X} \Rightarrow \Delta_{Y}$ (i.e. a morphism $\Delta_{X} \to \Delta_{Y}$ in the category $[\mathsf{J}, \mathsf{C}]$) such that the diagram
      \begin{equation*}
        \begin{tikzcd}
          \Delta_{X}
          \arrow[rr, Rightarrow, "\Xi"]
          \arrow[rd, Rightarrow, "\Phi"]
          & &  \Delta_{Y}
          \arrow[ld, swap, Rightarrow, "\Gamma"]
          \\
          & \mathcal{D}
        \end{tikzcd}
      \end{equation*}
      commutes, i.e.
    \item a morphism $\xi\colon X \to Y$ such that for each $J \in \Obj(\mathsf{J})$, the diagram
      \begin{equation*}
        \begin{tikzcd}
          X
          \arrow[rr, "\xi"]
          \arrow[rd, "\Phi_{J}"]
          & & Y
          \arrow[ld, swap, "\Gamma_{J}"]
          \\
          & \mathcal{D}(J)
        \end{tikzcd}
      \end{equation*}
      commutes.
  \end{itemize}
\end{note}

Cocones are the dual notion to cones. We make the following definition.
\begin{definition}[cocone]
  \label{def:cocone}
  A \defn{cocone over a diagram $\mathcal{D}$} is an object in the comma category $(\mathcal{D} \downarrow \Delta)$.
\end{definition}

\begin{definition}[category of cocones]
  \label{def:categoryofcocones}
  The \defn{category of cocones over a diagram $\mathcal{D}$} is the category $(\mathcal{D} \downarrow \Delta)$.
\end{definition}

The categorical definitions of cones and cocones allow us to define limits and colimits succinctly.
\begin{definition}[limits, colimits]
  \label{def:limitscolimits}
  A \defn{limit} of a diagram $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ is a final object in the category $(\Delta \downarrow \mathcal{D})$. A \defn{colimit} is an initial object in the category $(\mathcal{D} \downarrow \Delta)$.
\end{definition}

\begin{note}
  The above definition of a limit unwraps as follows. The limit of a diagram $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ is a cone $(X, \Phi)$ over $\mathcal{D}$ such that for any other cone $(Y, \Gamma)$ over $\mathcal{D}$, there is a unique map $\xi\colon Y \to X$ such that for each $J \in \Obj(\mathsf{J})$, the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
      Y
      \arrow[rr, "\xi"]
      \arrow[dr, swap, "\Gamma_{J}"]
      & & X
      \arrow[dl, "\Phi_{J}"]
      \\
      & \mathcal{D}(J)
    \end{tikzcd}
  \end{equation*}
\end{note} 

\begin{notation}
  We often denote the limit over the diagram $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ by 
  \begin{equation*}
    \lim_{\leftarrow} \mathcal{D}.
  \end{equation*}

  Similarly, we will denote the colimit over $\mathcal{D}$ by 
  \begin{equation*}
    \lim_{\rightarrow} \mathcal{D}.
  \end{equation*}
  If there is notational confusion over which functor we are taking a (co)limit, we will add a dummy index:
  \begin{equation*}
    \lim_{\leftarrow i} \mathcal{D}_{i}.
  \end{equation*}
\end{notation}

\begin{example}
  Here is a definition of the product $A \times B$ equivalent to that given in \hyperref[eg:universalpropertyofproducts]{Example \ref*{eg:universalpropertyofproducts}}: it is the limit of the following somewhat trivial diagram.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[loop left, "1_{A}"]
      & B
      \arrow[loop right, "1_{B}"]
    \end{tikzcd}
  \end{equation*}

  Let us unwrap this definition. We are saying that the product $A \times B$ is a cone over $A$ and $B$
  \begin{equation*}
    \begin{tikzcd}
      & A \times B
      \arrow[dl, swap, "\pi_{A}"]
      \arrow[dr, "\pi_{B}"]
      \\
      A & & B
    \end{tikzcd}
  \end{equation*}
  But not just any cone: a cone which is universal in the sense that any \emph{other} cone factors through it uniquely.
  \begin{equation*}
    \begin{tikzcd}
      & X
      \arrow[dl, swap, "f_{1}"]
      \arrow[dr, "f_{2}"]
      \arrow[d, "\exists!f"]
      \\
      A 
      & A \times B 
      \arrow[l, "\pi_{A}"]
      \arrow[r, swap, "\pi_{B}"]
      & B
    \end{tikzcd}
  \end{equation*}

  In fact, this allows us to generalize the product: the product of $n$ objects $\prod_{i=1}^{n} A_{i}$ is the limit over diagram consisting of all the $A_{i}$ with no morphisms between them.
\end{example}

\begin{definition}[equalizer]
  \label{def:equalizer}
  Let $\mathsf{J}$ be the category with objects and morphisms as follows. (The necessary identity arrows are omitted.)
  \begin{equation*}
    \begin{tikzcd}
      J
      \arrow[r, shift left, "1"]
      \arrow[r, shift right, swap, "2"]
      & J'
    \end{tikzcd}
  \end{equation*}

  A diagram $\mathcal{D}$ of shape $\mathsf{J}$ in some category $\mathsf{C}$ looks like the following.
  \begin{equation*}
    \begin{tikzcd}
      A 
      \arrow[r, shift left, "f"]
      \arrow[r, shift right, swap, "g"]
      & B
    \end{tikzcd}
  \end{equation*}

  The \defn{equalizer} of $f$ and $g$ is the limit of the diagram $\mathcal{D}$; that is to say, it is an object $\mathrm{eq} \in \Obj(\mathsf{C})$ and a morphism $e\colon \mathrm{eq} \to A$
  \begin{equation*}
    \begin{tikzcd}
      \mathrm{eq}
      \arrow[r, "e"]
      & A
      \arrow[r, shift left, "f"]
      \arrow[r, shift right, swap, "g"]
      & B
    \end{tikzcd}
  \end{equation*}
  such that for any \emph{other} object $Z$ and morphism $i\colon Z \to A$ such that $f \circ i = g \circ i$, there is a unique morphism $e\colon Z \to \mathrm{eq}$ making the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}
      Z 
      \arrow[d, swap, dashed, "\exists! u"]
      \arrow[dr, "i"]
      \\
      \mathrm{eq}
      \arrow[r, swap, "e"]
      & A
      \arrow[r, shift left, "f"]
      \arrow[r, shift right, swap, "g"]
      & B
    \end{tikzcd}
  \end{equation*}
\end{definition}
%\begin{example}
%  What does it mean for the above diagram to commute in, say, $\mathsf{Set}$? We need $(f \circ e)(x) = (g \circ e)(x)$ for all $x \in \mathrm{eq}$. That is, once we have been mapped by $e$ into $A$, we need to be taken to the same place by $f$ and $g$. The range of $e$ must lie entirely within the set 
%\end{example}

\subsection{Pullbacks and kernels}
In what follows, $\mathsf{C}$ will be a category and $A$, $B$, etc. objects in $\Obj(\mathsf{C})$.
\begin{definition}[pullback]
  \label{def:pullback}
  Let $f$, $g$ be morphisms as follows.
  \begin{equation*}
    \begin{tikzcd}
      & B \arrow[d, "g"] \\
      A \arrow[r, swap, "f"] & C
    \end{tikzcd}
  \end{equation*}
  A \defn{pullback of $f$ along $g$} (also called a pullback of $g$ over $f$, sometimes notated $A \times_{C} B$) is a commuting square 
  \begin{equation*}
    \begin{tikzcd}
      U \arrow[r, "q"] \arrow[d, "p", swap] & B \arrow[d, "g"] \\
      A \arrow[r, swap, "f"] & C
    \end{tikzcd}
  \end{equation*}
  such that for any other commuting square
  \begin{equation*}
    \begin{tikzcd}
      V \arrow[r, "t"] \arrow[d, "s", swap] & B \arrow[d, "g"] \\
      A \arrow[r, swap, "f"] & C
    \end{tikzcd}
  \end{equation*}
  there is a unique morphism $h\colon V \to U$ such that the diagram
  \begin{equation*}
    \begin{tikzcd}
      V \arrow[rrd, bend left, "t"] \arrow[rd, dashed, "\exists!h"] \arrow[rdd, bend right, swap, "s"] &   &  \\
      & U \arrow[r, "q"] \arrow[d, swap, "p"] & B \arrow[d, "g"] \\
      & A \arrow[r, swap, "f"] & C
    \end{tikzcd}
  \end{equation*}
\end{definition}
\begin{note}
  Here is another definition: the pullback $A \times_{C} B$ is the limit of the diagram
  \begin{equation*}
    \begin{tikzcd}
      & B
      \arrow[d, "g"]
      \\
      A
      \arrow[r, swap, "f"]
      & C
    \end{tikzcd}
  \end{equation*}

  This might at first seem odd; after all, don't we also need an arrow $A \times_{C} B \to C$? But this arrow is completely determined by the commutativity conditions, so it is superfluous.
\end{note}


\begin{example}
  In $\mathsf{Set}$, $U$ is given (up to unique isomorphism) by 
  \begin{equation*}
    U = \left\{ (a,b) \in A \times B \,\big|\, f(a) = g(b) \right\}.
  \end{equation*}

  The morphisms $p$ and $q$ are given by the projections $p(a,b) = a$, $q(a,b) = b$.

  To see that this really does satisfy the universal property, consider any other set $V$ and functions $s\colon V \to A$ and $t\colon V \to B$ making the above diagram commute. Then for all $v \in V$, $f(s(v)) = t(g(v))$.

  Now consider the map $V \to U$ sending $v$ to $(s(v), t(v))$. This certainly makes the above diagram commute; furthermore, any other map from $V$ to $U$ would not make the diagram commute. Thus $U$ and $h$ together satisfy the universal property.
\end{example}

\begin{lemma}
  Let $f\colon X \to Y$ be a monomorphism. Then any pullback of $f$ is a monomorphism.
\end{lemma}
\begin{proof}
  Suppose we have the following pullback square.
  \begin{equation*}
    \begin{tikzcd}
      X'
      \arrow[r, "p_{1}"]
      \arrow[d, swap, "p_{2}"]
      & X
      \arrow[d, hookrightarrow, "f"]
      \\
      Y'
      \arrow[r, "g"]
      & Y
    \end{tikzcd}
  \end{equation*}
  Our aim is to show that $p_{2}$ is a monomorphism.

  Suppose we are given an object $Z$ and two morphisms $\alpha$, $\beta\colon Z \to X'$.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[dr, shift left, "\alpha"]
      \arrow[dr, shift right, swap, "\beta"]
      \\
      & X'
      \arrow[r, "p_{1}"]
      \arrow[d, swap, "p_{2}"]
      & X
      \arrow[d, hookrightarrow, "f"]
      \\
      & Y'
      \arrow[r, "g"]
      & Y
    \end{tikzcd}
  \end{equation*}
  We can compose $\alpha$ and $\beta$ with $p_{2}$. Suppose that these agree, i.e.
  \begin{equation*}
    p_{2} \circ \alpha = p_{2} \circ \beta.
  \end{equation*}
  We will be done if we can show that this implies that $p_{1} = p_{2}$.

  We can compose with $g$ to find that 
  \begin{equation*}
    g \circ p_{2} \circ \alpha = g \circ p_{2} \circ \beta.
  \end{equation*}
  but since the pullback square commutes, we can replace $g \circ p_{2}$ by $f \circ p_{1}$.
  \begin{equation*}
    f \circ p_{1} \circ \alpha = f \circ p_{1} \circ \beta.
  \end{equation*}
  since $f$ is a monomorphism, this implies that 
  \begin{equation*}
    p_{1} \circ \alpha = p_{1} \circ \beta.
  \end{equation*}
  Now forget $\alpha$ and $\beta$ for a minute. we have constructed a commuting square as follows.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[drr, bend left, "p_{1} \circ \alpha = p_{1} \circ \beta"]
      \arrow[ddr, swap, bend right, "p_{2} \circ \alpha = p_{2} \circ \beta"]
      \\
      & X'
      \arrow[r, "p_{1}"]
      \arrow[d, swap, "p_{2}"]
      & X
      \arrow[d, hookrightarrow, "f"]
      \\
      & Y'
      \arrow[r, "g"]
      & Y
    \end{tikzcd}
  \end{equation*}
  the universal property for pullbacks tells us that there is a unique morphism $Z \to X'$ making the diagram commute. But either $\alpha$ or $\beta$ will do! So $\alpha = \beta$.
\end{proof}

\begin{definition}[kernel of a morphism]
  \label{def:kernelofmorphism}
  Let $\mathsf{C}$ be a category with an initial object (\hyperref[def:initialfinalzeroobject]{Definition \ref*{def:initialfinalzeroobject}}) $0$ and pullbacks. The \defn{kernel} $\ker(f)$ of a morphism $f\colon A \to B$ is the pullback along $f$ of the unique morphism $0 \to B$.
  \begin{equation*}
    \begin{tikzcd}
      \ker(f) \arrow[d, swap, "\iota"] \arrow[r] & 0 \arrow[d] \\
      A \arrow[r, "f"] & B
    \end{tikzcd}
  \end{equation*}

  That is to say, the kernel of $f$ is a pair $(\ker(f), \iota)$, where $\ker(f) \in \Obj(\mathsf{C})$ and $\iota\colon \ker(f) \to A$ which satisfies the above universal property.
\end{definition}

\begin{note}
  Although the kernel of a morphism $f$ is a pair $(\ker(f), \iota)$ as described above, we will sometimes sloppily say that the object $\ker(f)$ is the kernel of $f$, especially when the the morphism $\iota$ is obvious or understood. Such abuses of terminology are common; one occasionally even sees the morphism $\iota$ being called the kernel of $f$.
\end{note} 

\begin{example}
  In $\mathsf{Vect}_{k}$, the initial object is the zero vector space $\{0\}$. For any vector spaces $V$ and $W$ and any linear map $f\colon V \to W$, the kernel of $f$ is the pair $(\ker(f), \iota)$ where $\ker(f)$ is the vector space  
  \begin{equation*}
    \ker(f) = \left\{ v \in V \,\big|\, f(v) = 0 \right\}
  \end{equation*}
  and $\iota$ is the obvious injection $\ker(f) \to V$.
\end{example}

\begin{lemma}
  \label{lemma:canonicalinjectionismono}
  Let $f\colon A \to B$, and let $(\iota, \ker(f))$ be the kernel of $f$. Then $\iota$ is a monomorphism (\hyperref[def:monomorphism]{Definition \ref*{def:monomorphism}}).
\end{lemma}
\begin{proof}
  Suppose we have an object $Z \in \Obj(\mathsf{C})$ and two morphisms $g_{1}$, $g_{2}\colon Z \to \ker(f)$. We have the following diagram.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[rd, shift left, "g_{1}"]
      \arrow[rd, shift right, swap, "g_{2}"]
      \\
      & \ker(f)
      \arrow[r]
      \arrow[d, "\iota"]
      & 0
      \arrow[d]
      \\
      & A
      \arrow[r, "f"]
      & B
    \end{tikzcd}
  \end{equation*}
  Further suppose that $\iota \circ g_{1} = \iota \circ g_{2}$.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[rd, shift left, "g_{1}"]
      \arrow[rd, shift right, swap, "g_{2}"]
      \arrow[rrd, bend left]
      \arrow[rdd, bend right, swap, "\iota \circ g_{1} = \iota \circ g_{2}"]
      \\
      & \ker(f)
      \arrow[r]
      \arrow[d, swap, "\iota"]
      & 0
      \arrow[d]
      \\
      & A
      \arrow[r, "f"]
      & B
    \end{tikzcd}
  \end{equation*}

  Now pretend that we don't know about $g_{1}$ and $g_{2}$.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[rrd, bend left]
      \arrow[rdd, bend right, swap, "\iota \circ g_{1} = \iota \circ g_{2}"]
      \\
      & \ker(f)
      \arrow[r]
      \arrow[d, swap, "\iota"]
      & 0
      \arrow[d]
      \\
      & A
      \arrow[r, "f"]
      & B
    \end{tikzcd}
  \end{equation*}

  The universal property for kernels tells us that there is a unique map $Z \to \ker(f)$ making the above diagram commute. But since $g_{1}$ and $g_{2}$ both make the diagram commute, $g_{1}$ and $g_{2}$ must be the same map, i.e. $g_{1} = g_{2}$.
\end{proof}

\subsection{Pushouts and cokernels}
Pushouts are the dual notion to pullbacks.
\begin{definition}[pushouts]
  \label{def:pushout}
  Let $f$, $g$ be morphisms as follows.
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \\
      A
    \end{tikzcd}
  \end{equation*}
  The \defn{pushout of $f$ along $g$} (or $g$ along $f$) is a commuting square
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \arrow[d, "q"] \\
      A \arrow[r, swap, "p"] & U
    \end{tikzcd}
  \end{equation*}
  such that for any other commuting square
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \arrow[d, "t"] \\
      A \arrow[r, swap, "s"] & V
    \end{tikzcd}
  \end{equation*}
  there exists a unique morphism $h\colon U \to V$ such that the diagram 
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \arrow[d, "q"] \arrow[ddr, bend left, "t"] & \\
      A \arrow[drr, bend right, swap, "s"] \arrow[r, swap, "p"] & U \arrow[dr, dashed, "\exists!h"] & \\
      & & V
    \end{tikzcd}
  \end{equation*}
  commutes.
\end{definition}

\begin{note}
  As with pullbacks, we can also define a pushout as the colimit of the following diagram.
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \\
      A
    \end{tikzcd}
  \end{equation*}
\end{note}

\begin{example}
  Let us construct the pushout in $\mathsf{Set}$. If we ignore the object $C$ and the morphisms $f$ and $g$, we discover that $U$ must satisfy the universal property of the coproduct of $A$ and $B$. 
  \begin{equation*}
    \begin{tikzcd}
      & B \arrow[d, "q"] \arrow[ddr, bend left, "t"] & \\
      A \arrow[drr, bend right, swap, "s"] \arrow[r, swap, "p"] & U \arrow[dr, dashed, "\exists!h"] & \\
      & & V
    \end{tikzcd}
  \end{equation*}
  Let us therefore make the ansatz that $U = A \amalg B = A \sqcup B$ and see what happens when we add $C$, $f$, and $g$ back in. 
  \begin{equation*}
    \begin{tikzcd}
      C \arrow[r, "g"] \arrow[d, swap, "f"] & B \arrow[d, "q"] \arrow[ddr, bend left, "t"] & \\
      A \arrow[drr, bend right, swap, "s"] \arrow[r, swap, "p"] & U \arrow[dr, dashed, "\exists!h"] & \\
      & & V
    \end{tikzcd}
  \end{equation*}

  In doing so, we find that the square $A$-$C$-$B$-$U$ must also commute, i.e. we must have that $(q \circ g) (c) = (p \circ f)(c)$ for all $c \in C$. Since $p$ and $q$ are just inclusions, we see that
  \begin{equation*}
    U = A \amalg B / \sim,
  \end{equation*}
  where $\sim$ is the equivalence relation generated by the relations $f(c) \sim g(c)$ for all $c\in C$.
\end{example}

\begin{definition}[cokernel of a morphism]
  \label{def:cokernalofmorphism}
  Let $\mathsf{C}$ be a category with terminal object $1$. The \defn{cokernel} of a morphism $f\colon A \to B$ is the pushout of $f$ along the unique morphism $A \to 1$.
  \begin{equation*}
    \begin{tikzcd}
      A \arrow[r, "f"] \arrow[d] & B \arrow[d, "\pi"] \\
      1 \arrow[r] & \coker(f)
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{example}
  \label{eg:invectcokernelsarequotientsbyimage}
  In $\mathsf{Vect}_{k}$, the terminal object is the vector space $\{0\}$. If $V$ and $W$ are $k$-vector spaces and $f$ is a linear map $V \to W$, then $\coker(f)$ is
  \begin{equation*}
    W / \sim, 
  \end{equation*}
  where $\sim$ is the relation generated by $f(v) \sim 0$ for all $v \in V$. But this relation is exactly the one which mods out by $\mathrm{im}(f)$, so $\coker(f) = W / \mathrm{im}(f)$.
\end{example}

\begin{lemma}
  \label{lemma:canonicalsurjectionisepi}
  For any morphism $f\colon A \to B$, the canonical projection $\pi\colon B \to \coker(f)$ is an epimorphism.
\end{lemma}
\begin{proof}
  The proof is dual to the proof that the canonical injection $\iota$ is mono (\hyperref[lemma:canonicalinjectionismono]{Lemma \ref*{lemma:canonicalinjectionismono}}).
\end{proof}

\begin{definition}[normal monomorphism]
  \label{def:normalmonomorphism}
  A monomorphism (\hyperref[def:monomorphism]{Definition \ref*{def:monomorphism}}) $f\colon A \to B$ is \defn{normal} if it the kernel of some morphism. To put it more plainly, $f$ is normal if there exists an object $C$ and a morphism $g\colon B \to C$ such that $(A, f)$ is the kernel of $g$.
  \begin{equation*}
    \begin{tikzcd}
      A \arrow[r, "f"] & B \arrow[r, "g"] & C
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{example}
  In $\mathsf{Vect}_{k}$, monomorphisms are injective linear maps (\hyperref[eg:monomorphismsinkvect]{Example \ref*{eg:monomorphismsinkvect}}). If $f$ is injective then sequence
  \begin{equation*}
    \begin{tikzcd}
      \{0\} \arrow[r] & V \arrow[r, "f"] & W \arrow[r, "\pi"] & W/\mathrm{im}(f) \arrow[r], & \{0\}
    \end{tikzcd}
  \end{equation*}
  is exact, and we always have that $\mathrm{im}(f) = \ker(\pi)$. Thus in $\mathsf{Vect}_{k}$, every monomorphism is normal.
\end{example}

\begin{definition}[conormal epimorphism]
  \label{def:conormalepimorphism}
  An epimorphism $f\colon A \to B$ is \defn{conormal} if it is the cokernel of some morphism. That is to say, if there exists an object $C$ and a morphism $g\colon C \to A$ such that $(B, f)$ is the cokernel of $g$.
\end{definition}

\begin{example}
  In $\mathsf{Vect}_{k}$, epimorphisms are surjective linear maps. If $f\colon V \to W$ is a surjective linear map, then the sequence
  \begin{equation*}
    \begin{tikzcd}
      \{0\} \arrow[r] & \ker(f) \arrow[r, "\iota"] & V \arrow[r, "f"] & W \arrow[r] & \{0\}
    \end{tikzcd}
  \end{equation*}
  is exact. But then $\mathrm{im}(\iota) = \ker(f)$, so $f$ is conormal. Thus in $\mathsf{Vect}_{k}$, every epimorphism is conormal.
\end{example}

\begin{note}
  To show that in our proofs that in $\mathsf{Vect}_{k}$ monomorphisms were normal and epimorphisms were conormal, we showed that monomorphisms were the kernels of their cokernels, and epimorphisms were the cokernels of their kernels. This will be a general feature of Abelian categories.
\end{note}

\begin{definition}[binormal category]
  \label{def:binormalcategory}
  A category is \defn{binormal} if all monomorphisms are normal and all epimorphisms are conormal.
\end{definition}

\begin{example}
  As we have seen, $\mathsf{Vect}_{k}$ is binormal.
\end{example}

\subsection{A necessary and sufficient condition for the existence of finite limits}
In this section we prove a simple criterion to check that a category has all finite limits (i.e. limits over diagrams with a finite number of objects and a finite number of morphisms). The idea is as follows.

In general, adding more objects to a diagram makes the limit over it larger, and adding more morphisms makes the limit smaller. In the extreme case in which the only morphisms are the identity morphisms, the limit is simply the categorical product. As we add morphisms, roughly speaking, we have to get rid of the parts of the product which are preventing the necessary triangles from commuting. Thus, to make the universal cone over a diagram, we can start with the product, and cut out the bare minimum we need to make everything commute: the way to do that is with an equalizer.

The following proof was adapted from \cite{awodey-category-theory}. 
\begin{theorem}
  \label{thm:criterionforfinitelimits}
  Let $\mathsf{C}$ be a category. Then if $\mathsf{C}$ has all finite products and equalizers, $\mathsf{C}$ has all finite limits.

\end{theorem}
\begin{proof}
  Let $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ be a finite diagram. We want to prove that $\mathcal{D}$ has a limit; we will do this by constructing a universal cone over it, i.e.
  \begin{itemize}
    \item an object $L \in \Obj(\mathsf{C})$, and
    \item for each $j \in \Obj(\mathsf{J})$, a morphism $P_{j}\colon L \to \mathcal{D}(j)$ 
  \end{itemize}
  such that
  \begin{enumerate}
    \item for any $i$, $j \in \Obj(\mathsf{J})$ and any $\alpha\colon i \to j$ the following diagram commutes,
      \begin{equation*}
        \begin{tikzcd}[column sep=tiny]
          & L
          \arrow[dl, swap, "P_{i}"]
          \arrow[dr, "P_{j}"]
          \\
          \mathcal{D}(i)
          \arrow[rr, "\mathcal{D}(\alpha)"]
          & & \mathcal{D}(j)
        \end{tikzcd}
      \end{equation*}
      and
    \item for any other object $L' \in \Obj(\mathsf{C})$ and family of morphisms $Q_{j}\colon L' \to \mathcal{D}(j)$ which make the diagrams
      \begin{equation*}
        \begin{tikzcd}[column sep=tiny]
          & L'
          \arrow[dl, swap, "Q_{i}"]
          \arrow[dr, "Q_{j}"]
          \\
          \mathcal{D}(i)
          \arrow[rr, "\mathcal{D}(\alpha)"]
          & & \mathcal{D}(j)
        \end{tikzcd}
      \end{equation*}
      commute for all $i$, $j$, and $\alpha$, there is a unique morphism $f\colon L' \to L$ such that $Q_{j} = P_{j} \circ f$ for all $j \in \Obj(\mathsf{J})$.
  \end{enumerate}

  Denote by $\Mor(\mathsf{J})$ the set of all morphisms in $\mathsf{J}$. For any $\alpha \in \Hom_{\mathsf{J}}(i, j) \subseteq \Mor(\mathsf{J})$, let $\dom(\alpha) = i$ and $\cod(\alpha) = j$.

  Consider the following finite products:
  \begin{equation*}
    A = \prod_{j \in \Obj(\mathsf{J})} \mathcal{D}(j)\qquad\text{and}\qquad B = \prod_{\alpha \in \Mor(\mathsf{J})} \mathcal{D}(\cod(\alpha)).
  \end{equation*}

  From the universal property for products, we know that we can construct a morphism $f\colon A \to B$ by specifying a family of morphisms $f_{\alpha}\colon A \to \mathcal{D}(\cod(\alpha))$, one for each $\alpha \in \Mor(\mathsf{J})$. We will define two morphisms $R$, $S\colon A \to B$ in this way:
  \begin{equation*}
    R_{\alpha} = \pi_{\mathcal{D}(\cod(\alpha))};\qquad S_{\alpha} = \mathcal{D}(\alpha) \circ \pi_{\mathcal{D}(\dom(\alpha))}.
  \end{equation*}

  Now let $e\colon L \to A$ be the equalizer of $R$ and $S$ (we are guaranteed the existence of this equalizer by assumption). Further, define $P_{j}\colon L \to \mathcal{D}(j)$ by
  \begin{equation*}
    P_{j} = \pi_{\mathcal{D}(j)} \circ e
  \end{equation*}
  for all $j \in \Obj(\mathsf{J})$.

  The claim is that $L$ together with the $P_{j}$ is the limit of $\mathcal{D}$. We need to verify conditions 1 and 2 on $L$ and $P_{j}$ listed above.
  \begin{enumerate}
    \item We need to show that for all $i$, $j \in \Obj(\mathsf{J})$ and all $\alpha\colon i \to j$, we have the equality $\mathcal{D}(\alpha) \circ P_{i} = P_{j}$. Now, for every $\alpha\colon i \to j$ we have
      \begin{align*}
        \mathcal{D}(\alpha) \circ P_{i} &= \mathcal{D}(\alpha) \circ \pi_{\mathcal{D}(i)} \circ e \\
        &= S_{\alpha} \circ e \\
        &= R_{\alpha} \circ e \\
        &= \pi_{\mathcal{D}(j)} \circ e \\
        &= P_{j}.
      \end{align*}

    \item We need to show that for any other $L' \in \Obj(\mathsf{C})$ and any other family of morphisms $Q_{j}\colon L' \to \mathcal{D}(j)$ such that for all $\alpha\colon i \to j$, $Q_{j} = \mathcal{D}(\alpha) \circ Q_{i}$, there is a unique morphism $h\colon L' \to L$ such that $Q_{j} = P_{j} \circ h$ for all $j \in \Obj(\mathsf{J})$. Suppose we are given such an $L'$ and $Q_{j}$.

      The universal property for products allows us to construct from the family of morphisms $Q_{j}$ a morphism $Q\colon L' \to A$ such that $Q_{j} = \pi_{\mathcal{D}(j)} \circ Q$. Now, for any $\alpha\colon i \to j$,
      \begin{align*}
        R_{\alpha} \circ Q &= \pi_{\mathcal{D}(j)} \circ Q \\
        &= Q_{j} \\
        &= \mathcal{D}(\alpha) \circ Q_{i} \\
        &= \mathcal{D}(\alpha) \circ \pi_{i} \circ Q \\
        &= S_{\alpha} \circ Q.
      \end{align*}
      Thus, $Q\colon L' \to A$ equalizes $R$ and $S$. But the universal property for equalizers guarantees us a unique morphism $h\colon L' \to L$ such that $Q = P \circ h$. We can compose both sides of this equation on the left with $\pi_{\mathcal{D}(j)}$ to find
      \begin{equation*}
        \pi_{\mathcal{D}(j)} \circ Q = \pi_{\mathcal{D}(j)} \circ P \circ h,
      \end{equation*}
      i.e.
      \begin{equation*}
        Q_{j} = P_{j} \circ h
      \end{equation*}
      as required.
  \end{enumerate}
\end{proof}

\subsection{The hom functor preserves limits}
\begin{theorem}
  \label{thm:homfunctorpreserveslimits}
  Let $\mathsf{C}$ be a locally small category. The hom functor $\Hom_{\mathsf{C}}\colon \mathsf{C}^{\mathrm{op}} \times \mathsf{C} \rightsquigarrow \mathsf{Set}$ preserves limits in the second argument, i.e. for $\mathcal{D}\colon \mathsf{J} \rightsquigarrow \mathsf{C}$ a diagram in $\mathsf{C}$ we have a natural isomorphism
  \begin{equation*}
    \Hom_{\mathsf{C}}(Y, \lim_{\leftarrow}\mathcal{D}) \simeq \lim_{\leftarrow}\Hom_{\mathsf{C}}(Y, \mathcal{D}),
  \end{equation*}
  where the limit on the RHS is over the hom-set diagram
  \begin{equation*}
    \Hom_{\mathsf{C}}(Y, -) \circ \mathcal{D} \colon \mathsf{J} \rightsquigarrow \mathsf{Set}.
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $L$ be the limit over the diagram $\mathcal{D}$. Then for any map $f\colon Y \to L$, there is a cone from $Y$ to $\mathcal{D}$ by composition, and for any cone with tip $Y$ over $\mathcal{D}$ we get a map $f\colon Y \to L$ from the universal property of limits. Thus, there is a bijection
  \begin{equation*}
    \Hom_{\mathsf{C}}(Y, \lim_{\leftarrow}\mathcal{D}) \simeq \mathrm{Cones}(Y, \mathcal{D}),
  \end{equation*}
  which is natural in $Y$ since the diagram
  \begin{equation*}
    \begin{tikzcd}
      \Hom_{\mathsf{C}}(Y, \lim_{\leftarrow}\mathcal{D})
      \arrow[r, "(-) \circ f"]
      \arrow[d]
      & \Hom_{\mathsf{C}}(Z, \lim_{\leftarrow}\mathcal{D})
      \arrow[d]
      \\
      \mathrm{Cones}(Y, \mathcal{D})
      \arrow[r, "(-) \circ f"]
      & \mathrm{Cones}(Z, \mathcal{D})
    \end{tikzcd}
  \end{equation*}
  trivially commutes. We will be done if we can show that there is also a natural isomorphism
  \begin{equation*}
    \mathrm{Cones}(Y, \mathcal{D}) \simeq \lim_{\leftarrow} \Hom_{\mathsf{C}}(Y, \mathcal{D}).
  \end{equation*}

  Let us understand the elements of the set $\lim_{\leftarrow}\Hom_{\mathsf{C}}(Y, \mathcal{D})$. The diagram 
  \begin{equation*}
    \Hom_{\mathsf{C}}(Y, -) \circ \mathcal{D} \colon \mathsf{J} \rightsquigarrow \mathsf{Set}
  \end{equation*}
  maps $J \in \Obj(\mathsf{J})$ to $\Hom_{\mathsf{C}}(Y, \mathcal{D}(J)) \in \Obj(\mathsf{Set})$. A universal cone over this diagram is a set $S$ together with, for each $J_{i} \in \Obj(\mathsf{J})$, a function
  \begin{equation*}
    f_{i}\colon S \to \Hom_{\mathsf{C}}(Y, \mathcal{D}(J_{i}))
  \end{equation*}
  such that for each $\alpha \in \Hom_{\mathsf{J}}(J_{i}, J_{j})$, the diagram
  \begin{equation*}
    \begin{tikzcd}[row sep=huge]
      & S
      \arrow[dl, swap, "f_{i}"]
      \arrow[dr, "f_{j}"]
      \\
      \Hom_{\mathsf{C}}(Y, \mathcal{D}(J_{i}))
      \arrow[rr, swap, "\mathcal{D}(\alpha) \circ (-)"]
      & & \Hom_{\mathsf{C}}(Y, \mathcal{D}(J_{j}))
    \end{tikzcd}
  \end{equation*}
  commutes.

  Now pick any element $s \in S$. Each $f_{i}$ maps this to a function $Y \to \mathcal{D}(J_{i})$ which makes the diagram
  \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
      & Y
      \arrow[dl, swap, "f_{i}(s)"]
      \arrow[dr, "f_{j}(s)"]
      \\
      \mathcal{D}(J_{i})
      \arrow[rr, swap, "\mathcal{D}(\alpha)"]
      & & \mathcal{D}(J_{j})
    \end{tikzcd}
  \end{equation*}
  commute. But this is exactly an element of $\mathrm{Cones}(Y, \mathcal{D})$. Conversely, every element of $\mathrm{Cones}(Y, \mathcal{D})$ gives us an element of $S$, so we have a bijection 
  \begin{equation*}
    \mathrm{Cones}(Y, \mathcal{D}) \simeq \lim_{\leftarrow} \Hom_{\mathsf{C}}(Y, \mathcal{D}),
  \end{equation*}
  which is natural as required.
\end{proof}

\begin{corollary}
  The first slot of the hom functor turns colimits into limits. That is, 
  \begin{equation*}
    \Hom_{_\mathsf{C}}(\lim_{\rightarrow}\mathcal{D}, Y) \simeq \lim_{\leftarrow}\Hom_{C}(\mathcal{D}, Y).
  \end{equation*}
\end{corollary}
\begin{proof}
  Dual to that of \hyperref[thm:homfunctorpreserveslimits]{Theorem \ref*{thm:homfunctorpreserveslimits}}.
\end{proof}


\subsection{Filtered colimits and ind-objects}
\begin{definition}[preorder]
  \label{def:preorder}
  Let $S$ be a set. A \defn{preoreder} on $S$ is a binary relation $\leq$ which is 
  \begin{enumerate}
    \item reflexive ($a \leq a$)

    \item transitive (if $a \leq b$ and $b \leq c$, then $a \leq c$)
  \end{enumerate}

  A preorder is said to be \defn{directed} if for any two objects $a$ and $b$, there exists an `upper bound', i.e. an object $r$ such that $a \leq r$ and $b \leq r$.
\end{definition}

Filtered categories are a generalization of filtered preorders.

\begin{definition}[filtered category]
  \label{def:filteredcategory}
  A category $\mathsf{J}$ is \defn{filtered} if
  \begin{itemize}
    \item for each pair of objects $J$, $J' \in \Obj(\mathsf{J})$, there exists an object $K$ and morphisms $J \to K$ and $J' \to K$.
      \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
          J
          \arrow[dr]
          \\
          & K
          \\
          J'
          \arrow[ur]
        \end{tikzcd}
      \end{equation*}

      That is, every diagram with two objects and no morphisms is the base of a cocone.

    \item For every pair of morphisms $i$, $j\colon J \to J'$, there exists an object $K$ and a morphism $f\colon J' \to K$ such that $f \circ i = f \circ j$, i.e. the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}[row sep=tiny]
          J
          \arrow[r, shift left, "i"]
          \arrow[r, shift right, swap, "j"]
          & J'
          \arrow[r, "f"]
          & K
        \end{tikzcd}
      \end{equation*}

      That is, every diagram of the form $\begin{tikzcd}[column sep=small]\bullet \arrow[r, shift left] \arrow[r, shift right] & \bullet \end{tikzcd}$ is the base of a cocone.
  \end{itemize}
\end{definition}

\begin{definition}[filtered colimit]
  \label{def:filteredcolimit}
  A \defn{filtered colimit} is a colimit over a diagram $\mathcal{D}\colon \mathsf{J} \to \mathsf{C}$, where $\mathsf{J}$ is a filtered category.
\end{definition}

We now define the so-called \emph{category of inductive objects} (or simply \emph{ind-objects}).
\begin{definition}[category of ind-objects]
  \label{def:categoryofindobjects}
  Let $\mathsf{C}$ be a category. We define the \defn{category of ind-objects} of $\mathsf{C}$, denote $\mathsf{Ind}(\mathsf{C})$ as follows. 
  \begin{itemize}
    \item The objects $F \in \Obj(\mathsf{Ind}(\mathsf{C}))$ are defined to be filtered colimits of objects of diagrams $\mathcal{F}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$.

    \item For two objects $F = \lim_{\rightarrow d}\mathcal{F}_{d}$ and $G = \lim_{\rightarrow e}\mathcal{G}_{e}$, the morphisms $\Hom_{\mathsf{Ind}(\mathsf{C})}(F, G)$ are defined to be the set
      \begin{equation*}
        \Hom_{\mathsf{Ind}(\mathsf{C})}(F, G) = \lim_{\leftarrow d} \lim_{\rightarrow e} \Hom_{\mathsf{C}}(\mathcal{F}_{d}, \mathcal{G}_{e}).
      \end{equation*}
  \end{itemize}
\end{definition}

\begin{note}
  \label{note:fullinclusionintoindcategory}
  There is a fully faithful embedding $\mathsf{C} \hookrightarrow\mathsf{Ind}(\mathsf{C})$ which exhibits any object $A$ as the colimit over the trivial diagram $\begin{tikzcd} \bullet \arrow[loop right] \end{tikzcd}$.
\end{note}

The importance of the category of ind-objects can be seen in the following example.
\begin{example}
  \label{eg:inductiveobjectsaddinfinitedimensionalvectorspaces}
  Let $V$ be an infinite-dimensional vector space over some field $k$. Then $V$ can be realized as an object in the category $\mathsf{Ind}(\mathsf{FinVect})$.

  In fact, there is an equivalence of categories $\mathsf{Vect}_{k} \simeq \mathsf{Ind}(\mathsf{FinVect}_{k})$. Similarly, there is an equivalence of categories $\mathsf{SVect}_{k} \simeq \mathsf{Ind}(\mathsf{FinSVect}_{k})$
\end{example}
\begin{note}
  This is stated without proof as Example 3.39 of \cite{nlab-deligne-theorem}. I haven't been able to find a real source for it.
\end{note}

\section{Adjunctions} \label{sec:adjunctions}
Consider the following functors:
\begin{itemize}
  \item $\mathcal{U}\colon \mathsf{Grp} \rightsquigarrow \mathsf{Set}$, which sends a group to its underlying set, and

  \item $\mathcal{F}\colon \mathsf{Set} \rightsquigarrow \mathsf{Grp}$, which sends a set to the free group on it.
\end{itemize}

The functors $\mathcal{U}$ and $\mathcal{F}$ are dual in the following sense: $\mathcal{U}$ is the most efficient way of moving from $\mathsf{Grp}$ to $\mathsf{Set}$ since all groups are in particular sets; $\mathcal{F}$ might be thought of as providing the most efficient way of moving from $\mathsf{Set}$ to $\mathsf{Grp}$. But how would one go about formalizing this?

Well, these functors have the following property. Let $S$ be a set, $G$ be a group, and let $f\colon S \to \mathcal{U}(G)$, $s \mapsto f(s)$ be a set-function. Then there is an associated group homomorphism $\tilde{f}\colon \mathcal{F}(S) \to G$, which sends $s_{1}s_{2}\dots s_{n} \mapsto f(s_{1}s_{2}\dots s_{n}) = f(s_{1})\cdots f(s_{n})$. In fact, $\tilde{f}$ is the unique homomorphism $\mathcal{F}(S) \to G$ such that $f(s) = \tilde{f}(s)$ for all $s \in S$. 

Similarly, for every group homomorphism $g\colon \mathcal{F}(S) \to G$, there is an associated function $S \to \mathcal{U}(G)$ given by restricting $g$ to $S$. In fact, this is the unique function $\mathcal{F}(S) \to G$ such that $f(s) = \tilde{f}(s)$ for all $s \in S$.

Thus for each $f \in \Hom_{\mathsf{Grp}}(S, \mathcal{U}(G))$ we can construct an $\tilde{f} \in \Hom_{\mathsf{Set}}(\mathcal{F}(S), G)$, and vice versa.

Let us add some mathematical scaffolding to the ideas explored above. We build two functors $\mathsf{Set}^{\mathrm{op}} \times \mathsf{Grp} \rightsquigarrow \mathsf{Set}$ as follows.
\begin{enumerate}
  \item Our first functor maps the object $(S, G) \in \Obj(\mathsf{Set}^{\mathrm{op}} \times \mathsf{Grp})$ to the hom-set $\Hom_{\mathsf{Grp}}(\mathcal{F}(S), G)$, and a morphism $(\alpha, \beta)\colon (S,G) \to (S', G')$ to a function 
    \begin{equation*}
      \Hom_{\mathsf{Grp}}(\mathcal{F}(S), G) \to \Hom_{\mathsf{Grp}}(\mathcal{F}(S'), G');\qquad m \mapsto \mathcal{F}(\alpha) \circ m \circ \beta
    \end{equation*}

  \item Our second functor maps $(S, G)$ to $\Hom_{\mathsf{Set}}(S, \mathcal{U}(G))$, and $(\alpha, \beta)$ to 
    \begin{equation*}
      m \mapsto \alpha \circ m \circ \mathcal{U}(\beta).
    \end{equation*}
\end{enumerate}
We can define a natural isomorphism $\Phi$ between these functors with components
\begin{equation*}
  \Phi_{S, G}\colon \Hom_{\mathsf{Grp}}(\mathcal{F}(S), G) \to \Hom_{\mathsf{Set}}(S, \mathcal{U}(G));\qquad f \to \tilde{f}.
\end{equation*}

This mathematical structure turns out to be a recurring theme in the study of categories, called an \emph{adjuction}. We have encountered it before: we saw in \hyperref[eg:naturaltransformationsforcccs]{Example \ref*{eg:naturaltransformationsforcccs}} that there was a natural bijection between 
\begin{equation*}
  \Hom_{\mathsf{C}}(X \times (-), B)\qquad\text{and}\qquad \Hom_{\mathsf{C}}(X, B^{(-)}).
\end{equation*}

\begin{definition}[hom-set adjunction]
  \label{def:homsetadjunction}
  Let $\mathsf{C}$, $\mathsf{D}$ be categories and $\mathcal{F}$, $\mathcal{G}$ functors as follows.
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C} 
      \arrow[r, rightsquigarrow, bend left, "\mathcal{F}"]
      & \mathsf{D}
      \arrow[l, rightsquigarrow, bend left, "\mathcal{G}"]
    \end{tikzcd}
  \end{equation*}
  We say that \defn{$\mathcal{F}$ is left-adjoint to $\mathcal{G}$} (or equivalently $\mathcal{G}$ is right-adjoint to $\mathcal{F}$) and write $\mathcal{F} \dashv \mathcal{G}$ if there is a natural isomorphism
  \begin{equation*}
    \Phi\colon \Hom_{\mathsf{D}}(\mathcal{F}(-), -) \Rightarrow \Hom_{\mathsf{C}}(-, \mathcal{G}(-)),
  \end{equation*}
  which fits between $\mathcal{F}$ and $\mathcal{G}$ like this.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C}^{\mathrm{op}} \times \mathsf{D} 
      \arrow[r, bend left, rightsquigarrow, anchor=s, "{\Hom_{\mathsf{D}}(\mathcal{F}(-), -)}", ""{name=U, below}]
      \arrow[r, bend right, rightsquigarrow, swap, "{\Hom_{\mathsf{D}}(-, \mathcal{G}(-))}", ""{name=D, above}]
      \arrow[from=U, to=D, Rightarrow, "\Phi"]
      & \mathsf{Set}\qquad
    \end{tikzcd}
  \end{equation*}
  The natural isomorphsim amounts to a family of bijections
  \begin{equation*}
    \Phi_{A, B}\colon \Hom_{\mathsf{D}}(\mathcal{F}(A), B) \to \Hom_{\mathsf{C}}(A, \mathcal{G}(B))
  \end{equation*}
  which satisfies the coherence conditions for a natural transformation.

\end{definition}

Here are two equivalent definitions which are often used.

\begin{definition}[unit-counit adjunction]
  \label{def:unitcounitadjunction}
  We say that two functors $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ and $\mathcal{G}\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ form a \defn{unit-counit adjunction} if there are two natural transformations
  \begin{equation*}
    \eta\colon 1_{\mathsf{C}} \Rightarrow \mathcal{G} \circ \mathcal{F},\qquad\text{and}\qquad \varepsilon\colon \mathcal{F} \circ \mathcal{G} \Rightarrow 1_{\mathsf{D}},
  \end{equation*}
  called the \emph{unit} and \emph{counit} respectively, which make the following so-called \emph{triangle diagrams} 
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{F}
      \arrow[r, Rightarrow, "\mathcal{F}\eta"]
      \arrow[rd, Rightarrow, swap, "1_{\mathcal{F}}"]
      & \mathcal{FGF}
      \arrow[d, Rightarrow, "\varepsilon\mathcal{F}"]
      \\
      & \mathcal{F}
    \end{tikzcd},
    \qquad
    \begin{tikzcd}
      \mathcal{G}
      \arrow[r, Rightarrow, "\eta\mathcal{G}"]
      \arrow[rd, Rightarrow, swap, "1_{\mathcal{G}}"]
      & \mathcal{GFG}
      \arrow[d, Rightarrow, "\mathcal{G}\varepsilon"]
      \\
      & \mathcal{G}
    \end{tikzcd}
  \end{equation*}
  commute.

  The triangle diagrams take quite some explanation. The unit $\eta$ is a natural transformation $1_{\mathsf{C}} \Rightarrow \mathcal{G} \circ \mathcal{F}$. We can draw it like this.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{C} 
      \arrow[r, rightsquigarrow, bend left, "1_{\mathsf{C}}"{name=U}]
      \arrow[r, rightsquigarrow, swap, bend right, "\mathcal{G} \circ \mathcal{F}"{name=D}]
      \arrow[from=U, to=D, Rightarrow, "\eta"]
      & \mathsf{C}
    \end{tikzcd}.
  \end{equation*}
  Analogously, we can draw $\varepsilon$ like this.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      \mathsf{D} 
      \arrow[r, rightsquigarrow, bend left, "\mathcal{F} \circ \mathcal{G}"{name=U}]
      \arrow[r, rightsquigarrow, swap, bend right, "1_{\mathsf{D}}"{name=D}]
      \arrow[from=U, to=D, Rightarrow, "\varepsilon"]
      & \mathsf{D}
    \end{tikzcd}.
  \end{equation*}

  We can arrange these artfully like so. 
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, rightsquigarrow, "\mathcal{F}" description]
      \arrow[rr, rightsquigarrow, bend left=75, "1_{\mathsf{C}}"{name=UU}]
      \arrow[rr, swap, rightsquigarrow, bend left=30, "\mathcal{G} \circ \mathcal{F}"{name=U}]
      & \mathsf{D}
      \arrow[r, rightsquigarrow, "\mathcal{G}" description]
      \arrow[rr, rightsquigarrow, bend right=75, swap, "1_{\mathsf{D}}"{name=DD}]
      \arrow[rr, rightsquigarrow, bend right=30, "\mathcal{F} \circ \mathcal{G}"{name=D}]
      \arrow[from=D, to=DD, Rightarrow, "\varepsilon"]
      & \mathsf{C}
      \arrow[from=UU, to=U, Rightarrow, swap, "\eta"]
      \arrow[r, rightsquigarrow, "\mathcal{F}" description]
      & \mathsf{D}
    \end{tikzcd}
  \end{equation*}
  Notice that we haven't actually \emph{done} anything; this diagram is just the diagrams for the unit and counit, plus some extraneous information.

  We can whisker the $\eta$ on top from the right, and the $\varepsilon$ below from the left, to get the following diagram,
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, rightsquigarrow, "\mathcal{F}" description]
      \arrow[rrr, rightsquigarrow, bend right=60, swap, "\mathcal{F}"{name=D}]
      \arrow[rrr, rightsquigarrow, bend left=60, "\mathcal{F}"{name=U}]
      \arrow[rrr, rightsquigarrow, bend left=30, swap, "\mathcal{F} \circ \mathcal{G} \circ \mathcal{F}"{name=MU}]
      \arrow[rrr, rightsquigarrow, bend right=30, "\mathcal{F} \circ \mathcal{G} \circ \mathcal{F}"{name=MD}]
      & \mathsf{D}
      \arrow[from=U, to=MU, Rightarrow, swap, "\mathcal{F}\eta"]
      \arrow[r, rightsquigarrow, "\mathcal{G}" description]
      \arrow[from=MD, to=D, Rightarrow, "\varepsilon\mathcal{F}"]
      & \mathsf{C}
      \arrow[r, rightsquigarrow, "\mathcal{F}" description]
      & \mathsf{D}
    \end{tikzcd}
  \end{equation*}
  then consolidate to get this:
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathsf{C}
      \arrow[r, rightsquigarrow, bend left=60, "\mathcal{F}"{name=U}]
      \arrow[r, rightsquigarrow, "\mathcal{F} \circ \mathcal{G} \circ \mathcal{F}"{name=M} description]
      \arrow[r, rightsquigarrow, bend right=60, swap, "\mathcal{F}"{name=D}]
      & \mathsf{D}
      \arrow[from=U, to=M, Rightarrow, "\mathcal{F}\eta"]
      \arrow[from=M, to=D, Rightarrow, "\varepsilon\mathcal{F}"]
    \end{tikzcd}
  \end{equation*}
  We can then take the composition $\varepsilon \mathcal{F} \circ \mathcal{F}\eta$ to get a natural transformation $\mathcal{F} \Rightarrow \mathcal{F}$
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathsf{C} 
      \arrow[r, bend left, rightsquigarrow, "\mathcal{F}"{name=U}]
      \arrow[r, bend right, swap, rightsquigarrow, "\mathcal{F}"{name=D}]
      \arrow[from=U, to=D, Rightarrow, "\varepsilon\mathcal{F} \circ \mathcal{F}\eta"]
      &\mathsf{D};
    \end{tikzcd}
  \end{equation*}
  the first triangle diagram says that this must be the same as the identity natural transformation $1_{\mathcal{F}}$.

  The second triangle diagram is analogous.
\end{definition}

\begin{lemma}
  The functors $\mathcal{F}$ and $\mathcal{G}$ form a unit-counit adjunction if and only if they form a hom-set adjunction.
\end{lemma}
\begin{proof}
  Suppose $\mathcal{F}$ and $\mathcal{G}$ form a hom-set adjunction with natural isomorphism $\Phi$. Then for any $A \in \Obj(\mathsf{C})$, we have $\mathcal{F}(A) \in \Obj(\mathsf{D})$, so $\Phi$ give us a bijection
  \begin{equation*}
    \Phi_{A, \mathcal{F}(A)}\colon \Hom_{\mathsf{D}}(\mathcal{F}(A), \mathcal{F}(A)) \to \Hom_{\mathsf{C}}(A, (\mathcal{G} \circ \mathcal{F})(A)).
  \end{equation*}
  We don't know much in general about $\Hom_{\mathsf{D}}(\mathcal{F}(A), \mathcal{F}(A))$, but the category axioms tell us that it always contains $1_{\mathcal{F}(A)}$. We can use $\Phi_{A, \mathcal{F}(A)}$ to map this to
  \begin{equation*}
    \Phi_{A, \mathcal{F}(A)}(1_{\mathcal{F}(A)}) \in \Hom_{\mathsf{C}}(A, (\mathcal{G} \circ \mathcal{F})(A)).
  \end{equation*}
  Let's call $\Phi_{A, \mathcal{F}(A)}(1_{\mathcal{F}(A)}) = \eta_{A}$. 

  Similarly, if $B \in \Obj(\mathsf{D})$, then $\mathcal{G}(B) \in \Obj(\mathsf{C})$, so $\Phi$ gives us a bijection
  \begin{equation*}
    \Phi_{\mathcal{G}(B), B}\colon \Hom_{\mathsf{D}}((\mathcal{F} \circ \mathcal{G})(B), B) \to \Hom_{\mathsf{C}}(\mathcal{G}(B), \mathcal{G}(B)).
  \end{equation*}

  Since $\Phi_{\mathcal{G}(B), B}$ is a bijection, it is invertible, and we can evaluate the inverse on $1_{\mathcal{G}(B)}$. Let's call
  \begin{equation*}
    \Phi^{-1}_{\mathcal{G}(B), B}(1_{\mathcal{G}(B)}) = \varepsilon_{B}.
  \end{equation*}

  Clearly, $\eta_{A}$ and $\varepsilon_{B}$ are completely determined by $\Phi$ and $\Phi^{-1}$ respectively. It turns out that the converse is also true; in a manner reminiscent of the proof of the Yoneda lemma, we can express $\Phi_{A, B}$ in terms of $\eta$, and $\Phi^{-1}_{A, B}$ in terms of $\varepsilon$, for \emph{any} $A$ and $B$. Here's how this is done.

  We use the naturality of $\Phi$. We know that for any $A \in \Obj(\mathsf{C})$, $B \in \Obj(\mathsf{D})$, and $g\colon \mathcal{F}(A) \to B$, the following diagram has to commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \Hom_{\mathsf{D}}(\mathcal{F}(A), \mathcal{F}(A))
      \arrow[r, "g \circ (-)"]
      \arrow[d, swap, "\Phi_{A, \mathcal{F}(A)}"]
      & \Hom_{\mathsf{D}}(\mathcal{F}(A), B)
      \arrow[d, "\Phi_{A, B}"]
      \\
      \Hom_{\mathsf{C}}(A, (\mathcal{G} \circ \mathcal{F})(A))
      \arrow[r, "\mathcal{G}(g) \circ (-)"]
      & \Hom_{\mathsf{C}}(A, \mathcal{G}(B)).
    \end{tikzcd}
  \end{equation*}
  Let's start at the top left with $1_{\mathcal{F}(A)}$ and see what happens. Taking the top road to the bottom right, we have $\Phi_{A, B}(g)$, and from the bottom road we have $\mathcal{G}(g) \circ \eta_{A}$. The diagram commutes, so we have
  \begin{equation*}
    \Phi_{A, B}(g) = \mathcal{G}(g) \circ \eta_{A}.
  \end{equation*}
  Similarly, the commutativity of the diagram
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \Hom_{\mathsf{D}}((\mathcal{F} \circ \mathcal{G})(B), B)
      \arrow[r, "(-) \circ \mathcal{F}(f)"]
      & \Hom_{\mathsf{D}}(\mathcal{F}(A), B)
      \\
      \Hom_{\mathsf{C}}(\mathcal{G}(B), \mathcal{G}(B))
      \arrow[r, "(-) \circ f"]
      \arrow[u, "{\Phi^{-1}_{\mathcal{G}(B), B}}"]
      & \Hom_{\mathsf{C}}(A, \mathcal{G}(B))
      \arrow[u, swap, "\Phi^{-1}_{A, B}"]
    \end{tikzcd}
  \end{equation*}
  means that, for any $f\colon A \to \mathcal{G}(B)$,
  \begin{equation*}
    \Phi^{-1}_{A, B}(f) = \varepsilon_{B} \circ \mathcal{F}(f) 
  \end{equation*}

  To show that $\eta$ and $\varepsilon$ as defined here satisfy the triangle identities, we need to show that for all $A \in \Obj(\mathsf{C})$ and all $B \in \Obj(\mathsf{D})$,
  \begin{equation*}
    (\varepsilon\mathcal{F})_{A} \circ (\mathcal{F}\eta)_{A} = (1_{\mathcal{F}})_{A}\qquad\text{and}\quad (\mathcal{G}\varepsilon)_{B} \circ (\eta\mathcal{G})_{B} = (1_{\mathcal{G}})_{B}.
  \end{equation*}
  We have
  \begin{equation*}
    (\varepsilon\mathcal{F})_{A} \circ (\mathcal{F}\eta)_{A} = \varepsilon_{\mathcal{F}(A)} \circ \mathcal{F}(\eta_{A}) = \Phi^{-1}_{A, \mathcal{F}(A)}(\eta_{A}) = 1_{A} = (1_{\mathcal{F}})_{A}
  \end{equation*}
  and
  \begin{equation*}
    (\mathcal{G}_{\varepsilon})_{B} \circ (\eta\mathcal{G})_{B} = \mathcal{G}(\varepsilon_{B}) \circ \eta_{\mathcal{G}(B)} = \Phi_{\mathcal{G}(B), B}(\varepsilon_{B}) = 1_{B} = (1_{\mathcal{G}})_{B}.
  \end{equation*}
\end{proof}


\begin{definition}[adjunct]
  \label{def:adjunct}
  Let $\mathcal{F} \dashv \mathcal{G}$ be an adjunction as follows.
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, bend left, rightsquigarrow, "\mathcal{F}"]
      & \mathsf{D}
      \arrow[l, bend left, rightsquigarrow, "\mathcal{G}"]
    \end{tikzcd}
  \end{equation*}
  Then for each $A \in \Obj(\mathsf{C})$ $B \in \Obj(\mathsf{D})$, we have a natural isomorphism (i.e. a bijection)
  \begin{equation*}
    \Phi_{A, B}: \Hom_{\mathsf{D}}(\mathcal{F}(A), B) \to \Hom_{\mathsf{C}}(A, \mathcal{G}(B)).
  \end{equation*}
  Thus, for each $f \in \Hom_{\mathsf{D}}(\mathcal{F}(A), B)$ there is a corresponding element $\tilde{f} \in \Hom_{\mathsf{C}}(A, \mathcal{G}(B))$, and vice versa. The morphism $\tilde{f}$ is called the \defn{adjunct} of $f$, and $f$ is called the adjunct of $\tilde{f}$.
\end{definition}

\begin{lemma}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories, $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ and $\mathcal{G}$, $\mathcal{G}'\colon \mathsf{D} \rightsquigarrow \mathsf{C}$ functors,
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, rightsquigarrow, "\mathcal{F}"]
      & \mathsf{D}
      \arrow[l, rightsquigarrow, bend right=45, swap, "\mathcal{G}"]
      \arrow[l, rightsquigarrow, bend left=45, "\mathcal{G}'"]
    \end{tikzcd}
  \end{equation*}
  and suppose that $\mathcal{G}$ and $\mathcal{G}'$ are both right-adjoint to $\mathcal{F}$. Then there is a natural isomorphism $\mathcal{G} \Rightarrow \mathcal{G}'$.
\end{lemma}
\begin{proof}
  Since any adjunction is a hom-set adjunction, we have two isomorphisms
  \begin{equation*}
    \Phi_{C, D}\colon \Hom_{\mathsf{D}}(\mathcal{F}(C), D) \Rightarrow \Hom_{\mathsf{C}}(C, \mathcal{G}(D))\qquad\text{and}\qquad \Psi_{C, D}\colon \Hom_{\mathsf{D}}(\mathcal{F}(C), D) \Rightarrow \Hom_{\mathsf{C}}(C, \mathcal{G}'(D))
  \end{equation*}
  which are natural in both $C$ and $D$. By \hyperref[lemma:naturalisomorphismshaveinverses]{Lemma \ref*{lemma:naturalisomorphismshaveinverses}}, we can construct the inverse natural isomorphism
  \begin{equation*}
    \Phi^{-1}_{C, D}\colon \Hom_{\mathsf{C}}(C, \mathcal{G}(D)) \Rightarrow \Hom_{\mathsf{D}}(\mathcal{F}(C), D),
  \end{equation*}
  and compose it with $\Psi$ to get a natural isomorphsim
  \begin{equation*}
    (\Psi \circ \Phi^{-1})_{C, D}\colon \Hom_{\mathsf{C}}(C, \mathcal{G}(D)) \Rightarrow \Hom_{\mathsf{C}}(C, \mathcal{G}'(D)).
  \end{equation*}

  Thus for any morphism $f\colon D \to E$, the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \Hom_{\mathsf{C}}(C, \mathcal{G}(D))
      \arrow[r, "{\Hom_{\mathsf{C}}(C, \mathcal{G}(f))}"]
      \arrow[d, swap, "{(\Psi \circ \Phi^{-1})_{C, D}}"]
      & \Hom_{\mathsf{C}}(C, \mathcal{G}(E))
      \arrow[d, "{(\Psi \circ \Phi^{-1})_{C, E}}"]
      \\
      \Hom_{\mathsf{C}}(C, \mathcal{G}'(D))
      \arrow[r, "{\Hom_{\mathsf{C}}(C, \mathcal{G}'(f))}"]
      & \Hom_{\mathsf{C}}(C, \mathcal{G}'(E))
    \end{tikzcd}
  \end{equation*}
  But the fully faithfulness of the Yoneda embedding tells us that there exist isomorphisms $\mu_{D}$ and $\mu_{E}$ making the following diagram commute,
  \begin{equation*}
    \begin{tikzcd}
      \mathcal{G}(D)
      \arrow[r, "\mathcal{G}(f)"]
      \arrow[d, swap, "\mu_{D}"]
      & \mathcal{G}(E)
      \arrow[d, "\mu_{E}"]
      \\
      \mathcal{G}'(D)
      \arrow[r, "\mathcal{G}'(f)"]
      & \mathcal{G}'(E)
    \end{tikzcd}
  \end{equation*}
  and taking the collection of all such $\mu_{(-)}$ gives us a natural isomorphism $\mu\colon \mathcal{G} \Rightarrow \mathcal{G}'$.
\end{proof}

\begin{note}
  We do not give very many examples of adjunctions now because of the frequency with which category theory graces us with them. Howver, it is worth mentioning a specific class of adjunctions: the so-called \emph{free-forgetful adjunctions}. There are many \emph{free} objects in mathematics: free groups, free modules, free vector spaces, free categories, etc. These are all unified by the following property: the functors defining them are all left adjoints.

  Let us take a specific example: the free vector space over a set. This takes a set $S$ and constructs a vector space which has as a basis the elements of $S$.

  There is a forgetful functor $\mathcal{U}\colon \mathsf{Vect}_{k} \rightsquigarrow \mathsf{Set}$ which takes any set and returns the set underlying it. There is a functor $\mathcal{F}\colon \mathsf{Set} \rightsquigarrow \mathsf{Vect}_{k}$, which takes a set and returns the free vector space on it. It turns out that there is an adjunction $\mathcal{F} \dashv \mathcal{U}$.

  And this is true of any free object! (In fact by definition.) In each case, the functor giving the free object is left adjoint to a forgetful functor.
\end{note}

\begin{theorem}
  \label{thm:rightadjointspreservelimits}
  Let $\mathsf{C}$ and $\mathsf{D}$ be categories and $\mathcal{F}$ and $\mathcal{G}$ functors as follows. 
  \begin{equation*}
    \begin{tikzcd}
      \mathsf{C}
      \arrow[r, rightsquigarrow, bend left, "\mathcal{F}"]
      & \mathsf{D}
      \arrow[l, rightsquigarrow, bend left, "\mathcal{G}"]
    \end{tikzcd}
  \end{equation*}
  Let $\mathcal{F} \dashv \mathcal{G}$ be an adjunction. Then $\mathcal{G}$ preserves limits, i.e. if $\mathcal{D}\colon \mathsf{J} \to \mathsf{C}$ is a diagram and $\lim_{\leftarrow i}\mathcal{D}_{i}$ exists in $\mathsf{C}$, then
  \begin{equation*}
    \mathcal{G}(\lim_{\leftarrow i}\mathcal{D}_{i}) \simeq \lim_{\leftarrow i} (\mathcal{G} \circ \mathcal{D}_{i}).
  \end{equation*}
\end{theorem}
\begin{proof}
  We have the following chain of isomorphisms, natural in $Y \in \Obj(\mathsf{D})$.
  \begin{align*}
    \Hom_{\mathsf{D}}(Y, \mathcal{G}(\lim_{\leftarrow i}\mathcal{D}_{i})) &\simeq \Hom_{\mathsf{C}}(\mathcal{F}(Y), \lim_{\leftarrow i}\mathcal{D}_{i}) \\
    &\simeq \lim_{\leftarrow i} \Hom_{\mathsf{C}}(\mathcal{G}(Y), \mathcal{D}_{i}) &\left(\substack{\text{Hom functor commutes with} \\ \text{limits: \hyperref[thm:homfunctorpreserveslimits]{Theorem \ref*{thm:homfunctorpreserveslimits}}}}\right) \\
    &\simeq \lim_{\leftarrow i} \Hom_{\mathsf{D}}(Y, \mathcal{G}\circ \mathcal{D}_{i}) \\
    &\simeq \Hom_{\mathsf{C}}(Y, \lim_{\leftarrow i}(\mathcal{G}\circ \mathcal{D}_{i})).
  \end{align*}

  By the Yoneda lemma, specifically \hyperref[cor:yonedaembeddingrespectsisomorphisms]{Corollary \ref*{cor:yonedaembeddingrespectsisomorphisms}}, we have a natural isomorphism
  \begin{equation*}
    \mathcal{G}(\lim_{\leftarrow i}\mathcal{D}_{i}) \simeq \lim_{\leftarrow i}(\mathcal{G} \circ \mathcal{D}_{i}).
  \end{equation*}
\end{proof}

\begin{corollary}
  \label{cor:leftadjointspreservecolimits}
  Any functor $\mathcal{F}$ which is a left-ajoint preserves colimits.
\end{corollary}
\begin{proof}
  Dual to the proof of \hyperref[thm:rightadjointspreservelimits]{Theorem \ref*{thm:rightadjointspreservelimits}}.
\end{proof}

\section{Monoidal categories} \label{sec:monoidalcategories}
Recall the definition of a monoid (\hyperref[def:monoid]{Definition \ref*{def:monoid}}). Basically, a monoid is a group without inverses. 

Monoidal categories are the first ingredient in the categorification and generalization of the tensor product. Roughly speaking, the tensor product allows us to multiply two vector spaces to produce a new vector space. There are natural isomorphisms making this multiplication associative, and an `identity vector space' given by the ground field regarded as a one-dimensional vector space over itself. This means that the tensor product gives the set of all vector spaces the structure of a monoid.

A monoidal category will be a category in which the objects have the structure of a monoid, i.e. there is a suitably defined `multiplication' which is unital and associative. The prototypical example of categorical multiplication is the product (see \hyperref[eg:universalpropertyofproducts]{Example \ref*{eg:universalpropertyofproducts}}), although it is far from the only one.

\begin{definition}[monoidal category]
  \label{def:monoidalcategory}
  A \defn{monoidal category} is a category $\mathsf{C}$ equipped with a monoidal structure. A monoidal structure is the following:
  \begin{itemize}
    \item A bifunctor (\hyperref[def:bifunctor]{Definition \ref*{def:bifunctor}}) $\otimes\colon \mathsf{C} \times \mathsf{C} \to \mathsf{C}$ called the \emph{tensor product},
    \item An object $I$ called the \emph{unit object}, and
    \item Three natural isomorphisms (\hyperref[def:naturalisomorphism]{Definition \ref*{def:naturalisomorphism}}) subject to coherence conditions expressing the fact that the tensor product 
      \begin{itemize}
        \item is associative: there is a natural isomorphism $\alpha$
          \begin{equation*}
            \begin{tikzcd}[column sep=huge]
              \mathsf{C} \times \mathsf{C} \times \mathsf{C} 
              \arrow[r, rightsquigarrow, bend left, anchor=south, "(\_ \otimes \_) \otimes \_", ""{name=U below}]
              \arrow[r, rightsquigarrow, swap, bend right, anchor=north, "\_ \otimes (\_ \otimes \_)", ""{name=D, above}]
              \arrow[Rightarrow, from=U , to=D, "\alpha"] 
              & \mathsf{C} \qquad
            \end{tikzcd}
          \end{equation*}
          called the \emph{associator}, with components
          \begin{equation*}
            \alpha_{A,B,C}\colon (A\otimes B)\otimes C \xrightarrow{\sim} A \otimes (B \otimes C)
          \end{equation*}

        \item has left and right identity: there are two natural isomorphisms $\lambda$
          \begin{equation*}
            \begin{tikzcd}[column sep=huge]
              \mathsf{C} 
              \arrow[r, bend left, rightsquigarrow, "I \otimes \_"{name=U}]
              \arrow[r, swap, bend right, rightsquigarrow, "1_{\mathsf{C}}"{name=D}]
              \arrow[Rightarrow, from=U , to=D, "\lambda"] 
              &\mathsf{C}
            \end{tikzcd}
          \end{equation*}
          and $\rho$ 
          \begin{equation*}
            \begin{tikzcd}[column sep=huge]
              \mathsf{C} 
              \arrow[r, bend left, rightsquigarrow, "\_ \otimes I"{name=U}]
              \arrow[r, swap, bend right, rightsquigarrow, "1_{\mathsf{C}}"{name=D}]
              \arrow[Rightarrow, from=U , to=D, "\rho"] 
              &\mathsf{C}
            \end{tikzcd}
          \end{equation*}
          respectively called the \emph{left unitor} and \emph{right unitor} with components
          \begin{equation*}
            \lambda_{A}\colon I \otimes A \xrightarrow{\sim} A
          \end{equation*}
          and
          \begin{equation*}
            \rho_{A}\colon A \otimes I \xrightarrow{\sim} A.
          \end{equation*}
      \end{itemize}
  \end{itemize}
  The coherence conditions are that the following diagrams commute for all $A$, $B$, $C$, and $D\in \Obj(C)$.
  \begin{itemize}
    \item The \emph{triangle diagram}
      \begin{equation*}
        \begin{tikzcd}[column sep=tiny, row sep=6ex]
          (A \otimes I) \otimes B \arrow[rr, "\alpha_{A,I,B}"] \arrow[rd, swap, "\rho_{A} \otimes 1_{B}"] & & A \otimes(I \otimes B) \arrow[ld, "1_{A}\otimes \lambda_{B}"] \\
          & A \otimes B
        \end{tikzcd}
      \end{equation*}
    \item The \emph{home plate diagram}.\footnote{Unfortunately, the home plate, as drawn, is actually upside down.} (usually the \emph{pentagon diagram})
      \begin{equation*}
        \begin{tikzcd}[row sep=huge]
          & (A \otimes (B \otimes C)) \otimes D 
          \arrow[dr, "\alpha_{A,B\otimes C, D}"] 
          \\
          ((A \otimes B) \otimes C) \otimes D 
          \arrow[ur, "\alpha_{A,B,C}\otimes 1_{D}"] 
          \arrow[dd, swap, "\alpha_{A\otimes B, C,D}"]
          & & A \otimes ((B \otimes C) \otimes D) 
          \arrow[dd, "1_{A}\otimes\alpha_{B,C,D}"]  
          \\
          \\
          (A \otimes B) \otimes (C \otimes D) 
          \arrow[rr, swap, "\alpha_{A,B,C\otimes D}"] 
          & & A \otimes(B \otimes (C \otimes D))
        \end{tikzcd}
      \end{equation*}
  \end{itemize}

  More succinctly, a monoidal structure on a category $\mathsf{C}$ is a quintuple $(\otimes, 1, \alpha, \lambda, \rho)$. 
\end{definition}

\begin{notation}
  The notation $(\otimes, 1, \alpha, \lambda, \rho)$ is prone to change. If the associator and unitors are not important, or understood from context, they are often left out. We will often say ``Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category.''
\end{notation}

\begin{example}
  \label{eg:setisamonoidalcategory}
  The simplest (though not the prototypical) example of a monoidal category is $\mathsf{Set}$ with the Cartesian product. We have already studied this structure in some detail in \hyperref[sec:cartesianclosedcategories]{Section \ref*{sec:cartesianclosedcategories}}. We check that it satisfies the axioms in \hyperref[def:monoidalcategory]{Definition \ref*{def:monoidalcategory}}.

  \begin{itemize}
    \item The Cartesian product on $\mathsf{Set}$ \emph{is} a set-theoretic product, and can be naturally viewed, thanks to \hyperref[thm:productisafunctor]{Theorem \ref*{thm:productisafunctor}}, as the bifunctor.

    \item Any set with one element $I = \left\{ * \right\}$ functions as the unit object.

    \item For all sets $A$, $B$, $C$
      \begin{itemize}
        \item The universal property of products guarantees us an isomorphism $\alpha_{A,B,C}\colon (A \times B) \times C \to A \times (B \times C)$ which sends $((a,b),c) \mapsto (a,(b,c))$.

        \item Since $\{*\}$ is terminal, we get an isomorphism $\lambda_{A}\colon \left\{ * \right\}\times A \to A$ which sends $(*,a) \mapsto a$.

        \item Similarly, we get a map $\rho_{A}\colon A \times\left\{ * \right\} \to A$ which sends $(a, *) \mapsto a$.
      \end{itemize}
  \end{itemize}

  The pentagon and triangle diagram commute vacuously since the cartesian product is associative.
\end{example}

\begin{lemma}
  The tensor product $\otimes$ of vector spaces is a bifunctor $\mathsf{Vect}_{k} \times \mathsf{Ve ct}_{k} \rightsquigarrow \mathsf{Vect}_{k}$.
\end{lemma}
\begin{proof}
  It is clear what the domain and codomain of the tensor product is, and how it behaves on objects and morphisms (\hyperref[def:tensorproductoflinearmaps]{Definition \ref*{def:tensorproductoflinearmaps}}). The only non-trivial aspect is showing that the standard definition of the tensor product of morphisms respects composition, which is not difficult.
\end{proof}

\begin{example}
  \label{eg:vectisamonoidalcategory}
  The category $\mathsf{Vect}_{k}$ is a monoidal category with
  \begin{enumerate}
    \item The bifunctor $\otimes$ is given by the tensor product.

    \item The unit $1$ is given by the field $k$ regarded as a $1$-dimensional vector space over itself.

    \item The associator is the map which sends $(v_{1} \otimes v_{2}) \otimes v_{3}$ to $v_{1} \otimes (v_{2} \otimes v_{3})$. It is not \emph{a priori} obvious that this is well-defined, but it is also not difficult to check.

    \item The left unitor is the map which sends $(x, v) \in k \times V$
      to $xv \in V$.

    \item The right unitor is the map which sends 
      \begin{equation*}
        (v, x) \mapsto xv.
      \end{equation*}
  \end{enumerate}
\end{example}

\begin{definition}[monoidal subcategory]
  \label{def:monoidalsubcategory}
  Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category. A \defn{monoidal subcategory} of $\mathsf{C}$ is a subcategory (\hyperref[def:subcategory]{Definition \ref*{def:subcategory}}) $\mathsf{S} \subseteq \mathsf{C}$ which is closed under the tensor product, and which contains $1$.
\end{definition}

\begin{example}
  Recall that $\mathsf{FinVect}_{k}$ is the category of finite dimensional vector spaces over a field $k$. We saw in \hyperref[eg:finvectfullsubcategoryofvect]{Example \ref*{eg:finvectfullsubcategoryofvect}} that $\mathsf{FinVect}_{k}$ was a full subcategory of $\mathsf{Vect}_{k}$.

  We have just seen that $\mathsf{Vect}_{k}$ is a monoidal category with unit object $1 = k$. Since $k$ is a one-dimensional vector space over itself, $k \in \Obj(\mathsf{FinVect}_{k})$, and since the dimension of the tensor product of two finite-dimensional vector spaces is the product of their dimensions, the tensor product is closed in $\mathsf{FinVect}_{k}$. Hence $\mathsf{FinVect}_{k}$ is a monoidal subcategory of $\mathsf{Vect}_{k}$.
\end{example}

\begin{lemma}
  \label{lemma:leftandrightunitoragreewhenpossible}
  Let $\mathsf{C}$ be a category with monoidal structure $(\otimes, 1, \alpha, \lambda, \rho)$. Then the maps $\lambda_{1}$ and $\rho_{1}\colon 1 \otimes 1 \to 1$ agree.
\end{lemma}
\begin{proof}
  See \cite{nlab-deligne-theorem}, Lemma 3.9.
\end{proof}

%\begin{lemma}
%  Let $\mathsf{C}$ be a category with monoidal structure $(\otimes, 1, \alpha, \lambda, \rho)$. Then for all $X$, $Y \in \Obj(\mathsf{C})$, the diagram
%  \begin{equation*}
%    \begin{tikzcd}[row sep=large]
%      (1 \otimes X) \otimes Y 
%      \arrow[d, swap, "{\alpha_{1, X, Y}}"]
%      \arrow[dr, "\lambda_{X} \otimes 1_{Y}"]
%      & 
%      \\
%      1 \otimes (X \otimes Y)
%      \arrow[r, swap, "\lambda_{X \otimes Y}"]
%      & X \otimes Y
%    \end{tikzcd}
%  \end{equation*}
%  commutes.
%\end{lemma}
%\begin{proof}
%
%\end{proof}
%
\begin{note}
  This insight is due to \cite{unapolagetic-mathematician-mac-lanes-theorem}.

  The triangle and pentagon diagram are the first in a long list of hulking coherence diagrams designed to pacify categorical structures into behaving like their algebraic counterparts. For a monoid, multiplication is associative by fiat, and the extension of this to $n$-fold products follows by a trivial application of induction. In monoidal categories, associators are isomorphisms rather than equalities, and we have no a priori guarantee that different ways of composing associators give the same isomorphism. 

  That is exactly what the coherence diagrams do for us, as the below theorem shows.
\end{note}

\begin{theorem}[Mac Lane's coherence theorem]
  Any two ways of freely composing unitors and associators to go from one expression to another coincide. 
\end{theorem}

\begin{definition}[invertible object]
  \label{def:invertibleobject}
  Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category. An \defn{invertible object} (sometimes called a \emph{line object}) is an object $L \in \Obj(\mathsf{C})$ such that both of the functors $\mathsf{C} \rightsquigarrow \mathsf{C}$
  \begin{itemize}
    \item $\ell_{L}\colon A \mapsto L \otimes A$ 
    \item $\sr_{L}\colon A \mapsto A \otimes L$
  \end{itemize}
  are categorical equivalences.
\end{definition}

The following lemma is the categorification of \hyperref[lemma:monoidalmultiplicationbyunitbijective]{Lemma \ref*{lemma:monoidalmultiplicationbyunitbijective}}.
\begin{lemma}
  If $(\mathsf{C}, \otimes, 1)$ is a monoidal category and $L$ is an invertible object, then there is an object $L^{-1} \in \Obj(\mathsf{C})$, unique up to isomorphism, such that $L \otimes L^{-1} \simeq L^{-1} \otimes L \simeq 1$. Furthermore, $L$ is invertible only if there exists such an $L^{-1}$.
\end{lemma}
\begin{proof}
  Suppose $L$ is invertible. Then the functor $\ell_{L}\colon A \mapsto L \otimes A$ is bijective up to isomorphism, i.e. for any $A \in \Obj(\mathsf{C})$, there is an object $A' \in \Obj(\mathsf{C})$, unique up to isomorphism, such that
  \begin{equation*}
    \ell_{L}(A') = L \otimes A' \simeq A.
  \end{equation*}
  But if this is true for \emph{any} $A$, it must also be true for $1$, so there exists an object $L^{-1}$, unique up to isomorphism, such that
  \begin{equation*}
    \ell_{L}(L^{-1}) = L \otimes L^{-1} \simeq 1.
  \end{equation*}

  The same logic tells us that there exists some other element $L'^{-1}$, such that 
  \begin{equation*}
    \sr_{L}(L'^{-1}) = L'^{-1} \otimes L \simeq 1.
  \end{equation*}
  Now 
  \begin{equation*}
    1 \simeq L'^{-1} \otimes L \simeq L'^{-1} \otimes (L \otimes L^{-1}) \otimes L \simeq (L'^{-1} \otimes L) \otimes (L^{-1} \otimes L) \simeq (L'^{-1} \otimes L) \otimes 1 \simeq L'^{-1} \otimes L,
  \end{equation*}
  so $L'^{-1}$ is also a left inverse for $L$. But since $\ell_{L}$ is an equivalence of categories, $L$ only has one left inverse up to isomorphism, so $L^{-1}$ and $L'^{-1}$ must be isomorphic.
\end{proof}

\begin{definition}
  Let $(\mathsf{C}, \otimes, 1)$ be a monoidal category. Then the full subcategory (\hyperref[def:fullsubcategory]{Definition \ref*{def:fullsubcategory}}) $(\Line(\mathsf{C}), \otimes, 1) \subseteq (\mathsf{C}, \otimes, 1)$ whose objects are the line objects in $\mathsf{C}$ is called the \defn{line subcategory} of $(\mathsf{C}, \otimes, 1)$. Since invertibility is closed under the tensor product and $1$ is invertible ($1^{-1} \simeq 1$), $\mathrm{Line}(\mathsf{C})$ is a monoidal subcategory of $\mathsf{C}$.
\end{definition}

The following definition was taken \emph{mutatis mutandis} from \cite{baez-definitions-everyone-should-know}. 
\begin{definition}[monoidal functor]
  \label{def:monoidalfunctor}
  Let $\mathsf{C}$ and $\mathsf{C}'$ be monoidal categories. A functor $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{C}'$ is \defn{lax monoidal} if it is equipped with
  \begin{itemize}
    \item a natural transformation $\Phi_{X, Y}\colon \mathcal{F}(X) \otimes \mathcal{F}(Y) \to \mathcal{F}(X \otimes Y)$, and

    \item a morphism $\varphi\colon 1_{C'} \to \mathcal{F}(1_{C})$ such that

    \item the following diagrams commute for any $X$, $Y$, $Z \in \Obj(\mathsf{C})$.
      \begin{equation*}
        \begin{tikzcd}[row sep=huge, column sep=huge]
          (\mathcal{F}(X) \otimes \mathcal{F}(Y)) \otimes \mathcal{F}(Z)
          \arrow[r, "{\Phi_{X, Y} \otimes 1_{\mathcal{F}(Z)}}"]
          \arrow[d, swap, "\alpha_{\mathcal{F}(X), \mathcal{F}(Y), \mathcal{F}(Z)}"]
          & \mathcal{F}(X \otimes Y) \otimes \mathcal{F}(Z)
          \arrow[r, "{\Phi_{X \otimes Y, Z}}"]
          & \mathcal{F}((X \otimes Y) \otimes Z) 
          \arrow[d, "{\mathcal{F}(\alpha_{X, Y, Z})}"]
          \\
          \mathcal{F}(X) \otimes (\mathcal{F}(Y) \otimes \mathcal{F}(Z)) 
          \arrow[r, swap, "{1_{\mathcal{F}(X)} \otimes \Phi_{Y, Z}}"]
          & \mathcal{F}(X) \otimes \mathcal{F}(Y \otimes Z)
          \arrow[r, swap, "{\varphi_{X, Y \otimes Z}}"]
          & \mathcal{F}(X \otimes (Y \otimes Z))
        \end{tikzcd}
      \end{equation*}
      \begin{equation*}
        \begin{tikzcd}[row sep=huge, column sep=huge]
          1 \otimes \mathcal{F}(X)
          \arrow[r, "\lambda_{\mathcal{F}(X)}"]
          \arrow[d, swap, "\varphi \otimes 1_{\mathcal{F}(X)}"]
          & \mathcal{F}(X) 
          \\
          \mathcal{F}(1) \otimes \mathcal{F}(X)
          \arrow[r, "{\Phi_{1, X}}"]
          & \mathcal{F}(1 \otimes X)
          \arrow[u, swap, "\mathcal{F}(\lambda_{X})"]
        \end{tikzcd}
      \end{equation*}
      \begin{equation*}
        \begin{tikzcd}[row sep=huge, column sep=huge]
          \mathcal{F}(X) \otimes 1
          \arrow[r, "\rho_{\mathcal{F}(X)}"]
          \arrow[d, swap, "1_{\mathcal{F}(X)\otimes \varphi }"]
          & \mathcal{F}(X)
          \\
          \mathcal{F}(X) \otimes \mathcal{F}(1) 
          \arrow[r, "\Phi_{X, 1}"]
          & \mathcal{F}(X \otimes 1)
          \arrow[u, swap, "F(\lambda_{X})"]
        \end{tikzcd}
      \end{equation*}
  \end{itemize}

  If $\Phi$ is a natural isomorphism and $\varphi$ is an isomorphism, then $\mathcal{F}$ is called a \defn{strong monoidal functor}.

  We will denote the above monoidal functor by $(\mathcal{F}, \Phi, \varphi)$.
\end{definition}

\begin{note}
  The above diagrams above are exactly those necessary to ensure that the monoidal structure is preserved. They do this by demanding that the associator and the unitors be $\mathcal{F}$-equivariant.
\end{note} 

\begin{note}
  In much of the literature, a strong monoidal functor is simply called a monoidal functor.
\end{note}

\begin{lemma}
  The composition of lax (strong) monoidal functors is lax (strong) monoidal.
\end{lemma}
\begin{proof}

\end{proof}

\begin{definition}[monoidal natural transformation]
  \label{def:monoidalnaturatransformation}
  Let $(F, \Phi, \varphi)$ and $(G, \Gamma, \gamma)$ be monoidal functors. A natural transformation $\eta: \mathcal{F} \Rightarrow \mathcal{G}$ is \defn{monoidal} if the following diagrams commute.
  \begin{equation*}
    \begin{tikzcd}[column sep=large]
      \mathcal{F}(X) \otimes \mathcal{F}(Y) 
      \arrow[r, "\eta_{X} \otimes \eta_{Y}"]
      \arrow[d, swap, "{\Phi_{X, Y}}"]
      & \mathcal{G}(X) \otimes \mathcal{G}(Y)
      \arrow[d, "{\Gamma_{X, Y}}"]
      \\
      \mathcal{F}(X \otimes Y)
      \arrow[r, "\eta_{X \otimes Y}"]
      & \mathcal{G}(X \otimes Y)
    \end{tikzcd}
    \qquad
    \begin{tikzcd}
      1
      \arrow[d, swap, "\varphi"]
      \arrow[dr, "\gamma"]
      \\
      \mathcal{F}(1)
      \arrow[r, "\eta_{1}"]
      & \mathcal{G}(1)
    \end{tikzcd}
  \end{equation*}
\end{definition}

\subsection{Braided monoidal categories}
Any insightful remarks in this section are due to \cite{baez-this-weeks-finds-137}.

Braided monoidal categories capture the idea that we should think of morphisms between tensor products spatially, as diagrams embedded in 3-space. This sounds odd, but it turns out to be the correct way of looking at a wide class of problems. 

To be slightly more precise, we can think of the objects $X$, $Y$, etc. in any monoidal category as little dots.
\begin{center}
  \begin{tikzpicture}
    \filldraw[black] (0,0) circle (2pt) node[anchor=south]{$X$};
    \filldraw[black] (3,-1) circle (2pt) node[anchor=south]{$Y$};
  \end{tikzpicture}
\end{center}

We can express the tensor product $X \otimes Y$ by putting the dots representing $X$ and $Y$ next to each other.
\begin{center}
  \begin{tikzpicture}
    \filldraw[black] (0,0) circle (2pt) node[anchor=south]{$X$};
    \filldraw[black] (1,0) circle (2pt) node[anchor=south]{$Y$};
  \end{tikzpicture}
\end{center}

A morphism $f$ between, say, $X \otimes Y$ and $A \otimes B \otimes C$ can be drawn as a diagram consisting of some lines and boxes.

\begin{center}
  \begin{tikzpicture}
    \filldraw[black] (-0.5,0) circle (2pt) node[anchor=south]{$X$};
    \filldraw[black] (0.5,0) circle (2pt) node[anchor=south]{$Y$};
    \draw (-0.5, 0) -- (-0.5, -1);
    \draw (0.5, 0) -- (0.5, -1);
    \draw (-1.5, -1) -- (-1.5, -2) -- (1.5, -2) -- (1.5, -1) -- cycle;
    \draw (-1, -2) -- (-1, -3);
    \draw (0, -2) -- (0, -3);
    \draw (1, -2) -- (1, -3);
    \draw (0,-1.5) node{$f$};
    \filldraw[black] (-1,-3) circle (2pt) node[anchor=north]{$A$};
    \filldraw[black] (0,-3) circle (2pt) node[anchor=north]{$B$};
    \filldraw[black] (1,-3) circle (2pt) node[anchor=north]{$C$};
  \end{tikzpicture}
\end{center}

We can compose morphisms by concatenating their diagrams.
\begin{center}
  \begin{tikzpicture}
    \filldraw[black] (-0.5,0) circle (2pt) node[anchor=south]{$X$};
    \filldraw[black] (0.5,0) circle (2pt) node[anchor=south]{$Y$};
    \draw (-0.5, 0) -- (-0.5, -1);
    \draw (0.5, 0) -- (0.5, -1);
    \draw (-1.5, -1) -- (-1.5, -2) -- (1.5, -2) -- (1.5, -1) -- cycle;
    \draw (-1, -2) -- (-1, -3);
    \draw (0, -2) -- (0, -3);
    \draw (1, -2) -- (1, -3);
    \draw (0,-1.5) node{$f$};
    \draw (-1.5, -4) -- (-1.5, -3) -- (1.5, -3) -- (1.5, -4) -- cycle;
    \draw (0,-3.5) node{$g$};
    \draw (0, -4) -- (0, -5);
    \filldraw[black] (0, -5) circle (2pt) node[anchor=north]{$Z$};
  \end{tikzpicture}
\end{center}

In a braided monoidal category, we require that for any two objects $X$ and $Y$ we have an isomorphism $\gamma_{XY}\colon X \otimes Y \to Y \otimes X$, which we draw like this.

\begin{center}
  \begin{tikzpicture}
    \braid (braid) s_1;
    \node[at=(braid-1-s), anchor=south]{$X$};
    \node[at=(braid-2-s), anchor=south]{$Y$};
    \node[at=(braid-1-e), anchor=north]{$X$};
    \node[at=(braid-2-e), anchor=north]{$Y$};
  \end{tikzpicture}
\end{center}

Since $\gamma_{AB}$ is an isomorphism, it has an inverse $\gamma_{AB}^{-1}$ (not necessarily equal to $\gamma_{BA}$!) which we draw like this.

\begin{center}
  \begin{tikzpicture}
    \braid (braid) s_{1}^{-1};
    \node[at=(braid-1-s), anchor=south]{$X$};
    \node[at=(braid-2-s), anchor=south]{$Y$};
    \node[at=(braid-1-e), anchor=north]{$X$};
    \node[at=(braid-2-e), anchor=north]{$Y$};
  \end{tikzpicture}
\end{center}

The idea of a braided monoidal category is that we want to take these pictures seriously: we want two expressions involving repeated applications of the $\gamma_{\cdot\cdot}$ and their inverses to be equivalent if and only if the braid diagrams representing them are homotopic. Thus we want, for example,
\begin{equation*}
  \gamma_{XY} \circ \gamma_{YZ} \circ \gamma_{XY} =  \gamma_{YZ} \circ \gamma_{XY} \circ \gamma_{YZ}
\end{equation*}
since
\begin{equation*}
  \begin{aligned}
    \begin{tikzpicture}
      \braid (braid) s_1 s_{2} s_{1};
      \node[at=(braid-1-s), anchor=south]{$X$};
      \node[at=(braid-2-s), anchor=south]{$Y$};
      \node[at=(braid-3-s), anchor=south]{$Z$};
      \node[at=(braid-1-e), anchor=north]{$X$};
      \node[at=(braid-2-e), anchor=north]{$Y$};
      \node[at=(braid-3-e), anchor=north]{$Z$};
    \end{tikzpicture}
  \end{aligned}
  \qquad\text{is homotopic to}\qquad
  \begin{aligned}
    \begin{tikzpicture}
      \braid (braid) s_2 s_{1} s_{2};
      \node[at=(braid-1-s), anchor=south]{$X$};
      \node[at=(braid-2-s), anchor=south]{$Y$};
      \node[at=(braid-3-s), anchor=south]{$Z$};
      \node[at=(braid-1-e), anchor=north]{$X$};
      \node[at=(braid-2-e), anchor=north]{$Y$};
      \node[at=(braid-3-e), anchor=north]{$Z$};
    \end{tikzpicture}
  \end{aligned}
  .
\end{equation*}

A digression into the theory of braid groups would take us too far afield. The punchline is that to guarantee that all such compositions involving the $\gamma$ are identified in the correct way, we must define braided monoidal categories as follows.

\begin{definition}[braided monoidal category]
  \label{def:braidedmonoidalcategory}
  A catgory $\mathsf{C}$ with monoidal structure $(\otimes, 1, \alpha, \lambda, \rho)$ is \defn{braided} if for every two objects $A$ and $B \in \Obj(\mathsf{C})$, there is an isomorphism $\gamma_{A,B}\colon A \otimes B \to B \otimes A$ such that the following \emph{hexagon diagrams} commute.
  \begin{equation*}
    \begin{tikzcd}
      & A \otimes (B \otimes C) \arrow[r, "{\gamma_{A, B \otimes C}}"] & (B \otimes C) \otimes A \arrow[dr, "\alpha_{BCA}"] & \\
      (A \otimes B) \otimes C \arrow[ur, "\alpha_{ABC}"] \arrow[dr, swap, "\gamma_{AB} \otimes 1"] & & & B \otimes (C \otimes A) \\
      & (B \otimes A) \otimes C \arrow[r, swap, "\alpha_{BAC}"] & B \otimes (A \otimes C) \arrow[ur, swap, "1 \otimes \gamma"]
    \end{tikzcd}
  \end{equation*}
  \begin{equation*}
    \begin{tikzcd}
      & (A \otimes B) \otimes C \arrow[r, "{\gamma_{A \otimes B, C}}"] & C \otimes (A \otimes B) \arrow[dr, "\alpha^{-1}_{CAB}"] \\
      A \otimes (B \otimes C) \arrow[ur, "\alpha_{ABC}^{-1}"] \arrow[dr, swap, "1 \otimes \gamma_{BC}"] & & & (C \otimes A) \otimes B \\
      & A \otimes (C \otimes B) \arrow[r, swap, "\alpha^{-1}_{ACB}"] & (A \otimes C) \otimes B \arrow[ur, swap, "\gamma_{AC} \otimes 1"]
    \end{tikzcd} 
  \end{equation*}
  The collection of such $\gamma$ form a natural isomorphism betweem the bifunctors 
  \begin{equation*}
    (A, B) \mapsto A \otimes B\qquad\text{and}\qquad (A,B) \mapsto B \otimes A,
  \end{equation*}
  and is called a \defn{braiding}.
\end{definition}

\begin{definition}[braided monoidal functor]
  \label{def:braidedmonoidalfunctor}
  A lax monoidal functor $(\mathcal{F}, \Phi, \phi)$ (\hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}}) is \defn{braided monoidal} if it makes the following diagram commute.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      \mathcal{F}(x) \otimes \mathcal{F}(y)
      \arrow[r, "{\gamma_{\mathcal{F}(x), \mathcal{F}(y)}}"]
      \arrow[d, swap, "{\Phi_{x,  y}}"]
      & \mathcal{F}(y) \otimes \mathcal{F}(x)
      \arrow[d, "{\Phi_{x, y}}"]
      \\
      \mathcal{F}(x \otimes y)
      \arrow[r, "{\mathcal{F}(\gamma_{x, y})}"]
      & \mathcal{F}(y \otimes x)
    \end{tikzcd}
  \end{equation*}
  \begin{note}
    There are no extra conditions imposed on a monoidal natural transformation to turn it into a braided natural transformation. 
  \end{note}
\end{definition}

\subsection{Symmetric monoidal categories}
Until now, we have been calling the bifunctor $\otimes$ in \hyperref[def:monoidalcategory]{Definition \ref*{def:monoidalcategory}} a tensor product. This has been an abuse of terminology: in general, one defines tensor products not to be those bifunctors which come from any monoidal category, but only those which come from \emph{symmetric} monoidal categories. We will define these shortly.

Conceptually, passing from the definition of a braided monoidal category to that of a symmetric monoidal category is rather simple. One only requires that for any two objects $A$ and $B$, $\gamma_{BA} = \gamma_{AB}^{-1}$, i.e. $\gamma_{BA} \circ \gamma_{AB} = 1_{A \otimes B}$.

We can interpret this nicely in terms of our braid diagrams. We can draw $\gamma_{BA} \circ \gamma_{AB}$ like this.

\begin{center}
  \begin{tikzpicture}
    \braid (braid) s_1 s_1;
    \node[at=(braid-1-s), anchor=south]{$A$};
    \node[at=(braid-2-s), anchor=south]{$B$};
    \node[at=(braid-1-e), anchor=north]{$A$};
    \node[at=(braid-2-e), anchor=north]{$B$};
  \end{tikzpicture}
\end{center}

The requirement that this must be homotopic to the identity transformation
\begin{center}
  \begin{tikzpicture}
    \draw (0,0) node[anchor=south]{$A$} -- (0,-1) node[anchor=north]{$A$};
    \draw (1,0) node[anchor=south]{$B$} -- (1,-1) node[anchor=north]{$B$};
  \end{tikzpicture}
\end{center}

can be expressed by making the following rule: in a \emph{symmetric} monoidal category, we don't care about the difference between undercrossings and overcrossings:
\begin{equation*}
  \begin{aligned}
    \begin{tikzpicture}
      \braid (braid) s_1;
      \node[at=(braid-1-s), anchor=south]{$A$};
      \node[at=(braid-2-s), anchor=south]{$B$};
      \node[at=(braid-1-e), anchor=north]{$A$};
      \node[at=(braid-2-e), anchor=north]{$B$};
    \end{tikzpicture}
  \end{aligned}
  \qquad = \qquad
  \begin{aligned}
    \begin{tikzpicture}
      \braid (braid) s_1^{-1};
      \node[at=(braid-1-s), anchor=south]{$A$};
      \node[at=(braid-2-s), anchor=south]{$B$};
      \node[at=(braid-1-e), anchor=north]{$A$};
      \node[at=(braid-2-e), anchor=north]{$B$};
    \end{tikzpicture}
  \end{aligned}
  .
\end{equation*}

Then we can exchange the diagram representing $\gamma_{BA} \circ \gamma_{AB}$ for

\begin{center}
  \begin{tikzpicture}
    \braid (braid) s_1 s_1^{-1};
    \node[at=(braid-1-s), anchor=south]{$A$};
    \node[at=(braid-2-s), anchor=south]{$B$};
    \node[at=(braid-1-e), anchor=north]{$A$};
    \node[at=(braid-2-e), anchor=north]{$B$};
  \end{tikzpicture}
\end{center}
which is clearly homotopic to the identity transformation on $A \otimes B$.

\begin{definition}[symmetric monoidal category]
  \label{def:symmetricmonoidalcategory}
  Let $\mathsf{C}$ be a braided monoidal category with braiding $\gamma$. We say that $\mathsf{C}$ is a \defn{symmetric monoidal category} if for all $A$, $B \in \Obj(\mathsf{C})$, $\gamma_{BA} \circ \gamma_{AB} = 1_{A \otimes B}$. A braiding $\gamma$ which satisfies such a condition is called \defn{symmetric}.
\end{definition}

\begin{note}
  There are no extra conditions imposed on a monoidal natural transformation to turn it into a symmetric natural transformation. 
\end{note}
\section{Internal hom functors} \label{sec:internalhomfunctors}
We can now generalize the notion of an exponential object (\hyperref[def:exponential]{Definiton \ref*{def:exponential}}) to any monoidal category.
\subsection{The internal hom functor}
Recall the definition of the hom functor on a locally small category $\mathsf{C}$ (\hyperref[def:homfunctor]{Definition \ref*{def:homfunctor}}): it is the functor which maps two objects to the set of morphisms between them, so it is a functor
\begin{equation*}
  \mathsf{C}^{\mathrm{op}} \times \mathsf{C} \rightsquigarrow \mathsf{Set}.
\end{equation*}

If we take $\mathsf{C} = \mathsf{Set}$, then our hom functor never really leaves $\mathsf{Set}$; it is \emph{internal} to $\mathsf{Set}$. This is our first example of an \emph{internal hom functor}. In fact, it is the prototypical internal hom functor, and we can learn a lot by studying its properties. 

Let $X$ and $Y$ be sets. Denote the set of all functions $X \to Y$ by $[X,Y]$. 

Let $S$ be any other set, and consider a function $f\colon S \to [X, Y]$. For each element $s \in S$, $f$ picks out a function $h_{s}\colon X \to Y$. But this is just a curried version of a function $S \times X \to Y$! So as we saw in \hyperref[section:homfunctor]{Section \ref*{section:homfunctor}}, we have a bijection between the sets $[S, [X, Y]]$ and $[S \times X, Y]$. In fact, this is even a \emph{natural} bijection, i.e. a natural transformation between the functors 
\begin{equation*}
  [-,[-,-]]\qquad \text{and}\quad [- \times -, -]\colon \mathsf{Set}^{\mathrm{op}} \times \mathsf{Set}^{\mathrm{op}} \times \mathsf{Set} \rightsquigarrow \mathsf{Set}.
\end{equation*}

Let's check this. First, we need to figure out how our functors act on functions. Suppose we have sets and functions like so.
\begin{equation*}
  \begin{tikzcd}[row sep=tiny]
    A''
    & B''
    \arrow[l, swap, "f''"]
    \\
    A'
    & B'
    \arrow[l, swap, "f'"]
    \\
    A 
    \arrow[r, "f"]
    & B
  \end{tikzcd}
\end{equation*}
Our functor maps
\begin{equation*}
  (A'', A', A) \mapsto [A'', [A', A]] = \Hom_{\mathsf{Set}}(A'', \Hom_{\mathsf{Set}}(A', A)),
\end{equation*}
so it should map $(f'', f', f)$ to a function
\begin{equation*}
  [f'', [f', f]]\colon [A'', [A', A]] \to [B'', [B', B]].
\end{equation*}
The way to do that is by sending $m \in [A'', [A', A]]$ to 
\begin{equation*}
  [f', f] \circ m \circ f''.
\end{equation*}
You can check that this works as advertised.

The other one's not so tough. Our functor maps an object $(A'', A', A)$ to $[A'' \times A', A]$. We need to map $(f'', f', f)$ to a function
\begin{equation*}
  [f'' \times f' , f]\colon [A'' \times A', A] \to [B'' \times B', B].
\end{equation*}
We do that by sending $m \in [A'' \times A', A]$ to
\begin{equation*}
  f \circ m \circ (f'', f') \in [B'' \times B', B].
\end{equation*}

Checking that $[-,[-,-]]$ and $[-\times-, -]$ really \emph{are} functorial would be a bit much; each is just the composition of hom functor and the Cartesian product. We will however check that there is a natural isomorphism between them, which amounts to checking that the following diagram commutes.
\begin{equation*}
  \begin{tikzcd}
    {[A'' \times A', A]}
    \arrow[rrr, "{[f'' \times f', f]}"]
    \arrow[ddd, swap, "{\Phi_{[A'' \times A', A]}}"]
    & & & {[B'' \times B', B]}
    \arrow[ddd, "{\Phi_{[B'' \times B', B]}}"]
    \\
    & m
    \arrow[d, mapsto]
    \arrow[r, mapsto]
    & f \circ m \circ (f', f'')
    \arrow[d, mapsto]
    \\
    & \Phi(m)
    \arrow[r, mapsto]
    & {[f', f] \circ \Phi(m) \circ f'' \stackrel{!}{=} \Phi(f \circ m \circ (f', f''))} 
    \\
    {[A'', [A', A]]}
    \arrow[rrr, swap, "{[f'', [f', f]]}"]
    & & & {[B'', [B', B]]}
  \end{tikzcd}
\end{equation*}
In other words, we have to show that 
\begin{equation*}
  \Phi_{[A'' \times A', A]}(f \circ m \circ (f', f'')) = [f', f] \circ \Phi_{[B'' \times B', B]}(m) \circ f''.
\end{equation*}

So what is each of these? Well, $f \circ m \circ (f', f'')$ is a map $B'' \times B' \to B$, which maps (say) $(b'', b') \mapsto b$. 

The natural transformation $\Phi$ tells us to curry this, i.e. turn it into a map $B'' \to [B', B]$. Not just any map, though: a map which when evaluated on $b''$ turns into a map which, when evaluated on $b'$, yields $b$.

We know that $f \circ m \circ (f', f'')\colon (b'', b') \mapsto b$, i.e.
\begin{equation*}
  f(m(f''(b''), f'(b'))) = b.
\end{equation*}
If we can show that this is \emph{also} what $[f', f] \circ \Phi_{[B'' \times B', B]}(m) \circ f''$ is equal to when evaluated on $b''$ and then $b'$, we are done, since two functions are equal if they take the same value for all inputs.

Well, let's go through what this definition means. First, we take $b''$ and feed it to $f''$. Next, we let $\Phi_{[B'' \times B', B]}(m)$ act on the result, i.e. we fill the first argument of $m$ with $f''(b'')$. What we get is the following:
\begin{equation*}
  m(f''(b''), -).
\end{equation*}
Then we are to precompose this with $f'$ and stick the result into $f$:
\begin{equation*}
  f(m(f''(b''), f'(-))).
\end{equation*}
Finally, we are to evaluate this on $b'$ to get
\begin{equation*}
  f(m(f''(b''), f'(b'))).
\end{equation*}

Indeed, this is equal to $b$, so the diagram commutes.

In other words, $\Phi$ is a natural bijection between the hom-sets $[-,[-,-]]$ and $[- \times -, -]$.

We picked this example because the collection $[A, B]$ of all functions between two sets $A$ and $B$ is itself a set. Therefore it makes sense to think of the hom-sets $\Hom_{\mathsf{C}}(A, B)$ as living within the same category as $A$ and $B$. We saw in \hyperref[sec:cartesianclosedcategories]{Section \ref*{sec:cartesianclosedcategories}} that in a category with products, we could sometimes view hom-sets as exponential objects. However, we now have the technology to be even more general.

\begin{definition}[internal hom functor]
  \label{def:internalhomfunctor}
  Let $(\mathsf{C}, \otimes)$ be a monoidal category. An \defn{internal hom functor} is a functor
  \begin{equation*}
    [-, -]_{\mathsf{C}}\colon \mathsf{C}^{\mathrm{op}} \times \mathsf{C} \rightsquigarrow \mathsf{C}
  \end{equation*}
  such that for every $X \in \Obj(\mathsf{C})$ we have a pair of adjoint functors
  \begin{equation*}
    (-) \otimes X \dashv [X, -]_{\mathsf{C}}.
  \end{equation*}

  The objects $[A, B]_{\mathsf{C}}$ are called \defn{internal hom objects}.
\end{definition}

\begin{note}
  The reason for the long introduction to this section was that the pair of adjoint functors in \hyperref[def:internalhomfunctor]{Definition \ref*{def:internalhomfunctor}} really matches the one in $\mathsf{Set}$. Recall, in $\mathsf{Set}$ there was a natural transformation
  \begin{equation*}
    \Hom_{\mathsf{Set}}(S \times X, Y) \simeq \Hom_{\mathsf{Set}}(S, [X, Y]).
  \end{equation*}

  This means that for any set $X$, there is a pair of adjoint functors 
  \begin{equation*}
    (-) \times X \dashv [X, -],
  \end{equation*}
  which is in agreement with the statement of \hyperref[def:internalhomfunctor]{Definition \ref*{def:internalhomfunctor}}.
\end{note}

\begin{notation}
  The convention at the nLab is to denote the internal hom by square braces $[A,B]$, and this is for the most part what we will do. Unfortunately, we have already used this notation for the \emph{regular} hom functor. To remedy this, we will add a subscript if the category to which the hom functor belongs is not clear: $[-,-]_{\mathsf{C}}$ for a hom functor internal to $\mathsf{C}$, $[-,-]_{\mathsf{Set}}$ for the standard hom functor (or the hom functor internal to $\mathsf{Set}$, which amounts to the same). 

  There is no universally accepted notation for the internal hom functor. One often sees it denoted by a lower-case $\hom$: $\hom_{\mathsf{C}}(A, B)$. Many sources (for example DMOS \cite{DMOS}) distinguish the internal hom with an underline: $\underline{\Hom}_{\mathsf{C}}(A, B)$. Deligne typesets it with a script H: $\mathscr{H}om_{\mathsf{C}}(A, B)$.
\end{notation}

\begin{definition}[closed monoidal category]
  \label{def:closedmonoidalcategory}
  A monoidal category equipped with an internal hom functor is called a \defn{closed monoidal category}.
\end{definition}

\begin{note}
  Here is another (clearly equivalent) definition of $[X, Y]_{\mathsf{C}}$: it is the object representing (\hyperref[def:representablefunctor]{Definition \ref*{def:representablefunctor}}) the functor
  \begin{equation*}
    T \mapsto \Hom_\mathsf{C}(T \otimes X, Y).
  \end{equation*}
\end{note}

\begin{example}
  In many locally small categories whose objects can be thought of as ``sets with extra structure,'' it is possible to pile structure on top of the hom sets until they themselves can be viewed as bona fide objects in their categories. It often (\emph{but not always!}) happens that these beefed-up hom sets coincide (up to isomorphism) with the internal hom objects.

  Take for example $\mathsf{Vect}_{k}$. For any vector spaces $V$ and $W$, we can turn $\Hom_{\mathsf{Vect}_{k}}(V, W)$ into a vector space by defining addition and scalar multiplication pointwise; we can then view $\Hom_{\mathsf{Vect}_{k}}(V, W)$ as belonging to $\Obj(\mathsf{Vect}_{k})$. It turns out that this is precisely (up to isomorphism) the internal hom object $[V, W]_{\mathsf{Vect}_{k}}$.

  To see this, we need to show that there is a natural bijection 
  \begin{equation*}
    \Hom_{\mathsf{Vect}_{k}}(A, \Hom_{\mathsf{Vect}_{k}}(B, C)) \simeq \Hom_{\mathsf{Vect}_{k}}(A \otimes B, C).
  \end{equation*}

  Suppose we are given a linear map $f\colon A \to \Hom_{\mathsf{Vect}_{k}}(B, C)$. If we act with this on an element of $A$, we get a linear map $B \to C$. If we evaluate this on an element of $B$, we get an element of $C$. Thus, we can view $f$ as a bilinear map $A \times B \to C$, hence as a linear map $A \otimes B \to C$.

  Now suppose we are given a linear map $g\colon A \otimes B \to C$. By pre-composing this with the tensor product we can view this as a bilinear map $A \times B \to C$, and by currying this we get a linear map $A \to \Hom_{\mathsf{Vect}_{k}}(B, C)$.
\end{example}

For the remainder of this chapter, let $(\mathsf{C}, \otimes, 1)$ be a closed monoidal category with internal hom functor $[-,-]_{\mathsf{C}}$.

In a closed monoidal category, the adjunction between the internal hom and the tensor product even holds internally.
\begin{lemma}
  For any $X$, $Y$, $Z \in \Obj(\mathsf{C})$ there is a natural isomorphism
  \begin{equation*}
    [X \otimes Y, Z]_{\mathsf{C}} \stackrel{\sim}{\to} [X, [Y, Z]_{\mathsf{C}}]_{\mathsf{C}}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Let $A \in \Obj(\mathsf{C})$. We have the following string of natural isomorphisms.
  \begin{align*}
    \Hom_{\mathsf{C}}(A, [X \otimes Y, Z]_{\mathsf{C}}) &\simeq \Hom_{\mathsf{C}}(A \otimes (X \otimes Y), Z) \\
    &\simeq \Hom_{\mathsf{C}}((A \otimes X) \otimes Y, Z) \\
    &\simeq \Hom_{\mathsf{C}}(A \otimes X, [Y, Z]_{\mathsf{C}}) \\
    &\simeq \Hom_{\mathsf{C}}(A, [X, [Y, Z]_{\mathsf{C}}]_{\mathsf{C}}).
  \end{align*}

  Since this is true for each $A$ we have, by \hyperref[cor:yonedaembeddingrespectsisomorphisms]{Corollary \ref*{cor:yonedaembeddingrespectsisomorphisms}},
  \begin{equation*}
    [X \otimes Y, Z]_{\mathsf{C}} \stackrel{\sim}{\to} [X, [Y, Z]_{\mathsf{C}}]_{\mathsf{C}}.
  \end{equation*}
\end{proof}

\begin{lemma}
  \label{lemma:cantensorbothsidesofinternalhom}
  Let $(\mathsf{C}, \otimes, 1)$ be a closed symmetric monoidal category. For any $A$, $B$, $R \in \Obj(\mathsf{C})$, there is a transformation
  \begin{equation*}
    [A, B]_{\mathsf{C}} \to [R \otimes A, R \otimes B]_{\mathsf{C}}
  \end{equation*}
  which is natural in $A$ and $B$.
\end{lemma}
\begin{proof}
  The assignment $R \otimes (-)$ is a functor, hence induces a transformation of the regular hom functor
  \begin{equation*}
    \Hom_{\mathsf{C}}(A, B) \mapsto \Hom_{\mathsf{C}}(R \otimes A, R \otimes B)
  \end{equation*}
  which is natural in $A$ and $B$. We would like to show that the internal hom functor also has this property.

  The following string of natural transformations guarantees it by the Yoneda lemma.
  \begin{align*}
    \Hom_{\mathsf{C}}(X, [A, B]_{\mathsf{C}}) &\simeq \Hom_{\mathsf{C}}(X \otimes A, \otimes B) \\
    &\simeq \Hom_{\mathsf{C}}(R \otimes X \otimes A, R \otimes B) \\
    &\simeq \Hom_{\mathsf{C}}(X \otimes (R \otimes A), R \otimes B) \\
    & \simeq \Hom_{\mathsf{C}}(X, [R \otimes A, R \otimes B]_{\mathsf{C}}).
  \end{align*}
\end{proof}

\subsection{The evaluation map}
The internal hom functor gives us a way to talk about evaluating morphisms $f\colon X \to Y$ without mentioning elements of $X$.
\begin{definition}[evaluation map]
  \label{def:evaluationmap}
  Let $X \in \Obj(\mathsf{C})$. We have seen that the adjunction
  \begin{equation*}
    (-) \otimes X \dashv [X, -]_{\mathsf{C}}
  \end{equation*}
  gives us, for any $A$, $X$, $Y \in \Obj(\mathsf{C})$, a natural bijection
  \begin{equation*}
    \Hom_{\mathsf{C}}(A \otimes X, Y) \overset{\sim}{\to} \Hom_{\mathsf{C}}(A, [X, Y]_{\mathsf{C}}).
  \end{equation*}
  In particular, with $A = [X, Y]_{\mathsf{C}}$, we have a bijection
  \begin{equation*}
    \Hom_{\mathsf{C}}([X, Y]_{\mathsf{C}} \otimes X, Y) \overset{\sim}{\to} \Hom_{\mathsf{C}}([X, Y]_{\mathsf{C}}, [X, Y]_{\mathsf{C}}).
  \end{equation*}

  The adjunct (\hyperref[def:adjunct]{Definition \ref*{def:adjunct}}) of $1_{[X, Y]_{\mathsf{C}}} \in \Hom_{\mathsf{C}}([X, Y]_{\mathsf{C}}, [X, Y]_{\mathsf{C}})$ is an object in $\Hom_{\mathsf{C}}\left( [X, Y]_{\mathsf{C}} \otimes X, Y \right)$, denoted 
  \begin{equation*}
    \ev_{X, Y}\colon [X, Y]_{\mathsf{C}} \otimes X \to Y,
  \end{equation*}
  and called the \defn{evaluation map}.
\end{definition}

\begin{example}
  \label{eg:evaluationmapinset}
  As we saw in \hyperref[eg:setisamonoidalcategory]{Example \ref*{eg:setisamonoidalcategory}}, the category $\mathsf{Set}$ is a monoidal category with a bifunctor given by the cartesian product. The internal hom is simply the regular hom functor 
  \begin{equation*}
    \Hom_{\mathsf{Set}}(-,-) = [-,-].
  \end{equation*}
  Let us explore the evaluation map on $\mathsf{Set}$. It is the adjunct of the identity map $1_{[X, Y]}$ under the adjunction
  \begin{equation*}
    [[X, Y] \times X, Y] \dashv [[X, Y], [X, Y]].
  \end{equation*}
  Thus, it is a function 
  \begin{equation*}
    \ev_{X, Y}\colon [X, Y] \times X \to Y;\qquad (f, x) \mapsto \ev_{X, Y}(f, x).
  \end{equation*}
  So far, we don't know what $\ev_{X, Y}$ sends $(f, x)$ to; we just know that we'd \emph{like it} if it sent it to $f(x)$.

  The above adjunction is given by currying: we start on the LHS with a map $\ev_{X, Y}$ with two arguments, and we turn it into a map which fills in only the first argument. Thus the map on the RHS adjunct to $\ev_{X, Y}$ is given by 
  \begin{equation*}
    f \mapsto \ev_{X, Y}(f, -).
  \end{equation*}
  If we want the map $f \mapsto \ev_{X, Y}(f, -)$ to be the identity map, $f$ and $\ev_{X, Y}(f, -)$ must agree on all elements $x$, i.e.
  \begin{equation*}
    f(x) = \ev_{X, Y}(f, x)\qquad\text{for all }x \in X.
  \end{equation*}
  Thus, the evaluation map is the map which sends $(f, x) \mapsto f(x)$.
\end{example}

\subsection{The composition morphism}
The evaluation map allows us to define composition of morphisms without talking about internal hom objects as if they have elements. 
\begin{definition}[composition morphism]
  \label{def:compositionmorphism}
  For $X$, $Y$, $Z \in \Obj(\mathsf{C})$, the \defn{composition morphism}
  \begin{equation*}
    \circ_{X, Y, Z}\colon [Y, Z]_{\mathsf{C}} \otimes [X, Y]_{\mathsf{C}} \to [X, Z]_{\mathsf{C}}
  \end{equation*}
  is the $(-) \otimes X \vdash [X, -]_{\mathsf{C}}$-adjunct of the composition
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      {[Y, Z]_{\mathsf{C}}} \otimes {[X, Y]_{\mathsf{C}}} \otimes X 
      \arrow[r, "{\left(1_{[Y, Z]_{\mathsf{C}}}, \ev_{X, Y}\right)}"]
      & {[Y, Z]_{\mathsf{C}}} \otimes Y
      \arrow[r, "{\ev_{Y, Z}}"]
      & Z
    \end{tikzcd}.
  \end{equation*}
\end{definition}

\begin{example}
  In $\mathsf{Set}$, the composition morphism $\circ_{X, Y, Z}$ lives up to its name. Let $f\colon X \to Y$, $g\colon Y \to Z$, and $x \in X$. The above composition goes as follows.
  \begin{enumerate}
    \item The map $\left( 1_{[Y, Z]}, \ev_{X, Y} \right)$ turns the triple $(g, f, x)$ into the pair $(g, f(x))$.

    \item The map $\ev_{Y, Z}$ turns $(g, f(x))$ into $g(f(x)) = (g \circ f)(x)$.
  \end{enumerate}

  The evaluation morphism $\circ_{X, Y, Z}$ is the currying of this, i.e. it sends
  \begin{equation*}
    (f, g) \mapsto (f \circ g)(-).
  \end{equation*}
\end{example}

\subsection{Dual objects}
Recall that for any $k$-vector space $V$, there is a dual vector space 
\begin{equation*}
  V^{*} = \left\{ L\colon V \to k \right\}.
\end{equation*}
This definition generalizes to any closed monoidal category.

\begin{definition}[dual object]
  \label{def:dualobject}
  Let $X \in \Obj(\mathsf{C})$. The \defn{dual object} to $X$, denoted $X^{*}$, is defined to be the object
  \begin{equation*}
    [X, 1]_{\mathsf{C}}.
  \end{equation*}

  That is to say, $X^{*}$ is the internal hom object modelling the hom set of morphisms from $X$ to the identity object $1$.
\end{definition}

\begin{notation}
  The evaluation morphism (\hyperref[def:evaluationmap]{Definition \ref*{def:evaluationmap}}) has a component
  \begin{equation*}
    \ev_{X^{*}, X}\colon X^{*} \otimes X \to 1.
  \end{equation*}

  To clean things up a bit, we will write $\ev_{X}$ instead of $\ev_{X^{*}, X}$.
\end{notation}

\begin{notation}
  In many sources, e.g. DMOS (\cite{DMOS}), the dual object to $X$ is denoted $X^{\vee}$ instead of $X^{*}$.
\end{notation}

\begin{lemma}
  There is a natural isomorphism between the functors
  \begin{equation*}
    \Hom_{\mathsf{C}}(-, X^{*})\qquad\text{and}\qquad \Hom_{\mathsf{C}}((-) \otimes X, 1).
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $X$, $T \in \Obj(\mathsf{C})$, the definition of the internal hom $[-,-]_{\mathsf{C}}$ gives us a natural isomorphism
  \begin{equation*}
    \Hom_{\mathsf{C}}(T \otimes X, 1) \simeq \Hom_{\mathsf{C}}(T, [X, 1]_{\mathsf{C}}) = \Hom_{\mathsf{C}}(T, X^{*}).
  \end{equation*}
\end{proof}

\begin{theorem}
  The map $X \mapsto X^{*}$ can be extended to a contravariant functor.
\end{theorem}
\begin{proof}
  We need to figure out how our functor should act on morphisms. We define this by analogy with the familiar setting of vector spaces. Recall that for a linear map $L\colon V \to W$, the dual map $L^{t}\colon W^{*} \to V^{*}$ is defined by
  \begin{equation*}
    (L^{t}(w))(v) = w(L(v)).
  \end{equation*}

  By analogy, for $f \in \Hom_{\mathsf{C}}(X, Y)$, we should define the dual morphism $f^{t} \in \Hom_{\mathsf{C}}(B^{*}, A^{*})$ by demanding that the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      h^{*} \otimes X
      \arrow[r, "f^{t} \otimes 1_{X}"]
      \arrow[d, swap, "1_{Y} \otimes f"]
      & X^{*} \otimes X
      \arrow[d, "\ev_{X}"]
      \\
      h^{*} \otimes Y
      \arrow[r, "\ev_{Y}"]
      & 1
    \end{tikzcd}
  \end{equation*}

  To check that this is functorial, we must check that it respects compositions, i.e. that the following diagram commutes.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      Z^{*} \otimes X
      \arrow[r, "(f^{t} \circ g^{t}) \otimes 1_{X}"]
      \arrow[d, swap, "1_{Z} \otimes (g \circ f)"]
      & X^{*} \otimes X
      \arrow[d, "\ev_{X}"]
      \\
      Z^{*} \otimes Z
      \arrow[r, "\ev_{Z}"]
      & 1
    \end{tikzcd}
  \end{equation*}

  Let's add in some more objects and morphisms.
  \begin{equation*}
    \begin{tikzcd}[row sep=huge, column sep=huge]
      Z^{*} \otimes X
      \arrow[r, "g^{t} \otimes 1_{X}"]
      \arrow[d, swap, "1_{Z^{*}} \otimes f"]
      & h^{*} \otimes X
      \arrow[r, "f^{t} \otimes 1_{X}"]
      \arrow[d, swap, "1_{h^{*}} \otimes f"]
      & X^{*} \otimes X
      \arrow[dd, "\ev_{X}"]
      \\
      Z^{*} \otimes Y
      \arrow[r, "g^{t} \otimes 1_{Y}"]
      \arrow[d, swap, "1_{Z^{*}} \otimes g"]
      & h^{*} \otimes Y
      \arrow[dr, "\ev_{Y}"]
      \\
      Z^{*} \otimes Z
      \arrow[rr, swap, "\ev_{Z}"]
      & & 1
    \end{tikzcd}
  \end{equation*}

  We want to show that the outer square commutes. But it clearly does: that the top left square commutes is trivial, and the right and bottom `squares' are the commutativity conditions defining $f^{t}$ and $g^{t}$.
\end{proof}

\begin{note}
  It's not clear to me why $f^{t}$ as defined above exists and is unique.
\end{note}

The above is one, but not the only, way to define dual objects. We can be more general.
\begin{definition}[right duality]
  \label{def:rightduality}
  Let $\mathsf{C}$ be a category with monoidal structure $(\otimes, 1, \alpha, \lambda, \rho)$. \defn{Right duality} of two objects $A$ and $A^{*} \in \Obj(\mathsf{C})$ consists of
  \begin{enumerate}
    \item A morphism of the form
      \begin{equation*}
        \ev_{A}\colon A^{*} \otimes A \to 1,
      \end{equation*}
      called the \emph{evaluation map} (or \emph{counit} if you're into Hopf algebras)

    \item A morphism of the form
      \begin{equation*}
        i_{A}\colon 1 \to A \otimes A^{*},
      \end{equation*}
      called the \emph{coevaluation} map (or \emph{unit})
  \end{enumerate}
  such that the compositions
  \begin{equation*}
    \begin{tikzcd}[column sep=huge]
      X 
      \arrow[r, "i_{A} \otimes 1_{X}"]
      & (X \otimes X^{*}) \otimes X
      \arrow[r, "{\alpha_{X, X^{*}, X}}"]
      & X \otimes (X^{*} \otimes X) 
      \arrow[r, "{1_{X} \otimes \ev_{X}}"]
      & X
      \\
      X^{*}
      \arrow[r, "{1_{X^{*}} \otimes \ev_{X}}"]
      & X^{*} \otimes (X \otimes X^{*})
      \arrow[r, "{\alpha^{-1}_{X^{*}, X, X^{*}}}"]
      & (X^{*} \otimes X) \otimes X^{*}
      \arrow[r, "{\ev_{X}\otimes 1_{X^{*}}}"]
      & X^{*}
    \end{tikzcd}
  \end{equation*}
  are the identity morphism.
\end{definition}

\begin{definition}[rigid monoidal category]
  \label{def:rigidmonoidalcategory}
  A monoidal category $(\mathsf{C}, \otimes, 1)$ is \defn{rigid} if every object has a left and right dual.
\end{definition}

\begin{theorem}
  Every rigid monoidal category is a closed monoidal category (i.e. has an internal hom functor, see \hyperref[def:closedmonoidalcategory]{Definition \ref*{def:closedmonoidalcategory}}) with internal hom object 
  \begin{equation*}
    [A, B]_{\mathsf{C}} \simeq B \otimes A^{*}.
  \end{equation*}
\end{theorem}
\begin{proof}
  We can prove the existence of this isomorphism by showing, thanks to \hyperref[cor:yonedaembeddingrespectsisomorphisms]{Corollary \ref*{cor:yonedaembeddingrespectsisomorphisms}}, that for any $X \in \Obj(\mathsf{C})$ there is an isomorphsm
  \begin{equation*}
    \Hom_{\mathsf{C}}(X, [A, B]_{\mathsf{C}}) \simeq \Hom_{\mathsf{C}}(X, B \otimes A^{*}).
  \end{equation*}

  The defining adjunction of the internal hom gives us 
  \begin{equation*}
    \Hom_{\mathsf{C}}(X, [A, B]_{\mathsf{C}}) \simeq \Hom_{\mathsf{C}}(X \otimes A, B).
  \end{equation*}

  Now we can map any $f \in \Hom_{\mathsf{C}}(X \otimes A, B)$ to
  \begin{equation*}
    (f \otimes 1_{A}) \circ (1_{X} \otimes i_{A}) \in \Hom_{\mathsf{C}}(X, B \otimes A^{*}).
  \end{equation*}

  We will be done if we can show that the assignment
  \begin{equation*}
    f \mapsto (f \otimes 1_{A}) \circ (1_{X} \otimes i_{A})
  \end{equation*}
  is an isomorphism. We'll do this by exhibiting an inverse:
  \begin{equation*}
    \Hom_{\mathsf{C}}(X, B \otimes A^{*}) \ni g \mapsto (1_{W} \otimes \ev_{V}) \circ (g \otimes 1_{V}) \in \Hom_{\mathsf{C}}(X \otimes A, B).
  \end{equation*}

  Of course, first we should show that $(f \otimes 1_{A}) \circ (1_{X} \otimes i_{A})$ really does map $X \to B \otimes A^{*}$. But it does; it does this by first acting on $X$ with $i_{A}$:
  \begin{equation*}
    X \to X \otimes A \otimes A^{*}
  \end{equation*}
  and then acting on the $X \otimes A$ with $f$ and letting the $A^{*}$ hang around:
  \begin{equation*}
    X \otimes A \otimes A^{*} \to B \otimes A^{*}.
  \end{equation*}

  To show that 
  \begin{equation*}
    g \mapsto (1_{B} \otimes \ev_{A}) \circ (g \otimes 1_{A})
  \end{equation*}
  really is an inverse, we can shove the assignment
  \begin{equation*}
    f \mapsto (f \otimes 1_{A}) \circ (1_{X} \otimes i_{A})
  \end{equation*}
  into it and show that we get $f$ right back out. That is to say, we need to show that
  \begin{equation*}
    (1_{B} \otimes \ev_{A}) \circ (\left[ (f \otimes 1_{A}) \circ (1_{X} \otimes i_{A}) \right] \otimes 1_{A}) = f.
  \end{equation*}
  This is easy to see but hard to type. Write it out. You'll need to use first of the two composition identities. 

  To show that the other composition yields $g$, you have to use the other.
\end{proof} 

\section{Abelian categories} \label{sec:abeliancategories}
This section draws heavily from \cite{EGNO-tensor-categories}.
\subsection{Additive categories}
Recall that $\mathsf{Ab}$ is the category of abelian groups (\hyperref[def:abeliangroup]{Definition \ref*{def:abeliangroup}}).
\begin{definition}[$\mathsf{Ab}$-enriched category]
  \label{def:abenrichedcategory}
  A category $\mathsf{C}$ is \defn{$\mathsf{Ab}$-enriched} if 
  \begin{enumerate}
    \item for all objects $A$, $B \in \Obj(\mathsf{C})$, the hom-set $\text{Hom}_{\mathsf{C}}(A, B)$ has the structure of an abelian group (i.e. one can add morphisms), such that

    \item the composition
      \begin{equation*}
        \circ\colon \Hom_{\mathsf{C}}(B, C) \times \Hom_{\mathsf{C}}(A, B) \to \Hom_{C}(A, C)
      \end{equation*}
      is additive in each slot: for any $f_{1}$, $f_{2} \in \Hom_{\mathsf{C}}(B, C)$ and $g \in \Hom_{\mathsf{C}}(A, B)$, we must have
      \begin{equation*}
        (f_{1} + f_{2}) \circ g = f_{1} \circ g + f_{2} \circ g,
      \end{equation*}
      and similarly in the second slot.
  \end{enumerate}
\end{definition}

\begin{note}
  \label{note:inabenrichedcategorynoemptyhomsets}
  In any $\mathsf{Ab}$-enriched category, every hom-set has at least one element---the identity element of the hom-set taken as an abelian group.
\end{note}

\begin{definition}[endomorphism ring]
  \label{def:endomorphismring}
  Let $\mathsf{C}$ be an $\mathsf{Ab}$-enriched category, and let $A \in \Obj(\mathsf{C})$. The \defn{endomorphism ring} of $A$, denoted $\mathrm{End}(A)$, is $\Hom_{\mathsf{C}}(A, A)$, with addition given by the abelian structure and multiplication given by composition.
\end{definition}

\begin{lemma}
  \label{lemma:abeliancategorycoproductsareproducts}
  In an $\mathsf{Ab}$-enriched category $\mathsf{C}$, a finite product is also a coproduct, and vice versa. In particular, initial objects and terminal objects coincide.
\end{lemma}
\begin{proof}
  %First, we show that initial and terminal objects coincide. Let $\emptyset$ be initial and $*$ terminal in $\mathsf{C}$. 
  %
  %Since $\Hom_{\mathsf{C}}(\emptyset, \emptyset)$ has only one element, it must be both the identity morphism on $\emptyset$ \emph{and} the identity element of $\Hom_{\mathsf{C}}(\emptyset, \emptyset)$ as an abelian group. Similarly, $1_{*}$ must be the identity element of $\Hom_{\mathsf{C}}(*, *)$.

  %Since $\emptyset$ is initial and $*$ is final, $\Hom_{\mathsf{C}}(\emptyset, *)$ has exactly one element $f$. By \hyperref[note:inabenrichedcategorynoemptyhomsets]{Note \ref*{note:inabenrichedcategorynoemptyhomsets}}, we are guaranteed that $\Hom_{\mathsf{C}}(*, \emptyset)$ has at least one element.

  See \cite{nlab-additive-category}, Proposition 2.1 for details.
\end{proof}

\begin{definition}[additive category]
  \label{def:additivecategory}
  A category $\mathsf{C}$ is \defn{additive} if it has biproducts (\hyperref[def:categorywithbiproducts]{Definition \ref*{def:categorywithbiproducts}}) and is $\mathsf{Ab}$-enriched.
\end{definition}


\begin{example}
  The category $\mathsf{Ab}$ of Abelian groups is an additive category. We have already seen that it has the direct sum $\oplus$ as biproduct. Given any two abelian groups $A$ and $B$ and morphisms $f$, $g\colon A \to B$, we can define the sum $f + g$ via
  \begin{equation*}
    (f + g)(a) = f(a) + g(a)\qquad\text{for all }a \in A.
  \end{equation*}
  Then for another abelian group $C$ and a morphism $h\colon B \to C$, we have
  \begin{equation*}
    \left[ h \circ (f+g) \right](a) = h(f(a) + g(a)) = h(f(a)) + h(g(a)) = \left[ h \circ f + h \circ g \right](a),
  \end{equation*}
  so 
  \begin{equation*}
    h \circ (f+g) =h \circ f + h \circ g,
  \end{equation*}
  and similarly in the other slot.
\end{example}

\begin{example}
  The category $\mathsf{Vect}_{k}$ is additive. Since vector spaces are in particular abelian groups under addition, it is naturally $\mathsf{Ab}$-enriched,
\end{example}

\begin{definition}[additive functor]
  \label{def:additivefunctor}
  Let $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ be a functor between additive categories. We say that $\mathcal{F}$ is \defn{additive} if for each $X$, $Y \in \Obj(\mathsf{C})$ the map
  \begin{equation*}
    \Hom_{\mathsf{C}}(X, Y) \to \Hom_{\mathsf{D}}(\mathcal{F}(X), \mathcal{F}(Y))
  \end{equation*}
  is a homomorphism of abelian groups.
\end{definition}

\begin{lemma}
  For any additive functor $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$, there exists a natural isomorphism 
  \begin{equation*}
    \Phi\colon \mathcal{F}(-) \oplus \mathcal{F}(-) \Rightarrow \mathcal{F}(- \oplus -).
  \end{equation*}
\end{lemma}
\begin{proof}
  The commutativity of the following diagram is immediate.
  \begin{equation*}
    \begin{tikzcd}[column sep=huge, row sep=huge]
      \mathcal{F}(X \oplus Y)
      \arrow[r, "\mathcal{F}(f \oplus g)"]
      \arrow[d, swap, "\Phi_{X, Y}"]
      & \mathcal{F}(X' \oplus Y')
      \arrow[d, "\Phi_{X', Y'}"]
      \\
      \mathcal{F}(X) \oplus \mathcal{F}(Y)
      \arrow[r, "\mathcal{F}(f) \oplus \mathcal{F}(g)"]
      & \mathcal{F}(X') \oplus \mathcal{F}(Y')
    \end{tikzcd}
  \end{equation*}

  The $(X, Y)$-component $\Phi_{X, Y}$ is an isomorphism because
\end{proof}

\subsection{Pre-abelian categories} \label{sec:preabeliancategories}
\begin{definition}[pre-abelian category]
  \label{def:preabeliancategory}
  A category  $\mathsf{C}$ is \defn{pre-abelian} if it is additive and every morphism has a kernel (\hyperref[def:kernelofmorphism]{Definition \ref*{def:kernelofmorphism}}) and a cokernel (\hyperref[def:cokernalofmorphism]{Definition \ref*{def:cokernalofmorphism}}).
\end{definition}

\begin{lemma}
  \label{lemma:preabeliancategorieshaveequalizers}
  Pre-abelian categories have equalizers (\hyperref[def:equalizer]{Definition \ref*{def:equalizer}}).
\end{lemma}
\begin{proof}
  We show that in an pre-abelian category, the equalizer of $f$ and $g$ coincides with the kernel of $f - g$. It suffices to show that the kernel of $f - g$ satisfies the universal property for the equalizer of $f$ and $g$.

  Here is the diagram for the universal property of the kernel of $f - g$.
  \begin{equation*}
    \begin{tikzcd}
      Z
      \arrow[rrd, bend left]
      \arrow[ddr, bend right, swap, "i"]
      \arrow[dr, dashed, "\exists!\bar{i}"]
      \\
      & \ker(f - g)
      \arrow[d, swap, "\iota_{f - g}"]
      \arrow[r]
      & 0
      \arrow[d]
      \\
      & A
      \arrow[r, "f - g"]
      & B
    \end{tikzcd}
  \end{equation*}

  The universal property tells us that for any object $Z \in \Obj(\mathsf{C})$ and any morphism $i\colon Z \to A$ with $i \circ (f - g) = 0$ (i.e. $i \circ f = i \circ g$), there exists a unique morphism $\bar{i}\colon Z \to \ker(f - g)$ such that $i = \bar{i} \circ \iota_{f - g}$.
\end{proof}

\begin{corollary}
  Every pre-abelian category has all finite limits.
\end{corollary}
\begin{proof}
  By \hyperref[thm:criterionforfinitelimits]{Theorem \ref*{thm:criterionforfinitelimits}}, a category has finite limits if and only if it has finite products and equalizers. Pre-abelian categories have finite products by definition, and equalizers by \hyperref[lemma:preabeliancategorieshaveequalizers]{Lemma \ref*{lemma:preabeliancategorieshaveequalizers}}.
\end{proof}

Recall from \hyperref[sec:biproducts]{Section \ref*{sec:biproducts}} the following definition.

\begin{definition}[zero morphism]
  \label{def:zeromorphism}
  Let $\mathsf{C}$ be a category with zero object $0$. For any two objects $A$, $B \in \Obj(\mathsf{C})$, the \defn{zero morphism} $0_{A,B}$ is the unique morphism $A \to B$ which factors through $0$.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[rr, bend left, "0_{A, B}"]
      \arrow[r]
      & 0
      \arrow[r]
      & B
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{notation}
  It will often be clear what the source and destination of the zero morphism are; in this case we will drop the subscripts, writing $0$ instead of $0_{AB}$.
\end{notation}

It is easy to see that the left- or right-composition of the zero morphism with any other morphism results in the zero morphism: $f \circ 0 = 0$ and $0 \circ g = 0$.

\begin{lemma}
  \label{lemma:everymorphisminapreabeliancategoryfactors}
  Every morphism $f\colon A \to B$ in a pre-abelian catgory has a canonical decomposition
  \begin{equation*}
    \begin{tikzcd}
      A 
      \arrow[r, "p"]
      & \coker(\ker(f))
      \arrow[r, "\bar{f}"]
      & \ker(\coker(f))
      \arrow[r, "i"]
      & B
    \end{tikzcd},
  \end{equation*}
  where $p$ is an epimorphism (\hyperref[def:epimorphism]{Definition \ref*{def:epimorphism}}) and $i$ is a monomorphism (\hyperref[def:monomorphism]{Definition \ref*{def:monomorphism}}).
\end{lemma}
\begin{proof}
  We start with a map
  \begin{equation*}
    \begin{tikzcd}
      A 
      \arrow[r, "f"]
      & B
    \end{tikzcd}.
  \end{equation*}

  Since we are in a pre-abelian category, we are guaranteed that $f$ has a kernel $(\ker(f), \iota)$ and a cokernel $(\coker(f), \pi)$. From the universality squares it is immediate that $f \circ \iota = 0$ and $\pi \circ f = 0$ This tells us that the composition $\pi \circ f \circ \iota = 0$, so the following commutes.
  \begin{equation*}
    \begin{tikzcd}
      & A
      \arrow[r, "f"]
      & B
      \arrow[dr, "\pi"]
      \\
      \ker(f)
      \arrow[ur, "\iota"]
      \arrow[dr]
      & & & \coker(f)
      \\
      & 0
      \arrow[r]
      & 0
      \arrow[ur]
    \end{tikzcd}
  \end{equation*}
  We know that $\pi$ has a kernel $(\ker(\pi), i)$ and $\iota$ has a cokernel $(\coker(\iota), p)$, so we can add their commutativity squares as well.
  \begin{equation*}
    \begin{tikzcd}
      & A
      \arrow[r, "f"]
      \arrow[d, "p"]
      & B
      \arrow[dr, "\pi"]
      \\
      \ker(f)
      \arrow[ur, "\iota"]
      \arrow[dr]
      & \coker(\iota)
      & \ker(\pi)
      \arrow[u, "i"]
      \arrow[d]
      & \coker(f)
      \\
      & 0
      \arrow[u]
      \arrow[r]
      & 0
      \arrow[ur]
    \end{tikzcd}
  \end{equation*}

  If we squint hard enough, we can see the following diagram.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[rrd, bend left]
      \arrow[rdd, bend right, swap, "f"]
      \\
      & \ker(\pi)
      \arrow[r]
      \arrow[d, "i"]
      & 0
      \arrow[d]
      \\
      & B
      \arrow[r, "\pi"]
      & \coker(f)
    \end{tikzcd}
  \end{equation*}
  The outer square commutes because $\pi \circ f = 0$, so the universal property for $\ker(\pi)$ gives us a unique morphism $A \to \ker(\pi)$. Let's add this to our diagram, along with a morphism $0 \to \ker(\pi)$ which trivially keeps everything commutative.
  \begin{equation*}
    \begin{tikzcd}
      & A
      \arrow[r, "f"]
      \arrow[d, "p"]
      \arrow[dr]
      & B
      \arrow[dr, "\pi"]
      \\
      \ker(f)
      \arrow[ur, "\iota"]
      \arrow[dr]
      & \coker(\iota)
      & \ker(\pi)
      \arrow[u, "i"]
      \arrow[d]
      & \coker(f)
      \\
      & 0
      \arrow[u]
      \arrow[r]
      \arrow[ur]
      & 0
      \arrow[ur]
    \end{tikzcd}
  \end{equation*}

  Again, buried in the bowels of our new diagram, we find the following.
  \begin{equation*}
    \begin{tikzcd}
      \ker(f)
      \arrow[r, "\iota"]
      \arrow[d]
      & A
      \arrow[d, "\pi"]
      \arrow[ddr, bend left]
      \\
      0
      \arrow[r]
      \arrow[drr, bend right]
      & \coker(\iota)
      \\
      & & \ker(\pi)
    \end{tikzcd}
  \end{equation*}
  And again, the universal property of cokernels gives us a unique morphism $\bar{f}\colon \coker(\iota) \to \ker(\pi)$.
  \begin{equation*}
    \begin{tikzcd}
      & A
      \arrow[r, "f"]
      \arrow[d, "p"]
      \arrow[dr]
      & B
      \arrow[dr, "\pi"]
      \\
      \ker(f)
      \arrow[ur, "\iota"]
      \arrow[dr]
      & \coker(\iota)
      \arrow[r, "\bar{f}"]
      & \ker(\pi)
      \arrow[u, "i"]
      \arrow[d]
      & \coker(f)
      \\
      & 0
      \arrow[u]
      \arrow[r]
      \arrow[ur]
      & 0
      \arrow[ur]
    \end{tikzcd}
  \end{equation*}

  The fruit of our laborious construction is the following commuting square.
  \begin{equation*}
    \begin{tikzcd}
      A
      \arrow[r, "f"]
      \arrow[d, swap, "p"]
      & B
      \arrow[d, "i"]
      \\
      \coker(\iota)
      \arrow[r, "\bar{f}"]
      & \ker(\pi)
    \end{tikzcd}
  \end{equation*}
  We have seen (\hyperref[lemma:canonicalinjectionismono]{Lemma \ref*{lemma:canonicalinjectionismono}}) that $i$ is mono, and (\hyperref[lemma:canonicalsurjectionisepi]{Lemma \ref*{lemma:canonicalsurjectionisepi}}) that $p$ is epi. 

  Now we abuse terminology by calling $\iota = \ker(f)$ and $\pi = \coker(f)$. Then we have the required decomposition.
\end{proof} 

\begin{note}
  The abuse of notation above is ubiquitous in the literature.
\end{note}

\subsection{Abelian categories}
This section is under very heavy construction. Don't trust anything you read here.
\begin{definition}[abelian category]
  \label{def:abeliancategory}
  A pre-abelian category $\mathsf{C}$ is \defn{abelian} if for each morphism $f$, the canonical morphism guaranteed by \hyperref[lemma:everymorphisminapreabeliancategoryfactors]{Lemma \ref*{lemma:everymorphisminapreabeliancategoryfactors}}
  \begin{equation*}
    \bar{f}\colon \coker(\ker(f)) \to \ker(\coker(f))
  \end{equation*}
  is an isomorphism.
\end{definition}

\begin{note}
  The above piecemeal definition is equivalent to the following.

  A category $\mathsf{C}$ is \emph{abelian} if 
  \begin{enumerate}
    \item it is $\mathsf{Ab}$-enriched \hyperref[def:additivecategory]{Definition \ref*{def:additivecategory}}, i.e. each hom-set has the structure of an abelian group and composition is bilinear;
    \item it admits finite coproducts, hence (by \hyperref[lemma:abeliancategorycoproductsareproducts]{Lemma \ref*{lemma:abeliancategorycoproductsareproducts}}) biproducts and zero objects;
    \item every morphism has a kernel and a cokernel;
    \item for every morphism, $f$, the canonical morphism $\bar{f}\colon \coker(\ker(f)) \to \ker(\coker(f))$ is an isomorphism.
  \end{enumerate}
\end{note}

For the remainder of the section, let $\mathsf{C}$ be an abelian category.

\begin{lemma}
  In an abelian category, every morphism decomposes into the composition of an epimorphism and a monomorphism.
\end{lemma}
\begin{proof}
  For any morphism $f$, bracketing the decomposition $f = i \circ \bar{f} \circ p$ as
  \begin{equation*}
    i \circ (\bar{f} \circ p).
  \end{equation*}
  gives such a composition.
\end{proof}

\begin{note}
  The above decomposition is unique up to unique isomorphism.
\end{note}

\begin{definition}[image of a morphism]
  \label{def:imageofamorphism}
  Let $f\colon A \to B$ be a morphism. The object $\ker(\coker(f))$ is called the \defn{image} of $f$, and is denoted $\im(f)$.
\end{definition}

\begin{lemma}
  \label{lemma:morphismwhichkillsontherightismono}
  $\,$
  \begin{enumerate}
    \item A morphism $f\colon A \to B$ is mono iff for all $Z \in \Obj(\mathsf{C})$ and for all $g\colon Z \to A$, $f \circ g = 0$ implies $g = 0$.
    \item A morphism $f\colon A \to B$ is epi iff for all $Z \in \Obj(\mathsf{C})$ and for all $g\colon B \to Z$, $g \circ f = 0$ implies $g = 0$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  $\,$
  \begin{enumerate}
    \item First, suppose $f$ is mono. Consider the following diagram.
      \begin{equation*}
        \begin{tikzcd}
          Z
          \arrow[r, shift left, "g"]
          \arrow[r, swap, shift right, "0"]
          & A
          \arrow[r, "f"]
          & B
        \end{tikzcd}
      \end{equation*}

      If the above diagram commutes, i.e. if $f \circ g = 0$, then $g = 0$, so $1 \implies 2$.

      Now suppose that for all $Z \in \Obj(\mathsf{C})$ and all $g\colon Z \to A$, $f \circ g = 0$ implies $g = 0$.

      Let $g$, $g'\colon Z \to A$, and suppose that $f \circ g = f \circ g'$. Then $f \circ (g - g') = 0$. But that means that $g - g' = 0$, i.e. $g = g'$. Thus, $2 \implies 1$.

    \item Dual to the proof above.
  \end{enumerate}
\end{proof}

\begin{lemma}
  \label{lemma:monoifkernelzeroepiifcokerzero}
  Let $f\colon A \to B$. We have the following. 
  \begin{enumerate}
    \item The morphism $f$ is mono iff $\ker(f) = 0$ \label{part:monoifkernelzeroepiifcokerzero1}
    \item The morphism $f$ is epi iff $\coker(f) = 0$. \label{part:monoifkernelzeroepiifcokerzero2}
  \end{enumerate}
\end{lemma}
\begin{proof}
  $\,$
  \begin{enumerate}
    \item We first show that if $\ker(f) = 0$, then $f$ is mono. Suppose $\ker(f) = 0$. By the universal property of kernels, we know that for any $Z \in \Obj(\mathsf{C})$ and any $g\colon Z \to A$ with $f \circ g = 0$ there exists a unique map $\bar{g}\colon Z \to \ker(f)$ such that $g = \iota \circ \bar{g}$.
      \begin{equation*}
        \begin{tikzcd}
          Z
          \arrow[rdd, bend right, swap, "g"]
          \arrow[rd, dashed, "\exists!g"]
          \\
          & \ker(f) = 0
          \arrow[r]
          \arrow[d, swap, "\iota"]
          & 0
          \arrow[d]
          \\
          & A
          \arrow[r, "f"]
          & B
        \end{tikzcd}
      \end{equation*}
      But then $g$ factors through the zero object, so we must have $g = 0$. This shows that $f \circ g = 0 \implies g = 0$, and by \hyperref[lemma:morphismwhichkillsontherightismono]{Lemma \ref*{lemma:morphismwhichkillsontherightismono}} $f$ must be mono.

      Next, we show that if $f$ is mono, then $\ker(f) = 0$. To do this, it suffices to show that $\ker(f)$ is final, i.e. that there exists a unique morphism from every object to $\ker(f)$.

      Since $\Hom_{\mathsf{C}}(Z, \ker(f))$ has the structure of an abelian group, it must contain at least one element. Suppose it contains two morphisms $h_{1}$ and $h_{2}$. 
      \begin{equation*}
        \begin{tikzcd}
          Z
          \arrow[rd, shift left, "h_{1}"]
          \arrow[rd, shift right, swap, "h_{2}"]
          \\
          & \ker(f)
          \arrow[r]
          \arrow[d, swap, "\iota"]
          & 0
          \arrow[d]
          \\
          & A
          \arrow[r, "f"]
          & B
        \end{tikzcd}
      \end{equation*}
      Our aim is to show that $h_{1} = h_{2}$. To this end, compose each with $\iota$. 
      \begin{equation*}
        \begin{tikzcd}
          Z
          \arrow[rd, shift left, "h_{1}"]
          \arrow[rd, shift right, swap, "h_{2}"]
          \arrow[rdd, bend right, shift left, "\iota \circ h_{1}"]
          \arrow[rdd, bend right, shift right, swap, "\iota \circ h_{2}"]
          \\
          & \ker(f)
          \arrow[r]
          \arrow[d, swap, "\iota"]
          & 0
          \arrow[d]
          \\
          & A
          \arrow[r, "f"]
          & B
        \end{tikzcd}
      \end{equation*}
      Since $f \circ \iota = 0$, we have $f \circ (\iota \circ h_{1}) = 0$ and $f \circ (\iota \circ h_{2}) = 0$. But since $f$ is mono, by \hyperref[lemma:morphismwhichkillsontherightismono]{Lemma \ref*{lemma:morphismwhichkillsontherightismono}}, we must have $\iota \circ h_{1} = 0 = \iota \circ h_{2}$. But by \hyperref[lemma:canonicalinjectionismono]{Lemma \ref*{lemma:canonicalinjectionismono}}, $\iota$ is mono, so again we have
      \begin{equation*}
        h_{1} = h_{2} = 0, 
      \end{equation*}
      and we are done.

    \item Dual to the proof above.
  \end{enumerate}
\end{proof}

\begin{lemma}
  We have the following.
  \begin{enumerate}
    \item The kernel of the zero morphism $0: A \to B$ is the pair $(A, 1_{A})$.
    \item The cokernel of the zero morphism $0: A \to B$ is the pair $(B, 1_{B})$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  $\,$
  \begin{enumerate}
    \item We need only verify that the universal property is satisfied. That is, for any object $Z \in \Obj(\mathsf{C})$ and any morphism $h\colon Z \to A$ such that $0 \circ g = 0$, there exists a unique morphism $\bar{g}\colon Z \to \ker(0)$ such that the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}
          Z
          \arrow[rdd, swap, bend right, "g"]
          \arrow[rd, dashed, "\bar{g}=g"]
          \\
          & \ker(0) = A
          \arrow[r, "0"]
          \arrow[d, swap, "\iota = 1_{A}"]
          & 0
          \arrow[d]
          \\
          & A
          \arrow[r, "0"]
          & B
        \end{tikzcd}
      \end{equation*}
      But this is pretty trivial: $\bar{g} = g$.

    \item Dual to above.
  \end{enumerate}
\end{proof}

\begin{theorem}
  All abelian categories are binormal (\hyperref[def:binormalcategory]{Definition \ref*{def:binormalcategory}}). That is to say: 
  \begin{enumerate}
    \item all monomorphisms are kernels
    \item all epimorphisms are cokernels.
  \end{enumerate}
\end{theorem}
\begin{proof}
  $\,$
  \begin{enumerate}
    \item Consider the following diagram taken \hyperref[lemma:everymorphisminapreabeliancategoryfactors]{Lemma \ref*{lemma:everymorphisminapreabeliancategoryfactors}}, which shows the canonical factorization of any morphism $f$.
      \begin{equation*}
        \begin{tikzcd}
          & A
          \arrow[r, "f"]
          \arrow[d, "p"]
          \arrow[dr]
          & B
          \arrow[dr, "\pi"]
          \\
          \ker(f)
          \arrow[ur, "\iota"]
          \arrow[dr]
          & \coker(\iota)
          \arrow[r, "\bar{f}"]
          & \ker(\pi)
          \arrow[u, "i"]
          \arrow[d]
          & \coker(f)
          \\
          & 0
          \arrow[u]
          \arrow[r]
          \arrow[ur]
          & 0
          \arrow[ur]
        \end{tikzcd}
      \end{equation*}

      By definition of a pre-abelian category, we know that $\bar{f}$ is an isomorphism.
  \end{enumerate}
\end{proof}

\begin{note}
  The above theorem is actually an equivalent definition of an abelian category, but the proof of equivalence is far from trivial. See e.g. \cite{freyd-abelian-categories} for details.
\end{note}

\begin{definition}[subobject, quotient object, subquotient object]
  \label{def:subobjectquotientobject}
  Let $Y \in \Obj(\mathsf{C})$.
  \begin{enumerate}
    \item A \defn{subobject} of $Y$ is an object $X \in \Obj(\mathsf{C})$ together with a monomorphism $i\colon X \hookrightarrow Y$. If $X$ is a subobject of $Y$ we will write $X \subseteq Y$.
    \item A \defn{quotient object} of $Y$ is an object $Z$ together with an epimorphism $p\colon Y \twoheadrightarrow Z$.
    \item A \defn{subquotient object} of $Y$ is a quotient object of a subobject of $Y$.
  \end{enumerate}
\end{definition}

%\begin{lemma}
%  Let $O$ be an object, $Z$ be a subquotient of $O$, and $Z'$ a subquotient of $Z$. Then $Z'$ is a subquotient of $O$.
%\end{lemma}
%\begin{proof}
%  A subquotient $Z$ of $O$ is a quotient $Z$ of a subobject $X$ of $O$.
%  \begin{equation*}
%    \begin{tikzcd}
%      X 
%      \arrow[r, hookrightarrow]
%      \arrow[d, twoheadrightarrow]
%      & O
%      \\
%      Z
%    \end{tikzcd}
%  \end{equation*}
%  A subquotient $Z'$ of $Z$ looks like this.
%  \begin{equation*}
%    \begin{tikzcd}
%      & X
%      \arrow[r, hookrightarrow]
%      \arrow[d, twoheadrightarrow]
%      & O
%      \\
%      X'
%      \arrow[r, hookrightarrow]
%      \arrow[d, twoheadrightarrow]
%      & Z
%      \\
%      Z'
%    \end{tikzcd}
%  \end{equation*}
%
%  Take pullback yadda yadda. Will finish later.
%\end{proof}

\begin{definition}[quotient]
  \label{def:quotient}
  Let $X \subseteq Y$, i.e. let there exist a monomorphism $f\colon X \hookrightarrow Y$. The \defn{quotient} $Y/X$ is the cokernel $(\coker(f), \pi_{f})$.
\end{definition}

\begin{example}
  Let $V$ be a vector space, $W \subseteq V$ a subspace. Then we have the canonical inclusion map $\iota\colon W \hookrightarrow V$, so $W$ is a subobject of $V$ in the sense of \hyperref[def:subobjectquotientobject]{Definition \ref*{def:subobjectquotientobject}}.

  According to \hyperref[def:quotient]{Definition \ref*{def:quotient}}, the quotient $V / W$ is the cokernel $(\coker(\iota), \pi_{\iota})$ of $\iota$. We saw in \hyperref[eg:invectcokernelsarequotientsbyimage]{Example \ref*{eg:invectcokernelsarequotientsbyimage}} that the cokernel of $\iota$ was $V / \im(\iota)$. However, $\im(\iota)$ is exactly $W$! So the categorical notion of the quotient $V / W$ agrees with the linear algebra notion.
\end{example}

\begin{definition}[$k$-linear category]
  \label{def:linearcategory}
  An abelian category \hyperref[def:abeliancategory]{Definition \ref*{def:abeliancategory}} $\mathsf{C}$ is \defn{$k$-linear} if for all $A$, $B \in \Obj(\mathsf{C})$ the hom-set $\Hom_{\mathsf{C}}(A, B)$ has the structure of a $k$-vector space whose additive structure is the abelian structure, and for which the composition of morphisms is $k$-linear.
\end{definition}

\begin{example}
  The category $\mathsf{Vect}_{k}$ is $k$-linear.
\end{example}

\begin{definition}[$k$-linear functor]
  \label{def:linearfunctor}
  let $\mathsf{C}$ and $\mathsf{D}$ be two $k$-linear categories, and $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ a functor. Suppose that for all objects $C$, $D \in \Obj(\mathsf{C})$ all morphisms $f$, $g\colon C \to D$, and all $\alpha$, $\beta \in k$, we have
  \begin{equation*}
    \mathcal{F}(\alpha f + \beta g) = \alpha \mathcal{F}(f) + \beta \mathcal{G}(g).
  \end{equation*}

  Then we say that $\mathcal{F}$ is \defn{$k$-linear}.
\end{definition}

\subsection{Exact sequences}
\begin{definition}[exact sequence]
  \label{def:exactsequence}
  A sequence of morphisms
  \begin{equation*}
    \begin{tikzcd}
      \cdots
      \arrow[r]
      & X_{i-1}
      \arrow[r, "f_{i-1}"]
      & X_{i}
      \arrow[r, "f_{i}"]
      & X_{i+1}
      \arrow[r]
      & \cdots
    \end{tikzcd}
  \end{equation*}
  is called \defn{exact in degree $i$} if the image (\hyperref[def:imageofamorphism]{Definition \ref*{def:imageofamorphism}}) of $f_{i-1}$ is equal to the kernel (\hyperref[def:kernelofmorphism]{Definition \ref*{def:kernelofmorphism}}) of $f_{i}$. A sequence is \defn{exact} if it is exact in every degree.
\end{definition}

\begin{lemma}
  If a sequence
  \begin{equation*}
    \begin{tikzcd}
      \cdots
      \arrow[r]
      & X_{i-1}
      \arrow[r, "f_{i-1}"]
      & X_{i}
      \arrow[r, "f_{i}"]
      & X_{i+1}
      \arrow[r]
      & \cdots
    \end{tikzcd}
  \end{equation*}
  is exact in degree $i$, then $f_{i} \circ f_{i-1} = 0$.
\end{lemma}

\begin{definition}[short exact sequence]
  \label{def:shortexactsequence}
  A \defn{short exact sequence} is an exact sequence of the following form.
  \begin{equation*}
    \begin{tikzcd}
      0
      \arrow[r]
      & X
      \arrow[r]
      & Y
      \arrow[r]
      & Z
      \arrow[r]
      & 0
    \end{tikzcd}
  \end{equation*}
\end{definition}

%This definition has some immediate trivial consequences.
%\begin{lemma}
%  Let 
%  \begin{equation*}
%    \begin{tikzcd}
%      0
%      \arrow[r]
%      & X
%      \arrow[r, "f"]
%      & Y
%      \arrow[r, "g"]
%      & Z
%      \arrow[r]
%      & 0
%    \end{tikzcd}
%  \end{equation*}
%  be a short exact sequence. Then
%  \begin{enumerate}
%    \item $f$ is mono
%    \item $g$ is epi
%    \item $Z \simeq Y/X$.
%  \end{enumerate}
%\end{lemma}
%\begin{proof}
%  $\,$
%  \begin{enumerate}
%    \item An easy consequence of \hyperref[lemma:monoifkernelzeroepiifcokerzero]{Lemma \ref*{lemma:monoifkernelzeroepiifcokerzero}, part \ref*{part:monoifkernelzeroepiifcokerzero1}}.
%    \item An easy consequence of \hyperref[lemma:monoifkernelzeroepiifcokerzero]{Lemma \ref*{lemma:monoifkernelzeroepiifcokerzero}, part \ref*{part:monoifkernelzeroepiifcokerzero2}}.
%    \item We need to show that
%  \end{enumerate}
%\end{proof}

\begin{definition}[exact functor]
  \label{def:exactfunctor}
  Let $\mathsf{C}$, $\mathsf{D}$ be abelian categories, $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ a functor. We say that $\mathcal{F}$ is
  \begin{itemize}
    \item \defn{left exact} if it preserves biproducts and kernels
    \item \defn{right exact} if it preserves biproducts and cokernels
    \item \defn{exact} if it is both left exact and right exact.
  \end{itemize}
\end{definition}

%\begin{theorem}
%  Let $\mathcal{F}\colon \mathsf{C} \rightsquigarrow \mathsf{D}$ be a functor between abelian categories. Let
%  \begin{equation*}
%    \begin{tikzcd}
%      0
%      \arrow[r]
%      & A
%      \arrow[r]
%      & B
%      \arrow[r]
%      & C
%      \arrow[r]
%      & 0
%    \end{tikzcd}
%  \end{equation*}
%  be a short exact sequence in $\mathsf{C}$. Then 
%  \begin{itemize}
%    \item if $\mathcal{F}$ is left exact, then
%      \begin{equation*}
%        \begin{tikzcd}
%          0
%          \arrow[r]
%          & \mathcal{F}(A)
%          \arrow[r]
%          & \mathcal{F}(B)
%          \arrow[r]
%          & \mathcal{F}(C)
%        \end{tikzcd}
%      \end{equation*}
%      is an exact sequence in $\mathsf{D}$.
%
%    \item if $\mathcal{F}$ is right exact, then
%      \begin{equation*}
%        \begin{tikzcd}
%          \mathcal{F}(A)
%          \arrow[r]
%          & \mathcal{F}(B)
%          \arrow[r]
%          & \mathcal{F}(C)
%          \arrow[r]
%          & 0
%        \end{tikzcd}
%      \end{equation*}
%      is an exact sequence in $\mathsf{D}$.
%    \item if $\mathcal{F}$ is exact, then
%      \begin{equation*}
%        \begin{tikzcd}
%          0
%          \arrow[r]
%          & \mathcal{F}(A)
%          \arrow[r]
%          & \mathcal{F}(B)
%          \arrow[r]
%          & \mathcal{F}(C)
%          \arrow[r]
%          & 0
%        \end{tikzcd}
%      \end{equation*}
%      is an exact sequence in $\mathsf{D}$.
%  \end{itemize}
%\end{theorem}
%\begin{proof}
%
%\end{proof}

\subsection{Length of objects}
\begin{definition}[simple object]
  \label{def:simpleobject}
  A nonzero object $X \in \Obj(\mathsf{C})$ is called \defn{simple} if $0$ and $X$ are its only subobjects.
\end{definition}

\begin{example}
  In $\mathsf{Vect}_{k}$, the only simple object (up to isomorphism) is $k$, taken as a one-dimensional vector space over itself.
\end{example}

\begin{definition}[semisimple object]
  \label{def:semisimpleobject}
  An object $Y \in \Obj(\mathsf{C})$ is \defn{semisimple} if it is isomorphic to a dirct sum of simple objects.
\end{definition}

\begin{example}
  In $\mathsf{Vect}_{k}$, all finite-dimensional vector spaces are semisimple.
\end{example}

\begin{definition}[semisimple category]
  \label{def:semisimple category}
  An abelian category $\mathsf{C}$ is \defn{semisimple} if every object of $\mathsf{C}$ is semisimple.
\end{definition}

\begin{example}
  The category $\mathsf{FinVect}_{k}$ is semisimple.
\end{example}

\begin{definition}[Jordan-H{\"o}lder series]
  \label{def:jordanholderseries}
  Let $X \in \Obj(\mathsf{C})$. A filtration 
  \begin{equation*}
    0 = X_{0} \subset X_{1} \subset \cdots \subset X_{n-1} \subset X_{n} = X
  \end{equation*}
  of $X$ such that $X_{i} / X_{i-1}$ is simple for all $i$ is called a \defn{Jordan H{\"o}lder series} for $X$. The integer $n$ is called the \defn{length} of the series $X_{i}$.
\end{definition}

The importance of Jordan-H{\"o}lder series is the following.

\begin{theorem}[Jordan-H{\"o}lder]
  \label{thm:jordanholdertheorem}
  Let $X_{i}$ and $Y_{i}$ be two Jordan-H{\"o}lder series for some object $X \in \Obj(\mathsf{C})$. Then the length of $X_{i}$ is equal to the length of $Y_{i}$, and the objects $Y_{i}/Y_{i-1}$ are a reordering of $X_{i}/X_{i-1}$.
\end{theorem}
\begin{proof}
  See \cite{EGNO-tensor-categories}, pg. 5, Theorem 1.5.4.
\end{proof}

\begin{definition}[length]
  \label{def:length}
  The \defn{length} of an object $X$ is defined to be the length of any of its Jordan-H{\"o}lder series. This is well-defined by \hyperref[thm:jordanholdertheorem]{Theorem \ref*{thm:jordanholdertheorem}}.
\end{definition}


\section{Tensor Categories}
The following definition is taken almost verbatim from \cite{nlab-deligne-theorem}.
\begin{definition}[tensor category]
  \label{def:tensorcategory}
  Let $k$ be a field. A \defn{$k$-tensor category} $\mathsf{A}$ (as considered by Deligne in \cite{deligne-categories-tensorielle}) is an
  \begin{enumerate}
    \item essentially small (\hyperref[def:essentiallysmall]{Definition \ref*{def:essentiallysmall}})

    \item $k$-linear\footnote{Hence abelian.} (\hyperref[def:linearcategory]{Definition \ref*{def:linearcategory}})

    \item rigid (\hyperref[def:rigidmonoidalcategory]{Definition \ref*{def:rigidmonoidalcategory}})

    \item symmetric (\hyperref[def:symmetricmonoidalcategory]{Definition \ref*{def:symmetricmonoidalcategory}})

    \item monoidal category (\hyperref[def:monoidalcategory]{Definition \ref*{def:monoidalcategory}})
  \end{enumerate}
  such that 
  \begin{enumerate}
    \item the tensor product functor $\otimes\colon \mathsf{A} \times \mathsf{A} \rightsquigarrow \mathsf{A}$ is, in both arguments separately,
      \begin{enumerate}
        \item $k$-linear (\hyperref[def:linearfunctor]{Definition \ref*{def:linearfunctor}})

        \item exact (\hyperref[def:exactfunctor]{Definition \ref*{def:exactfunctor}})
      \end{enumerate}

    \item $\mathrm{End}(1) \simeq k$, where $\mathrm{End}$ denotes the endomorphism ring (\hyperref[def:endomorphismring]{Definition \ref*{def:endomorphismring}}).
  \end{enumerate}
\end{definition}

\begin{example}
  $\mathsf{Vect}_{k}$ is \emph{not} a tensor category because it is not essentially small; there is one isomorphism class of vector spaces for each cardinal, and there is no set of all cardinals. However, its subcategory $\mathsf{FinVect}_{k}$ is a tensor category. 
\end{example}

In Deligne's proof of \hyperref[thm:delignestheorem]{Theorem \ref*{thm:delignestheorem}}, several notions of size are used. We collect them here.

\begin{definition}[finite tensor category]
  \label{def:finitetensorcategory}
  A $k$-tensor category $\mathsf{A}$ is called \defn{finite} (over $k$) if 
  \begin{enumerate}
    \item There are only finitely many simple objects in $\mathsf{A}$, and each of them admits a projective presentation. 

    \item Each object $A$ of $\mathsf{A}$ is of finite length.

    \item For any two objects $A$, $B$ of $\mathsf{A}$, the hom-object (i.e. $k$-vector space) $\Hom_{\mathsf{A}}(A, B)$ is finite-dimensional.
  \end{enumerate}
\end{definition}

\begin{example}
  The category $\mathsf{FinVect}_{k}$ is finite. 
  \begin{enumerate}
    \item The only simple object is $k$ taken as a one-dimensional vector space over itself.

    \item The length of a finite-dimensional vector space is simply its dimension.

    \item The vector space $\Hom_{\mathsf{FinVect}}(V, W)$ has dimension $\mathrm{dim}(V) \times\mathrm{dim}(W)$.
  \end{enumerate}
\end{example}

\begin{definition}[finitely $\otimes$-generated]
  \label{def:finitelygenerated}
  A $k$-tensor category $\mathsf{A}$ is called \defn{finitely $\otimes$-generated} if there exists an object $E \in \Obj(\mathsf{A})$ such that every other object $X \in A$ is a subquotient (\hyperref[def:subobjectquotientobject]{Definition \ref*{def:subobjectquotientobject}}) of a finite direct sum of tensor products of $E$; that is to say, if there exists a finite collection of integers $n_{i}$ such that $X$ is a subquotient of $\bigoplus_{i} E^{\otimes^{n_{i}}}$.
  \begin{equation*}
    \begin{tikzcd}
      & \bigoplus_{i} E^{\otimes^{n_{i}}}
      \arrow[d, twoheadrightarrow, "\pi"]
      \\
      X
      \arrow[r, hookrightarrow, "\iota"]
      & \left( \bigoplus_{i} E^{\otimes^{n_{i}}} \right) / Q
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{example}
  The category $\mathsf{FinVect}_{k}$ is finitely generated since any finite-dimensional vector space is isomorphic to $k^{n} = k \oplus \cdots \oplus k$ for some $n$.
\end{example}

\begin{definition}[subexponential growth]
  \label{def:subexponentialgrowth}
  A tensor category $\mathsf{A}$ has \defn{subexponential growth} if, for each object $X$ there exists a natural number $N_{X}$ such that 
  \begin{equation*}
    \mathrm{len}(X^{\otimes_{n}}) \leq (N_{X})^{n}.
  \end{equation*}
\end{definition}

\begin{example}
  The category $\mathsf{FinVect}_{k}$ has subexponential growth. For any finite-dimensional vector space $V$, we always have 
  \begin{equation*}
    \mathrm{dim}(V^{\otimes^{n}}) = (\dim(V))^{n},
  \end{equation*}
  so we can take $N_{V} = \mathrm{dim}(V)$.
\end{example}

\begin{theorem}
  Let $\mathsf{A}$ be a tensor category, and suppose that
  \begin{enumerate}
    \item every object $A \in \Obj(\mathsf{A})$ has a finite length

    \item the dimension of every hom space $\Hom_{\mathsf{A}}(A, B)$ is finite over $k$.
  \end{enumerate}

  Then the category $\mathsf{Ind}(\mathsf{A})$ of ind-objects of $\mathsf{A}$ (\hyperref[def:categoryofindobjects]{Definition \ref*{def:categoryofindobjects}}) has the following properties.
  \begin{enumerate}
    \item $\mathsf{Ind}(\mathsf{A})$ is abelian (\hyperref[def:abeliancategory]{Definition \ref*{def:abeliancategory}}).

    \item $A \hookrightarrow \mathsf{Ind}(\mathsf{A})$ is a full subcategory (cf. \hyperref[note:fullinclusionintoindcategory]{Note \ref*{note:fullinclusionintoindcategory}}).

    \item The tensor product on $\mathsf{A}$ extends to $\mathsf{Ind}(\mathsf{A})$ via
      \begin{align*}
        X \otimes Y &\simeq (\lim_{\rightarrow i}X_{i}) \otimes (\lim_{\rightarrow j}Y_{j}) \\
        &\simeq \lim_{\rightarrow i,j} (X_{i} \otimes Y_{j}).
      \end{align*}

    \item The category $\mathsf{Ind}(\mathsf{A})$ fails only to be a tensor category because it is not necessarily essentially small and rigid. More specifically, an object $A \in \mathsf{Ind}(\mathsf{A})$ is dualizable if and only if it is in $\mathsf{A}$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Proposition 3.38 in \cite{nlab-deligne-theorem}.
\end{proof}

\begin{definition}[tensor functor]
  \label{def:tensorfunctor}
  Let $(\mathscr{A}, \otimes_{A}, 1_{A})$ and $(\mathscr{B}, \otimes_{B}, 1_{B})$ be $k$-tensor categories. A functor $\mathcal{F}\colon \mathscr{A} \to \mathscr{B}$ is called a \defn{tensor functor} if it is
  \begin{enumerate}
    \item braided (\hyperref[def:braidedmonoidalfunctor]{Definition \ref*{def:braidedmonoidalfunctor}}) and

    \item strong monoidal (\hyperref[def:monoidalfunctor]{Definition \ref*{def:monoidalfunctor}}).
  \end{enumerate}
\end{definition}

%In any $k$-tensor category $\mathscr{A}$, the hom-sets have the structure of $k$-vector spaces, and we can view them as living in $\mathsf{Vect}_{k}$. This allows us to treat vector spaces, in certain situations, as `honorary $\mathscr{A}$-objects'.
%\begin{definition}[tensor product of a vector space and a tensor category object]
%  \label{def:tensorproductofavectorspaceandatensorcategoryobject}
%  Let $\mathscr{A}$ be a $k$-tensor category. Let $V$ be a vector space. Define a functor $(-) \tilde{\otimes} X\colon \mathsf{Vect}_{k} \to \mathscr{A}$ as left-adjoint to $\Hom_{\mathscr{A}}(X,-)$:
%  \begin{equation*}
%    \Hom_{\mathscr{A}}(V \tilde{\otimes} X, Y) \simeq \Hom_{\mathsf{Vect}_{k}}(V, \Hom_{\mathscr{A}}(X, Y)).
%  \end{equation*}
%  Indeed, this extends to a functor $\tilde{\otimes}\colon \mathsf{Vect}_{k} \times \mathscr{A} \to \mathscr{A}$ in the obvious way. It is also not difficult to check that it is $k$-linear in each slot.
%\end{definition}
%
%\begin{definition}[hom between vector space and tensor category object]
%  \label{def:hombetweenvectorspaceandtensorcategoryobject}
%  Let $\mathscr{A}$ be a tensor category and $(-) \tilde{\otimes} X$ the functor from \hyperref[def:tensorproductofavectorspaceandatensorcategoryobject]{Definition \ref*{def:tensorproductofavectorspaceandatensorcategoryobject}}. Define a functor $\mathscr{H}om()$
%\end{definition}
%
%\begin{notation}
%  Although the functor $\tilde{\otimes}$ defined above is obviously not the same as the bifunctor $\otimes$ from the monoidal structure on $\mathscr{A}$, we will drop the tilde from now on. The idea is to view vector spaces as \emph{almost} $\mathscr{A}$-objects.
%\end{notation}

\section{Internalization}
One of the things category theory is best for is generalization. One of the most powerful ways of doing this is known as \emph{internalization}.

\subsection{Internal groups}
\begin{definition}[group object]
  \label{def:groupobject}
  Let $\mathsf{C}$ be a category with binary products $\times$ and a terminal object $*$. A \defn{group object} in $\mathsf{C}$ (or a \emph{group internal to $\mathsf{C}$}) is an object $G \in \Obj(\mathsf{C})$ together with 
  \begin{itemize}
    \item a map $e\colon * \to G$, called the \emph{unit map};

    \item a map $(-)^{-1}\colon G \to G$, called the \emph{inverse map}; and

    \item a map $m\colon G \times G \to G$, called the \emph{multiplication map}
  \end{itemize}
  such that 
  \begin{itemize}
    \item multiplication is associative, i.e. the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}
          G \times G \times G
          \arrow[r, "\mathrm{id}_{G} \times m"]
          \arrow[d, swap, "m \times \mathrm{id}_{G}"]
          & G \times G
          \arrow[d, "m"]
          \\
          G \times G
          \arrow[r, swap, "m"]
          & G
        \end{tikzcd}
      \end{equation*}

    \item the unit picks out the `identity element,' i.e. the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}
          G
          \arrow[r, "e \times \mathrm{id}_{G}"]
          \arrow[d, swap, "\mathrm{id}_{G} \times e"]
          & G \times G
          \arrow[d, "m"]
          \\
          G \times G
          \arrow[r, swap, "m"]
          & G
        \end{tikzcd}
      \end{equation*}

    \item The inverse map behaves as an inverse, i.e. the following diagram commutes.
      \begin{equation*}
        \begin{tikzcd}[row sep=3.6em,column sep=1em]
          & G\times G 
          \arrow[rr,"(-)^{-1}\times\id"] 
          & & G\times G 
          \arrow[dr,"m"] 
          \\
          G 
          \arrow[ur,"\delta"] 
          \arrow[rr,"\exists!"] 
          \arrow[dr,"\delta"'] 
          & & * 
          \arrow[rr,"e"] 
          & & G 
          \\
          & G\times G 
          \arrow[rr,"\id\times (-)^{-1}"'] 
          & & G\times G 
          \arrow[ur,"m"']
        \end{tikzcd}
      \end{equation*}
      Here, $\delta\colon G \to G \times G$ is the diagonal map defined uniquely by the universal property of the product
      \begin{equation*}
        \begin{tikzcd}
          & G
          \arrow[dr, "\mathrm{id}_{G}"]
          \arrow[dl, swap, "\mathrm{id}_{G}"]
          \arrow[d, "\delta"]
          \\
          G 
          & G \times G
          \arrow[l, "\pi_{1}"]
          \arrow[r, swap, "\pi_{2}"]
          & G
        \end{tikzcd}.
      \end{equation*}
  \end{itemize}

  It will be convenient to represent the above data in the following way.
  \begin{equation*}
    \begin{tikzcd}
      G \times G
      \arrow[d, "m"]
      \\
      G
      \arrow[loop right, "i"]
      \\
      *
      \arrow[u, swap, "e"]
    \end{tikzcd}
  \end{equation*}
\end{definition} 

%\subsection{Internal categories}
%\begin{definition}[internal category]
%  \label{def:internalcategory}
%  Let $\mathsf{A}$ be a category with pullbacks.\footnote{This condition can be relaxed; see \hyperref[note:internalcategoriesdontneedallpullbacks]{Note \ref*{note:internalcategoriesdontneedallpullbacks}}.} A \defn{category internal to $\mathsf{A}$} is a sextuple $(C_{0}, C_{1}, s, t, e, c)$, where
%  \begin{itemize}
%    \item $C_{0} \in \Obj(\mathsf{A})$ is the \emph{object of objects}, 
%    \item $C_{1} \in \Obj(\mathsf{A})$ is the \emph{object of morphisms},
%    \item $s$, $t\colon C_{1} \to C_{0}$ are morphisms called the \emph{source} and \emph{target morphisms}.
%    \item $e\colon C_{0} \to C_{1}$ is the \emph{identity-assigning morphism}
%    \item $c\colon C_{1} \times_{C_{0}} C_{1} \to C_{1}$ is the \emph{composition morphism},
%  \end{itemize}
%  where $C_{1} \times_{C_{0}} C_{1}$ is given by the following pullback square.
%  \begin{equation*}
%    \begin{tikzcd}
%      C_{1} \times_{C_{0}} C_{1}
%      \arrow[r, "\pi_{2}"]
%      \arrow[d, swap, "\pi_{1}"]
%      & C_{1}
%      \arrow[d, "s"]
%      \\
%      C_{1}
%      \arrow[r, swap, "t"]
%      & C_{0}
%    \end{tikzcd}
%  \end{equation*}
%
%  Further, we require that the following diagrams commute.
%  \begin{itemize}
%    \item The identity morphism on each object goes to and from that object.
%      \begin{equation*}
%        \begin{tikzcd}
%          C_{0} 
%          \arrow[r, "e"]
%          \arrow[dr, swap, "1_{C_{0}}"]
%          & C_{1} 
%          \arrow[d, "s"]
%          \\
%          & C_{0}
%        \end{tikzcd}
%        \qquad
%        \begin{tikzcd}
%          C_{0} 
%          \arrow[r, "e"]
%          \arrow[dr, swap, "1_{C_{0}}"]
%          & C_{1} 
%          \arrow[d, "t"]
%          \\
%          & C_{0}
%        \end{tikzcd}
%      \end{equation*}
%
%    \item The source and target morphisms $s$ and $t$ should behave naturally under composition, i.e. we should have a law of the form: The source of $f \circ g$ should be the source of $g$, and the target should be the target of $f$.
%      \begin{equation*}
%        \begin{tikzcd}
%          C_{1} \times_{C_{0}} C_{1} 
%          \arrow[r, "c"]
%          \arrow[d, swap, "\pi_{1}"]
%          & C_{1}
%          \arrow[d, "s"]
%          \\
%          C_{1}
%          \arrow[r, swap, "s"]
%          & C_{0}
%        \end{tikzcd}
%        \qquad
%        \begin{tikzcd}
%          C_{1} \times_{C_{0}} C_{1} 
%          \arrow[r, "c"]
%          \arrow[d, swap, "\pi_{1}"]
%          & C_{1}
%          \arrow[d, "s"]
%          \\
%          C_{1}
%          \arrow[r, swap, "s"]
%          & C_{0}
%        \end{tikzcd}
%      \end{equation*}
%
%    \item Composition should be associative.
%      \begin{equation*}
%        \begin{tikzcd}[column sep=large, row sep=large]
%          C_{1} \times_{C_{0}} C_{1} \times_{C_{0}} C_{1}
%          \arrow[r, "c \times_{C_{0}} 1_{C_{1}}"]
%          \arrow[d, swap, "1_{C_{1}} \times_{C_{0}} c"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[d, "c"]
%          \\
%          C_{1} \times_{C_{0}} C_{1}
%          \arrow[r, swap, "c"]
%          & C_{1}
%        \end{tikzcd}
%      \end{equation*}
%
%    \item The units should act like units.
%      \begin{equation*}
%        \begin{tikzcd}[column sep=large, row sep=large]
%          C_{0} \times_{C_{0}} C_{1}
%          \arrow[r, "e \times_{C_{0}} 1_{C_{1}}"]
%          \arrow[dr, swap, "\pi_{2}"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[d, "c"]
%          & C_{1} \times_{C_{0}} C_{0}
%          \arrow[l, swap, "1_{C_{1}} \times_{C_{0}} e"]
%          \arrow[dl, "\pi_{1}"]
%          \\
%          & C_{1}
%        \end{tikzcd}
%      \end{equation*}
%  \end{itemize}
%
%  We can represent diagrammatically the information in the sextuple $(C_{0}, C_{1}, s, t, e, c)$ as follows.
%  \begin{equation*}
%    \begin{tikzcd}
%      C_{1} \times_{C_{0}} C_{1}
%      \arrow[d, "c"]
%      \\
%      C_{1}
%      \arrow[d, shift left, bend left, "t"]
%      \arrow[d, shift right, bend right, swap, "s"]
%      \\
%      C_{0}
%      \arrow[u, swap, "i"]
%    \end{tikzcd}
%  \end{equation*}
%\end{definition}
%
%\begin{note}
%  We need the fibered product in the definition of the pullback to ensure that (in the language of elements) given morphisms $f$ and $g$, we can only form the composition if the codomain of $f$ is equal to the domain of $g$.
%\end{note}
%
%\begin{note}
%  \label{note:internalcategoriesdontneedallpullbacks}
%  It is not strictly necessary that $\mathsf{A}$ have pullbacks; all that is necessary is that $\mathsf{A}$ have pullbacks along $s$ and $t$. This is a considerably less restrictive assumption.
%\end{note}
%
%\begin{example}
%  A category internal to $\mathsf{Set}$ is a small category.
%\end{example}
%
%\begin{definition}[groupoid]
%  \label{def:groupoid}
%  A \defn{groupoid} is a category in which every morphism is an isomorphism.
%\end{definition}
%
%\begin{note}
%  A groupoid with a single object coincides with idea given in \hyperref[eg:groupsaregroupoidswithoneobject]{Example \ref*{eg:groupsaregroupoidswithoneobject}}. This led Aluffi to make the following joke in \cite{aluffi-algebra-chapter-0}.
%\end{note}
%
%\begin{joke}[Aluffi, \cite{aluffi-algebra-chapter-0}, pg. 41]
%  A group is a groupoid with a single object.
%\end{joke}
%
%\begin{definition}[internal groupoid]
%  \label{def:internalgroupoid}
%  Let $\mathsf{A}$ be a category with `enough pullbacks' (see \hyperref[note:internalcategoriesdontneedallpullbacks]{Note \ref*{note:internalcategoriesdontneedallpullbacks}}), and let $C = (C_{0}, C_{1}, s, t, e, c)$ be a category internal to $\mathsf{A}$. We can turn $C$ into a \defn{groupoid internal to $\mathsf{A}$} with an additional morphism
%  \begin{equation*}
%    i\colon C_{1} \to C_{1},
%  \end{equation*}
%  the \emph{inverse morphism}, such that the following diagrams commute.
%  \begin{itemize}
%    \item The source of the morphism is equal to the target of the inverse, and vice versa.
%      \begin{equation*}
%        \begin{tikzcd}
%          C_{1}
%          \arrow[r, "i"]
%          \arrow[rd, swap, "t"]
%          & C_{1}
%          \arrow[d, "s"]
%          \\ 
%          & C_{0}
%        \end{tikzcd}
%        \qquad
%        \begin{tikzcd}
%          C_{1}
%          \arrow[r, "i"]
%          \arrow[rd, swap, "s"]
%          & C_{1}
%          \arrow[d, "t"]
%          \\ 
%          & C_{0}
%        \end{tikzcd}
%      \end{equation*}
%
%    \item The inverse behaves like a real inverse on the right,
%      \begin{equation*}
%        \begin{tikzcd}[column sep=large, row sep=large]
%          C_{1}
%          \arrow[r, "\Delta"]
%          \arrow[d, swap, "s"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[r, "1_{C_{1}} \times_{C_{0}} i"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[d, "c"]
%          \\
%          C_{0}
%          \arrow[rr, swap, "e"]
%          & & C_{1}
%        \end{tikzcd}
%      \end{equation*}
%      and on the left.
%      \begin{equation*}
%        \begin{tikzcd}[column sep=large, row sep=large]
%          C_{1}
%          \arrow[r, "\Delta"]
%          \arrow[d, swap, "s"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[r, "1_{C_{1}} \times_{C_{0}} i"]
%          & C_{1} \times_{C_{0}} C_{1}
%          \arrow[d, "c"]
%          \\
%          C_{0}
%          \arrow[rr, swap, "e"]
%          & & C_{1}
%        \end{tikzcd}
%      \end{equation*}
%  \end{itemize}
%\end{definition}
%
%\begin{note}
%  An internal groupoid should be thought of as a family of internal groups indexed by the `elements' of the object of objects. 
%\end{note}
%
%\begin{lemma}
%  \label{lemma:interalgroupoidsaregroupobjectsinslicecategory}
%  Let $C = (C_{0}, C_{1}, s, t, e, c, i)$ be a groupoid internal to $\mathsf{C}$ such that $s = t$. Then $C$ is equivalently a group internal to the slice category $(\mathsf{C} \downarrow C_{0})$.
%\end{lemma}
%
%%\section{Higher categories} \label{sec:highercategories}
%%This section follows \cite{baez-higher-categories} closely.
%%
%%One of the main benefits of categories is that they let us distinguish between different notions of sameness. In a set, two elements are either the same or different, and that is that. In a category, however, objects can be the same in certain ways without being identical: there are many $n$-dimensional real vector spaces, but they are all isomorphic. However this notion of `sameness without identity' is not omnipresent in category theory; we still have to resort to the limited, set-theoretic sense of equivalence when talking about morphisms.
%%
%%In higher category theory, this is remedied by introducing 2-morphisms, which are `morphisms between morphisms.' We can then say that two 1-morphisms are isomorphic if there is a pair of mutually inverse 2-morphisms between them. If you take a category and add 2-morphisms, what you get is called a 2-category.
%%\begin{equation*}
%%  \begin{tikzcd}[row sep=huge, column sep=huge]
%%    A 
%%    \arrow[r, bend left, "f", ""{name=UL, anchor=south east}, ""{name=UR, anchor=south west}]
%%    \arrow[r, swap, bend right, "g", ""{name=DL, anchor=north east}, ""{name=DR, anchor=north west}]
%%    \arrow[Rightarrow, from=UL , to=DL,] 
%%    \arrow[Leftarrow, from=UR , to=DR,] 
%%    & B
%%  \end{tikzcd}
%%\end{equation*}
%%Of course, we now have no notion of isomorphism for 2-categories, so we must add 3-morphisms, etc. The result of the $n$th iteration of this procedure is called an $n$-category. If you keep going forever (or more rigorously, formally consider the object you'd get if you did), you get an $\infty$-category.
%%
%%You may have noticed that we have been using the same notation ($\Rightarrow$) for 2-morphisms and natural transformations. The reason for this, as you may have guessed, is that natural transformations are the morphisms in the 2-category $\mathsf{Cat}$ of (small) categories.
%
%\chapter{A few concepts from algebraic geometry}
%\section{Elementary notions}
%The elementary notions of algebraic geometry are assumed, but a few definitions are given here for concreteness. For the remainder of this section, let $k$ be an algebraically closed field of characteristic $0$.
%\begin{definition}[Zariski topology]
%  \label{def:zariskitopology}
%  We define a topology on $k^{n}$, called the \defn{Zariski topology}, as follows. Let $A = k[x_{1},\dots,x_{n}]$ be the ring of polynomials on $k^{n}$. For any subset $S \subseteq A$, define
%  \begin{equation*}
%    Z(S) = \left\{ (x_{1},\dots,x_{n}) \in k^{n}\,\Big|\, f(x_{1},\dots,x_{n}) = 0\text{ for all }f \in S \right\}.
%  \end{equation*}
%  Clearly, if $(S) = \mathfrak{a}$ is the ideal generated by $S$, then $Z(S) = Z(\mathfrak{a})$.
%
%  The Zariski topology on $k^{n}$ is then the topology whose closed sets are given by $Z(\mathfrak{a})$ for ideals $\mathfrak{a}$ of $A$.
%\end{definition}
%
%\begin{definition}[affine space]
%  \label{def:affinespace}
%  Denote by $\mathbb{A}^{n}$ the space $k^{n}$, together with the Zariski topology. 
%
%  Note that if this were a real textbook on algebraic geometry, we would be careful about the definition of $\mathbb{A}^{n}$, defining it as a torsor over the the vector space $k^{n}$.
%\end{definition} 
%
%\begin{definition}[affine algebraic variety]
%  \label{def:algebraicvariety}
%  A subset $S \subseteq \mathbb{A}^{n}$ is called an \defn{affine algebraic variety} if it is closed in the Zariski topology.
%\end{definition}
%
%\begin{note}
%  Many authors (e.g. Hartshorne \cite{hartshorne-algebraic-geometry}) demand that a variety be irreducible. 
%\end{note}
%
%\section{Sheaves}
%\subsection{Presheaves}
%
%\begin{definition}[category of opens] 
%  \label{def:opencategory} 
%  Let $(X, \tau)$ be a topological space. The \defn{category of opens} of $X$, $\mathsf{Open}(X)$ is the category whose objects are 
%  \begin{equation*}
%    \Obj(\mathsf{Open(X)}) = \tau 
%  \end{equation*} and whose morphisms are, for $U$, $V \in \tau$, 
%  \begin{equation*} 
%    \Hom(U,V) =
%    \begin{cases} 
%      r(V,U), & U \subseteq V \\ 
%      \emptyset, & \text{otherwise}.  
%    \end{cases} 
%  \end{equation*}
%
%  To put it another way, if $U$ and $V$ are open sets in $X$, then there is exactly one morphism from $U$ to $V$ if $U \subseteq V$, and none otherwise.
%\end{definition}
%
%Of course, we must check that $\mathsf{Open}(X)$ really is a category.
%\begin{enumerate} 
%  \item If $U \subseteq V$ and $V \subseteq W$, then $U \subseteq W$, so we are forced to define the composition 
%    \begin{equation*} 
%      r(W,V) \circ r(V,U) = r(W,U).  
%    \end{equation*} 
%  \item The composition $\circ$ is associative since set inclusion is.
%
%  \item The identity morphism $r(U,U)$ is the identity with respect to composition: restricting a set to itself is the same thing as not restricting.  
%\end{enumerate}
%
%\begin{definition}[presheaf] 
%  \label{def:presheaf}
%  A \defn{presheaf} on a topological space $(X,\tau)$ is a contravariant functor from $\mathsf{Open}(X)$ to some category $\mathsf{C}$. Usually, $\mathsf{C}$ will be the category $\mathsf{Ring}$ of rings.  
%\end{definition}
%
%\begin{example}[important example] 
%  \label{eg:functionpresheaf} 
%  The prototypical example of a presheaf on a topological space $X$ is the presheaf $\mathcal{C}_{X}$ of real continuous functions on $X$. Since by \hyperref[def:presheaf]{Definition \ref*{def:presheaf}} $\mathcal{C}_X$ must be a contravariant functor, we need to specify what $\mathcal{C}_{X}$ does to the objects $\Obj(\mathsf{Open}(X))$ and the morphisms $\mathrm{res}_{V,U}$.
%
%  For every open set $U \in \Obj(\mathsf{Open}(X))$, define
%  \begin{equation*} 
%    \mathcal{C}_{X}(U) = \left\{ f\colon U \to \R\,\big| \, f\; \mathrm{ continuous} \right\}, 
%  \end{equation*} 
%  the ring of all real continuous functions $U \to \R$.
%
%  Recall that for two open sets $U$ and $V \in \Obj(\mathsf{Open}(X))$, there is a unique morphism $r(V,U)$ from $U$ to $V$ if and only if $V \subseteq U$. Thus, we need to assign to each $r(V,U)$ a ring homomorphism from $\mathcal{C}_{X}(V)$ to $\mathcal{C}_{X}(U)$. We use the restriction homomorphism, which maps $f \in \mathcal{C}_{X}(V)$ to $f|_{U}$, its restriction to $U$. We denote this by
%  \begin{equation*}
%    \mathrm{res}_{V,U}(f) = f|_{U}.
%  \end{equation*}
%
%  In other words,
%  \begin{equation*}
%    \mathcal{C}_{X}(r(V,U)) = \mathrm{res}_{V,U}.
%  \end{equation*}
%\end{example}
%
%\begin{example}
%  \label{eg:smoothpresheaf}
%  Let $M$ be a $C^{\infty}$ manifold, and let $\mathsf{Open}(M)$ be its category of opens. Then for each $U \in \Obj(\mathsf{Open}(M))$, define a functor $C^{\infty}\colon \mathsf{Open}(M)^{\text{op}} \rightsquigarrow \mathsf{Ring}$ on objects by
%  \begin{equation*}
%    C^{\infty}(U) = \left\{ f\colon U \to \R\,\big|\, f\text{ smooth} \right\},
%  \end{equation*}
%  and on morphisms by restriction. Then $C^{\infty}$ is a presheaf.
%\end{example}
%
%\subsection{Sheaves}
%\begin{definition}[sheaf]
%  \label{def:sheaf}
%  Let $X$ be a topological space. A presheaf $\mathcal{F}$ on $X$ is called a \defn{sheaf} if it satisfies the following:
%  \begin{enumerate}
%    \item \textbf{Identity:} Let $U \subseteq X$ be an open set, and let $\left\{ U_{i} \right\}$ be an open cover of $U$. If $f_{1}$, $f_{2} \in \mathcal{F}(U)$ such that
%      \begin{equation*}
%        f_{1}|_{U_{i}} = f_{2}|_{U_{i}}
%      \end{equation*}
%      for all $i$, then $f_{1} = f_{2}$. That is to say, if two sections of $\mathcal{F}$ over $U$ agree on every element of an open cover of $U$, then they must agree on $U$.
%
%    \item \textbf{Glueability:} Suppose $\left\{ U_{i} \right\}$ is an open cover of $U$, and $f_{i} \in \mathcal{F}(U_{i})$ is a collection of sections of $\mathcal{F}$ such that if $U_{i} \cap U_{j} \neq \emptyset$ then
%      \begin{equation*}
%        f_{i}|_{U_{i} \cap U_{j}} = f_{j}|_{U_{i} \cap U_{j}}.
%      \end{equation*}
%      Then there exists some $f \in \mathcal{F}(U)$ such that 
%      \begin{equation*}
%        f|_{U_{i}} = f_{i}
%      \end{equation*}
%      for all $i$. In other words, If we have sections of an open cover of $U$ which agree on overlaps, then we can glue them together to get a section on all of $U$.
%  \end{enumerate}
%\end{definition}
%
%\begin{note}
%  Here is another definition of a sheaf: A presheaf $\mathcal{F}$ on a topological space $X$ is a \emph{sheaf} if for any open cover $\left\{ U_{i} \right\}_{i \in I}$ of an open set $U$, the diagram
%  \begin{equation*}
%    \begin{tikzcd}
%      \mathcal{F}(U)
%      \arrow[r, "\gamma"]
%      & \prod\limits_{i \in I} \mathcal{F}(U_{i})
%      \arrow[r, shift left, "\alpha"]
%      \arrow[r, shift right, swap, "\beta"]
%      & \prod\limits_{i,j \in I} \mathcal{F}(U_{i} \cap U_{j})
%    \end{tikzcd}
%  \end{equation*}
%  is an equalizer.
%
%  The map $\gamma$ is constructed as follows. For each $U_{i} \subseteq U$, there is a restriction map $\mathrm{res}_{U_{i}, U}\colon \mathcal{F}(U) \to \mathcal{F}(U_{i})$. The universal property of the product turns these into one map $\mathcal{F}(U) \to \prod_{i \in I} \mathcal{F}(U_{i})$.
%
%  The map $\alpha$ is constructed similary from the maps $\mathrm{res}_{U_{i} \cap U_{j}, U_{i}}$.
%
%  For each $\mathcal{F}(U_{i} \cap U_{j})$, there is a canonical projection down to $\mathcal{F}(U_{j})$, and from there a restriction $\mathrm{res}_{U_{j} \cap U_{j}, U_{i}}$. The map $\beta$ is the composition of these.
%
%  To see that this is an equality, 
%\end{note} 
%
%\begin{example}[important example continued]
%  \label{eg:functionsheaf}
%  The presheaf $\mathcal{C}_{X}$ from \hyperref[eg:functionpresheaf]{Example \ref*{eg:functionpresheaf}} is a sheaf, as is the presheaf from \hyperref[eg:smoothpresheaf]{Example \ref*{eg:smoothpresheaf}}.
%\end{example}
%
%\begin{example}[a presheaf which is not a sheaf]
%  \label{eg:presheafwhichisnotasheaf}
%  Let $X = \R$, and define a presheaf $\mathcal{F}$ on $X$ via
%  \begin{equation*}
%    \mathcal{F}(U) = \left\{ f\colon U \to \R\, \big|\, f\;\mathrm{ bounded} \right\}.
%  \end{equation*}
%  This is a presheaf since the restriction of a bounded map is bounded.
%
%  However, we do not have condition 2 of \hyperref[def:sheaf]{Definition \ref*{def:sheaf}}: glueability. To see this, consider the following open cover
%  \begin{equation*}
%    \R = \bigcup_{n=-\infty}^{\infty} U_{n}, \qquad U_{n} = (n-1, n+1).
%  \end{equation*}
%  Clearly, the identity function $1_{U_{n}}$ on $U_{n}$ is bounded for all $n$. However, the function $f:\R \to \R$ which agrees with $1_{U_{n}}$ on all the $U_{n}$ is the identity function $1_{\R}$, which is unbounded.
%
%\end{example}
%
%
%\begin{definition}[stalk]
%  \label{def:stalk}
%  Let $X$ be a topological space, $x \in X$, and let $\mathcal{F}$ be a presheaf on $X$. The \defn{stalk} of $\mathcal{F}$ at $x$ is
%  \begin{equation*}
%    \mathcal{F}_{x} = \left\{ \left( f, U \right)\,\big|\, x \in U,\, f \in \mathcal{F}(U) \right\}/\sim,
%  \end{equation*}
%  where 
%  \begin{equation*}
%    \left( f, U \right) \sim \left( g, V \right)
%  \end{equation*}
%  if there exists an open set $W \subseteq U \cap V$ with $x\in W$ such that $f|_{W} = g|_{W}$.
%\end{definition}
%
%\begin{lemma}
%  If $\mathcal{F}$ is a sheaf of objects with some algebraic structure, like rings, then the stalk $\mathcal{F}_{x}$ inherits this algebraic structure. 
%\end{lemma}
%\begin{proof}
%  For the sake of concreteness, consider the case in which $\mathcal{F}$ is a presheaf of rings. Let $[(f, U)]$, $[(g, V)] \in \mathcal{F}_{x}$. We need to define 
%  \begin{equation*}
%    [(f, U)] + [(g, V)]\qquad\text{and}\qquad [(f, U)] \cdot [(g, V)].
%  \end{equation*}
%  In each case, we can simply use representatives of the equivalence classes, letting
%  \begin{equation*}
%    [(f, U)] + [(g, V)] = [(f+g, U \cap V)],\qquad\text{and}\qquad[(f, U)]\cdot [(g, V)] = [(f\cdot g, U \cap V)].
%  \end{equation*}
%  We need of course to prove well-definition: that if $(f, U) \sim (f', U')$ and $(g, V) \sim (g', V')$, then
%  \begin{equation*}
%    [f+g] = [f'+g'],
%  \end{equation*}
%  and similarly for multiplication.
%
%  But if $(f, U) \sim (f', U')$, then there exists an open set $U'' \in U \cap U'$ with $x \in U''$ such that $f|_{U''} = f'|_{U''}$, and similarly $V''$. But since $x \in U''$ and $x \in V''$, we must also have that $x \in U'' \cap V''$. Since $f$ and $f'$ (and $g$ and $g'$) agree on $U'' \cap V''$, so must $f+g$ and $f' + g'$. Thus $[(f + g, U \cap V)]$ is well-defined.
%
%  The proof of the well-definition of multiplication is exactly analogous.
%\end{proof}
%
%We now drop the open set in the notation of an element of a stalk, writing $[f]$ instead of $[(f, U)]$.
%
%\begin{example}[important example continued]
%  \label{eg:stalksofcx}
%  What do the stalks of $\mathcal{C}_{X}$ (\hyperref[eg:functionpresheaf]{Example \ref*{eg:functionpresheaf}}) look like? Let $x \in X$, and define $\varphi\colon (\mathcal{C}_{X})_{x} \to \R$ via
%  \begin{equation*}
%    \varphi\colon [f] \mapsto f(x).
%  \end{equation*}
%
%  This is a ring homomorphism since
%  \begin{equation*}
%    \varphi([f][g]) = \varphi([fg]) = (fg)(x) = f(x)g(x) = \varphi([f])\varphi([g]),
%  \end{equation*}
%  and similarly for addition. It is also surjective; to see this, we need only find a single function which maps to any $r \in \R$. (The constant function will do.)
%
%  Denote $\ker(\varphi) = \mathfrak{m}_{x}$. By the first isomorphism theorem,
%  \begin{equation*}
%    (\mathcal{C}_{X})_{x} / \mathfrak{m}_{x} \simeq \R.
%  \end{equation*}
%  But since $\R$ is a field, $\mathfrak{m}_{x}$ must be maximal. 
%
%  If $\mathfrak{M} \neq \mathfrak{m}_{x}$ is another maximal ideal, then there must exist $[g] \in \mathfrak{M} \setminus \mathfrak{m}_{x}$ (since if $\mathfrak{M} \subseteq \mathfrak{m}_{x}$, $\mathfrak{M}$ isn't maximal). Since $[g] \notin \ker(\varphi)$, $g(x) \neq 0$. But since $g$ is continuous, there exists a neighborhood $V$ of $x$ such that $g \neq 0$ on $V$. 
%
%  But then g is invertible on $V$, and hence is invertible in the stalk $(\mathcal{C}_{X})_{x}$; thus, $\mathfrak{M}$ contains an invertible element, hence contains the identity $1_{R}$, hence is the whole ring; and we have a contradiction.
%
%  Thus each stalk of $\mathcal{C}_{X}$ has a unique maximal ideal.
%\end{example}
%
%\subsection{Ringed spaces}
%\begin{definition}[ringed space] 
%  \label{def:ringedspace} A \defn{ringed space} is a double $(X, \mathcal{O}_{X})$, where $X$ is a topological space and $\mathcal{O}_{X}$ is a sheaf of rings on $X$.
%\end{definition}
%
%\begin{definition}[local ring]
%  \label{def:local ring} 
%  A ring $R$ is \defn{local} if it has a unique maximal ideal.
%\end{definition}
%
%\begin{definition}[locally ringed space]
%  A \defn{locally ringed space} is a ringed space whose stalks are local rings.
%\end{definition}
%
%\begin{example}[important example continued]
%  By the toil of \hyperref[eg:stalksofcx]{Example \ref*{eg:stalksofcx}}, each stalk of $\mathcal{C}_{X}$ is a local ring, hence $\mathcal{C}_{X}$ is a locally ringed space.
%\end{example}
%
%\begin{note}
%  It is \emph{not} necessary that each local section of a locally ringed space be a local ring; only the stalks must be local.
%\end{note}
%
%There are many ways to build sheaves. Here are two.
%
%\begin{definition}[restriction sheaf]
%  \label{def:restrictionsheaf}
%  Let $X$ be a topological space, and $\mathcal{F}$ a sheaf on $X$. Let $U$ be an open subset of $X$. The \defn{restriction sheaf} $\mathcal{F}|_{U}$ is the sheaf which maps open subsets $V\subseteq U$ to their images under $\mathcal{F}$:
%  \begin{equation*}
%    \mathcal{F}_{U}(V) = \mathcal{F}(V),\qquad\text{for }V \subseteq U.
%  \end{equation*}
%
%  For open sets $V$, $W \subseteq U$, the restriction $r(W,V)$ is mapped to 
%  \begin{equation*}
%    \mathcal{F}|_{U}(r(W,V)) \equiv \mathcal{F}(r(W,V)) = \mathrm{res}_{W,V},
%  \end{equation*}
%  the same restriction as in $\mathcal{F}$.
%\end{definition}
%
%\begin{definition}[pushforward sheaf]
%  \label{def:pushforwardsheaf}
%  Let $X$ and $Y$ be topological spaces, $\pi\colon X \to Y$ a continuous function. Let $\mathcal{F}$ be a sheaf on $X$. Define the \defn{pushforward of $\mathcal{F}$ by $\pi$}, denoted $\pi_{*}\mathcal{F}$, by
%  \begin{equation*}
%    (\pi_{*}\mathcal{F})(V) = \mathcal{F}(\pi^{-1}(V)),\qquad V\subseteq Y\text{ open},
%  \end{equation*}
%  and
%  \begin{equation*}
%    (\pi_{*}\mathcal{F})(r(V,U)) = \mathrm{res}_{\pi^{-1}(V), \pi^{-1}(U)}.
%  \end{equation*}
%\end{definition}
%
%We can view sheaves as objects in a category. We can then talk about their morphisms, and find the correct definition of a sheaf isomorphism. 
%
%First, we need to define a homomorphism of presheaves. Since we defined a presheaf on a topological space $X$ as a functor $\mathsf{Open}(X) \rightsquigarrow \mathsf{C}$ for some category $\mathsf{C}$ (\hyperref[def:presheaf]{Definition \ref*{def:presheaf}}), we can define a sheaf homomorphism as a morphism in the functor category $\mathsf{C}^{\mathsf{Open}(X)}$, which is to say
%\begin{definition}[presheaf homomorphism]
%  \label{def:presheafhomomorphism}
%  Let $\mathcal{C}$, $\mathcal{D}$ be $\mathsf{C}$-presheaves on a topological space $X$. A \defn{presheaf homomorphism} $\mathcal{C} \to \mathcal{D}$ is a morphism in the category $\mathsf{C}^{\mathsf{Open}(X)}$, i.e. a natural transformation between $\mathcal{C}$ and $\mathcal{D}$.
%\end{definition}
%
%\begin{definition}[sheaf homomorphism]
%  \label{def:sheafhomomorphism}
%  Since sheaves are in particular presheaves, a \defn{sheaf homomorphism} is simply a presheaf homomorphism between sheaves. 
%\end{definition}
%\begin{definition}[sheaf isomorphism]
%  \label{def:sheafisomorphism}
%  A sheaf isomorphism is defined in the obvious way.
%\end{definition}
%\begin{definition}[locally isomorphic]
%  \label{def:locallyisomorphic}
%  Let $X$ be a topological space, $\mathcal{F}$, and let $\mathcal{G}$ be $\mathsf{C}$-sheaves on $X$. We say that $\mathcal{F}$ is \defn{locally isomorphic} to $\mathcal{G}$ if for every $x \in X$, there exists a neighborhood $U$ of $x$ such that the restriction sheaf (\hyperref[def:restrictionsheaf]{Definition \ref*{def:restrictionsheaf}}) $\mathcal{F}|_{U}$ is sheaf-isomorphic to $\mathcal{G}$.
%\end{definition}
%
%
%\section{Schemes}
%We follow closely the treatment in \cite{milne-affine-group-schemes}, together with some help from \cite{vakil-rising-sea}.
%
%\begin{note}
%  This section really needs a lot of filling out, as well as a fair amount of fixing. The goal of the section is an explanation of the fact that $\mathsf{Alg}_{k}^{\mathrm{op}}$ is categorically equivalent to the category of affine schemes over $k$. This will justify talking about objects in $\mathsf{Alg}_{k}^{\mathrm{op}}$ as if they had geometric structure.
%\end{note}
%
%Let $A$ be a commutative ring. Denote by $V$ the set of all prime ideals in $A$. For any ideal $\mathfrak{a} \in V$, we write
%\begin{equation*}
%  V(\mathfrak{a}) = \left\{ \mathfrak{q} \in V\,\big|\, \mathfrak{a} \subset \mathfrak{q} \right\}.
%\end{equation*}
%
%\begin{theorem}
%  \label{thm:zartopisatopology}
%  The sets $V(\mathfrak{a})$ form the closed sets for a topology for $V$.
%\end{theorem}
%\begin{proof}
%  We need to show three things:
%  \begin{enumerate}
%    \item There exists prime ideals $\mathfrak{p}$, $\mathfrak{q} \in V$ such that $V(\mathfrak{p}) = V$ and $V(\mathfrak{q}) = \emptyset$.
%
%    \item For any prime ideals $\mathfrak{a}$ and $\mathfrak{b}$, there exists a prime ideal $\mathfrak{c}$ such that 
%      \begin{equation*}
%        V(\mathfrak{a}) \cup V(\mathfrak{b}) = V(\mathfrak{c}).
%      \end{equation*}
%
%    \item For any family $\mathfrak{g}_{i}$ of prime ideals, there exists a prime ideal $\mathfrak{h}$ such that 
%      \begin{equation*}
%        \bigcap_{i} V(\mathfrak{g}_{i}) = V(\mathfrak{h}).
%      \end{equation*}
%  \end{enumerate}
%
%  For 1., take $\mathfrak{p} = 0$ and $\mathfrak{q} = V$. For 2., take $\mathfrak{c} = \mathfrak{a} \cap \mathfrak{b}$. For 3., take 
%  \begin{equation*}
%    \mathfrak{h} = \sum_{i} \mathfrak{g}_{i}.
%  \end{equation*}
%\end{proof}
%
%\begin{definition}[prime spectrum]
%  \label{def:primespectrumofaring}
%  The topology defined in \hyperref[thm:zartopisatopology]{Theorem \ref*{thm:zartopisatopology}} is called the \defn{Zariski topology}, and the set $V$ together with the Zariski topology is called the \defn{(prime) spectrum} of $A$, and denoted $\spec(A)$.
%\end{definition}
%
%\begin{definition}[principal open subsets]
%  \label{def:principalopensubsets}
%  For any $f \in A$ define 
%  \begin{equation*}
%    D(f) \equiv A\big((f)\big)^{\mathrm{c}} = \left\{ \mathfrak{p} \in V\,\big|\, f \notin \mathfrak{p} \right\},
%  \end{equation*}
%  where $(f)$ is the ideal generated by $f$. Subsets of this form are called the \emph{principal open subsets} of $V$.
%
%  We will denote by $\mathcal{B}$ the set of all principal open subsets.
%\end{definition}
%
%\begin{theorem}
%  The principle open subsets form a basis for the Zariski topology on $V$.
%\end{theorem}
%\begin{proof}
%
%\end{proof}
%
%Since $\spec(A)$ is a topological space, it makes sense to talk about a sheaf of rings on it. This would entail defining for each open subset of $U \subset A$ a ring $\mathscr{O}_{\spec(A)}(U)$ on it, and checking some axioms. However, it will be efficient to define first a sheaf on the principal open subsets $\mathcal{B}$ of $\spec(A)$, and then show that such a sheaf extends uniquely to all of $\spec(A)$.
%
%Since $\mathcal{B}$ is closed under finite intersections, we can use it as the collection of objects in a category of opens $\mathsf{Open}(\mathcal{B})$ (see \hyperref[def:opencategory]{Definition \ref*{def:opencategory}}).\footnote{This is a slightly different notation to the one in \hyperref[def:opencategory]{Definition \ref*{def:opencategory}}. Here we're specifying the open subsets rather than the underlying topological space.} A sheaf of on $\mathcal{B}$ is a contravariant functor $\mathsf{Open}(\mathcal{B}) \to \mathsf{Ring}$ which satisfies the sheaf conditions (\hyperref[def:sheaf]{Definition \ref*{def:sheaf}}).
%
%We now define a sheaf $\mathscr{O}_{V}$ on $\mathcal{B}$ as follows: For any principal open subset $D$, define
%\begin{equation*}
%  S_{D} = A \setminus \bigcup_{\mathfrak{p} \in D} \mathfrak{p}.
%\end{equation*}
%Clearly, $S_{D}$ is multiplicatively closed. It is also \emph{saturated}, i.e. if $fg \in S_{D}$ then $f \in S_{D}$ and $g \in S_{D}$.
%
%In particular, if $D = D(f)$, then $S_{D}$ is the smallest multiplicatively closed saturated subset containing $f$, i.e.
%\begin{equation*}
%  S_{D(f)} = \left\{ f^{n}\,\big|\, n = 0, 1, 2, \ldots \right\}. 
%\end{equation*}
%
%We then define $\mathscr{O}_{V}(D) = S_{D}^{-1}A$, the localization of $A$ by $S_{D}$ (\hyperref[def:localizationofaring]{Definition \ref*{def:localizationofaring}}). If $D \subset D'$ then $S_{D'} \subset S_{D}$ so there is an embedding $S_{D'}^{-1} A \hookrightarrow S_{D}^{-1} A$. These embeddings are functorial, so $\mathscr{O}_{V}$ is indeed a presheaf. In fact, they satisfy the sheaf conditions.
%
%Write $\Spec(A)$ for the ringed space $(\spec(A), \mathscr{O}_{\spec(A)})$. In fact it is a locally ringed space.
%
%\begin{definition}[affine scheme]
%  \label{def:affinescheme}
%  An \defn{affine scheme} is a ringed space which is isomorphic to $(\spec(A), \mathscr{O}_{\spec(A)}))$ for some $A$. 
%\end{definition}
%
%\begin{lemma}
%  Let $\varphi\colon A \to B$ be a homomorphism of commutative rings. Then if $\mathfrak{b}$ is a prime ideal in $B$, $\varphi^{-1}(\mathfrak{b})$ is a prime ideal in $A$.
%\end{lemma}
%
%\begin{corollary}
%  The map $\spec$ extends to a contravariant functor $\mathsf{CRing} \to \mathsf{Top}$.
%\end{corollary}
%
%
\chapter{Some basic superalgebra} \label{ch:superalgebra}
In this chapter we give some basic definitions of various super-* structures.

Most of the material in this chapter is from \cite{manin-gauge-fields}.

\section{Super rings}
\begin{definition}[super ring]
  \label{def:superring}
  A \defn{super ring} (or \defn{$\Z/2\Z$ graded ring}) $A$ is, additively, an abelian group $A$ with a direct sum decomposition
  \begin{equation*}
    A = A_{0} \oplus A_{1}.
  \end{equation*}
  Elements $a\in A_{0}$ are said to be of degree zero, denoted $\tilde{a} = 0$; elements $b\in A_{1}$ are said to be of degree one, denoted $\tilde{b} = 1$.

  Additionally, there is a multiplicative structure which is associative and distributive from the left and the right; the only change from the normal definition of a ring is that the multiplicative structure must obey the axiom
  \begin{equation*}
    \widetilde{ab} = \tilde{a} + \tilde{b},
  \end{equation*}
  where the addition is taken modulo 2.
\end{definition}

\begin{note}
  Of course, not all elements $c \in A$ are in $A_{0}$ or $A_{1}$; formulae which talk about the grading must be understood to be extended via additivity. If $a = a_{0} + a_{1}$ and $b = b_{0} + b_{1}$ so
  \begin{equation*}
    ab = a_{0} b_{0} + a_{1} b_{0} + a_{0} b_{1} + a_{1} b_{1},
  \end{equation*}
  the axiom concerning the multiplicative grading should be taken to say that the grading applies to each monomial.
\end{note}

\begin{example}[exterior algebra]
  \label{eg:exterioralgebra}
  Consider the exterior algebra
  \begin{equation*}
    \bigwedge \R^{n} = \bigoplus_{i=0}^{n} \bigwedge\nolimits^{i} \R^{n}.
  \end{equation*}
  With multiplication given by the wedge product, $\bigwedge \R^{n}$ is a super ring with grading given by
  \begin{equation*}
    \left( \bigoplus_{i\text{ even}} \bigwedge\nolimits^{i}\R^{n} \right) \oplus \left( \bigoplus_{i\text{ odd}}\bigwedge\nolimits^{i}\R^{n} \right).
  \end{equation*}
\end{example}

\begin{example}
  Let $V$ be a vector space over a field $k$ of characteristic not 2. The so-called \emph{parity transformation} on $V$ is the map
  \begin{equation*}
    P\colon V \to V;\qquad v \mapsto -v.
  \end{equation*}
  This preserves any quadratic form, and hence extends to an algebra homomorphism $\cliff(P) \equiv \alpha$ of any quadratic vector space $\cliff(V,q)$. Since 
  \begin{equation*}
    \alpha^2 = \cliff(P) \circ \cliff(P) = \cliff(P \circ P) = \cliff(\mathrm{id}_{(V,q)}) = \mathrm{id}_{\cliff(V,q)},
  \end{equation*}
  $\alpha$ is invertible, hence an isomorphism. Indeed, from the above equation we can say more. For the moment ignoring the multiplicative structure of $\cliff(V,q)$ and considering it only as a vector space, $\alpha$ can be thought of as a linear bijection whose square is the identity. This means $\alpha$ has two eigenvalues $+1$ and $-1$, and additively $\cliff(V,q)$ decomposes into a direct sum
  \begin{equation*}
    \cliff(V,q) = \cliff^{0}(V,q) \oplus \cliff^{1}(V,q),
  \end{equation*}
  where 
  \begin{equation*}
    \cliff^{0}(V,q) = \left\{ \varphi \in \cliff(V,q)\,\big|\, \alpha(\varphi) = \varphi \right\},\qquad\text{and} \qquad\cliff^{0}(V,q) = \left\{ \varphi \in \cliff(V,q)\,\big|\, \alpha(\varphi) = -\varphi \right\}.
  \end{equation*}

  For $\varphi_{1} \in \cliff^{i}(V,q)$ and $\varphi_{2} \in \cliff^{j}(V,q)$,
  \begin{equation*}
    \alpha(\varphi_{1}\varphi_{2}) = \alpha(\varphi_{1})\alpha(\varphi_{2});
  \end{equation*}
  this gives us the following multiplication table.
  \begin{equation*}
    \begin{tabular}[c]{c | c c}
      $\times$ & $\cliff^{0}(V,q)$ & $\cliff^{1}(V,q)$ \\
      \hline
      $\cliff^{0}(V,q)$ & $\cliff^{0}(V,q)$ & $\cliff^{1}(V,q)$\\
      $\cliff^{1}(V,q)$ & $\cliff^{1}(V,q)$ & $\cliff^{0}(V,q)$\\
    \end{tabular}
  \end{equation*}
  Thus, $\cliff(V,q)$ is a superalgebra.

\end{example}

\begin{definition}[supercommutator]
  \label{def:supercommutator}
  Let $A$ be a commutative ring. The \defn{supercommutator} is the map
  \begin{equation*}
    [\cdot ,\cdot ]\colon A \times A \to A;\qquad (a,b) \to [a,b] = ab - (-1)^{\tilde{a}\cdot \tilde{b}} ba.
  \end{equation*}
\end{definition}

\begin{definition}[supercommutation]
  \label{def:supercommutes}
  We will say that $a$ supercommutes with $b$ if $[a,b] = 0$; this means in particular that two even elements commute if $ab = ba$, and two odd elements supercommute if $ab = -ba$.
\end{definition}
\begin{note}
  Supercommutativity should not be confused with regular commutativity. Here is an example of a super ring which is commutative but not supercommutative.
\end{note}
\begin{example}
  Consider the ring $C^{0}(\R)$ of continuous functions $\R \to \R$. This is a super ring with the direct sum decomposition is given by
  \begin{equation*}
    C^{0}(\R) = \left\{ \text{even functions} \right\} \oplus \left\{ \text{odd functions} \right\};
  \end{equation*}
  this can be seen since any $f \in C^{0}(\R)$ can be written
  \begin{equation*}
    f(x) = \underbrace{\frac{f(x) + f(-x)}{2}}_{\text{even}} + \underbrace{\frac{f(x) - f(-x)}{2}}_{\text{odd}}.
  \end{equation*}
  The multiplication defined pointwise inherits its commutativity from the real numbers; but \emph{all} elements are commutative, even the odd elements. Hence $C^{0}(\R)$ is commutative but not supercommutative.
\end{example}

\begin{definition}[super ring homomorphism]
  \label{def:superringhomomorphism}
  Let $A$, $B$ be super rings. A \defn{super ring homomorphism} $f\colon A \to B$ is a ring homomorphism which preserve the grading.
\end{definition}

One might wonder if, in fact, \emph{all} ring homomorphisms between super rings are super ring homomorphisms. This is not the case.
\begin{counterexample}[Not all super ring homomorphisms preserve the grading]  
  Consider the set
  \begin{equation*}
    R = \left\{ f\colon \R \to \bigwedge \R^2 \right\}.
  \end{equation*}
  A general element $f \in R$ is of the form
  \begin{equation*}
    f(x) = f_{00}(x) + f_{10}(x)\bm{\hat{x}} + f_{01}(x)\bm{\hat{y}} + f_{11}(x)\bm{\hat{x}} \wedge \bm{\hat{y}},
  \end{equation*}
  where $\bm{\hat{x}}$ and $\bm{\hat{y}}$ are basis vectors for $\R^{2}$. Define multiplication pointwise:
  \begin{align*}
    (f\cdot g)(x) = f(x) g(x) =& f_{00}(x) g_{00}(x) \\
    &+(f_{00}(x) g_{10}(x) + f_{10}(x)g_{00}(x)) \bm{\hat{x}} \\
    &+(f_{00}(x)g_{01}(x) + f_{01}(x)g_{00}(x)) \bm{\hat{y}}\\
    &+(f_{11}(x) g_{00}(x) + f_{00}(x) g_{11}(x) + f_{10}(x) g_{01}(x) - f_{01}(x) g_{10}(x))\bm{\hat{x}} \wedge \bm{\hat{y}}.
  \end{align*}
  Then $R$ becomes a ring. In fact it is more; it is easy (if tedious) to check from the above multiplication law that $R$ can be seen as a super ring with grading given by
  \begin{equation*}
    f(x) = \underbrace{\frac{f(x) + f(-x)}{2}}_{R_{0}} + \underbrace{\frac{f(x) - f(-x)}{2}}_{R_{1}}.
  \end{equation*}

  Now consider the evaluation homomorphism 
  \begin{equation*}
    \iota\colon R \to \bigwedge \R^{2};\qquad f \mapsto f(0).
  \end{equation*}
  This is a ring homomorphism which does not preserve grading. To see this, consider the constant function
  \begin{equation*}
    \varphi(x) = \bm{\hat{x}}\qquad\text{for all }x.
  \end{equation*}
  Non-zero constant functions such as $\varphi$ are even, but $\bm{\hat{x}}$ is odd in $\bigwedge \R^{2}$.
\end{counterexample}
\begin{definition}[supercenter]
  \label{def:supercenter}
  The \defn{supercenter} of $A$ is the set
  \begin{equation*}
    Z(A) = \left\{ a \in A\,\big|\, \text{for all } b \in A,\, [a,b]=0 \right\}.
  \end{equation*}
\end{definition}

In the theory of commutative algebra, we have \hyperref[def:algebraoveraring]{Definition \ref*{def:algebraoveraring}} of an algebra over a ring. This definition generalizes naturally to the case where $A$ and $R$ are super rings:
\begin{definition}[super algebra over a ring]
  \label{def:superalgebraoveraring}
  Let $R$ be a supercommutative super ring. An \defn{$R$-super-algebra} is a super ring $A$ which is also an $R$-module (with left multiplication $*\colon R \times A \to A$) such that the multiplication map $\cdot \colon A \times A \to A$ is $R$-bilinear, i.e.
  \begin{equation*}
    r*(a\cdot b) = (r*a)\cdot b = (-1)^{\tilde{a}\cdot \tilde{r}} a\cdot (r*b).
  \end{equation*}
\end{definition}
\hyperref[thm:ringhomomorphisminducesalgebra]{Theorem \ref*{thm:ringhomomorphisminducesalgebra}} also generalizes nicely.
\begin{theorem}
  Let $A$, $R$ be unital super rings, $R$ supercommutative, and let $f\colon R \to A$ be a unital super ring homomorphism. Then $A$ naturally has the structure of an $R$-module. Furthermore, if the image $f(R)$ is in the supercenter of $A$, then $A$ naturally has the structure of an $R$-super algebra.
\end{theorem}
\begin{proof}
  The module axioms (\hyperref[def:module]{Definition \ref*{def:module}}) don't involve commutivity, so their verification is trivial as before. We must only check that if $f(R)$ is in the supercenter of $A$ then $*$ is $R$-bilinear, i.e. that
  \begin{equation*}
    r*(a\cdot b) = f(r)\cdot a\cdot b = (f(r)\cdot a)\cdot b = (r*a)\cdot b,
  \end{equation*}
  and
  \begin{equation*}
    r*(a\cdot b) = f(r)\cdot a\cdot b = (-1)^{ \tilde{a}\cdot \widetilde{f(r)}}a\cdot f(r)\cdot b = (-1)^{\tilde{a}\cdot \tilde{r}}a\cdot f(r)\cdot b = (-1)^{\tilde{a}\cdot \tilde{r}}(r*b).
  \end{equation*}
\end{proof}

\begin{theorem}
  Let $R$ be a supercommutative super ring, $r \in R_{1}$. Then $r$ is nilpotent (\hyperref[def:nilpotent]{Definition \ref*{def:nilpotent}}).
\end{theorem}
\begin{proof}
  $\displaystyle r^2 = \frac{1}{2}[r,r] = 0$.
\end{proof}

We have the following analog to \hyperref[thm:propertiesofcommutator]{Theorem \ref*{thm:propertiesofcommutator}}.
\begin{lemma}
  Let $B$ be an associative superring. The supercommutator satisfies the following identities.
  \begin{itemize}
    \item $[a,b] = -(-1)^{\tilde{a}\cdot \tilde{b}}[b,a]$.
    \item $[a,[b,c]] + (-1)^{\tilde{a}(\tilde{b} + \tilde{c})}[b,[c,a]] + (-1)^{\tilde{c}(\tilde{a}+\tilde{b})}[c,[a,b]] = 0$
  \end{itemize}
  Furthermore, if $B$ is an $A$-algebra, then the following holds for all $a \in A$, $b,c \in B$.
  \begin{itemize}
    \item $a[b,c] \overset{1}{=} [ab,c] \overset{2}{=} (-1)^{\tilde{a}\cdot \tilde{b}}[ba,c] \overset{3}{=} (-1)^{\tilde{a}(\tilde{b} + \tilde{c})}[b,ca] \overset{4}{=} (-1)^{\tilde{a}(\tilde{b} + \tilde{c})}[b,c]a$
  \end{itemize}
\end{lemma}
\begin{proof}
  $\,$
  \begin{itemize}
    \item $[a,b] = ab-(-1)^{\tilde{a}\cdot \tilde{b}} ba = ba(-1)^{\tilde{a}\cdot \tilde{b}} - \left( (-1)^{\tilde{a}\cdot \tilde{b}} \right)^{2} ab = (-1)^{\tilde{a}\cdot \tilde{b}}\left( ba - (-1)^{\tilde{a}\cdot \tilde{b}}ab \right) = (-1)^{\tilde{a}\cdot \tilde{b}}[b,a] $
    \item 
      \begin{align*}
        [a,[b,c]] &= a(bc-(-1)^{\tilde{b}\cdot \tilde{c}}cb) - (-1)^{\tilde{a}(\tilde{b} + \tilde{c})}(bc-(-1)^{\tilde{b}\cdot \tilde{c}}cb)a \\
        [b,[c,a]] &= b(ca-(-1)^{\tilde{c}\cdot \tilde{a}}ac) - (-1)^{\tilde{b}(\tilde{c} + \tilde{a})}(ca-(-1)^{\tilde{c}\cdot \tilde{a}}ac)b \\
        [c,[a,b]] &= c(ab-(-1)^{\tilde{a}\cdot \tilde{b}}ba) - (-1)^{\tilde{c}(\tilde{a} + \tilde{b})}(ab-(-1)^{\tilde{a}\cdot \tilde{b}}ba)c 
      \end{align*}

    \item My apologies for the formatting.
      \begin{enumerate}[label= Equality \arabic*:]
        \item 
          \begin{align*}
            a[b,c] &= a(bc-(-1)^{\tilde{b}\cdot \tilde{c}}cb) \\
            &= abc-(-1)^{\tilde{b}\cdot \tilde{c}}acb \\
            &= abc-(-1)^{\tilde{b}\cdot \tilde{c}} (-1)^{\tilde{a}\cdot \tilde{c}}cab\\
            &=(ab)c - (-1)^{\tilde{ab}\cdot \tilde{c}} c(ab) \\
            &= [ab,c]
          \end{align*}

        \item 
          \begin{align*}
            [ab,c] &= (ab)c - (-1)^{\widetilde{ab}\cdot \tilde{c}} c(ab) \\
            &= (-1)^{\tilde{a}\cdot \tilde{b}}\left((ba)c  -  (-1)^{\widetilde{ab}\cdot \tilde{c}}c(ba) \right)\\
            &= (-1)^{\tilde{a}\cdot \tilde{b}}[ba,c] \\
          \end{align*}
        \item
          \begin{align*}
            (-1)^{\tilde{a}\cdot \tilde{b}}[ba,c] &= (-1)^{\tilde{a}\cdot \tilde{b}}\left((ba)c  -  (-1)^{\tilde{ab}\cdot \tilde{c}}c(ba) \right) \\
            &= (-1)^{\tilde{a}\cdot \tilde{b}} \left( b(ac) - (-1)^{\tilde{c}(\tilde{a} + \tilde{b})} (-1)^{\tilde{a}\cdot \tilde{b}} (-1)^{\tilde{a}\cdot \tilde{c}} (ac)b\right) \\
            &= (-1)^{\tilde{a}\cdot \tilde{b}}\left( b(ac) - (-1)^{\tilde{b}(\tilde{a}+\tilde{c})} (ac)b \right) \\
            &= (-1)^{\tilde{a}\cdot \tilde{b}} [b,ac]
          \end{align*}

        \item Similar.
      \end{enumerate}
  \end{itemize}
\end{proof}

\section{Supermodules}
The definition of a supermodule is very similar to that of a regular module (\hyperref[def:module]{Definition \ref*{def:module}}).
\begin{definition}[supermodule]
  \label{def:supermodule}
  Let $A$ be an abelian group with a $\Z/2\Z$ grading, i.e.
  \begin{equation*}
    A = A_{0}\oplus A_{1}.
  \end{equation*}

  Further, let $R$ be a superring with identity. A \defn{(left) supermodule} is a triple $(A,R,* )$, with $A$ and $R$ as above and $*$ a function $R \times A \to A$ such that
  \begin{enumerate}
    \item if $r \in R_{i}$ and $a \in A_{j}$ then $r*a \in A_{i+j \mod{2}}$.
    \item $*$ satisfies all the module axioms listed in \hyperref[def:module]{Definition \ref*{def:module}}.
  \end{enumerate}
\end{definition}

If $R$ is supercommutative and $A$ is a left supermodule, we can make $A$ into a right supermodule or a super bimodule as in \hyperref[thm:leftmoduletransformstoothermodules]{Theorem \ref*{thm:leftmoduletransformstoothermodules}}.

\begin{definition}[super bimodule]
  \label{def:superbimodule}
  An abelian group $A$ which is both a left and a right $R$ module is a \defn{super bimodule} if left and right multiplication obey the compatibility condition
  \begin{equation*}
    ra = (-1)^{\tilde{r}\cdot \tilde{a}} ar\qquad \text{for all }a\in A,\quad r \in R.
  \end{equation*}
\end{definition}

\begin{lemma}
  Let $R$ be a supercommutative superring, $A$ be a left $R$-module. Then $A$ can naturally be given the structure of a right $R$-module or a bimodule.
\end{lemma}
\begin{proof}
  Define a right action $ra \equiv (-1)^{\tilde{r}\cdot \tilde{a}} ar$. This makes $A$ into a right $R$-module, and 
\end{proof}

\begin{definition}[superlinear map]
  \label{def:superlinear}
  Let $R$ be a super ring, $A$ and $B$ be $R$-supermodules. A map $f\colon A \to B$ is called 
\end{definition}

\section{Super vector spaces}
\begin{definition}[$\Z_{2}$-graded vector space]
  \label{def:z2gradedvectorspace}
  A \defn{$\Z_{2}$-graded vector space} over a field $k$ (for simplicity of characteristic zero) is a $\Z_{2}$-graded vector space
  \begin{equation*}
    V = V_{0} \oplus V_{1}.
  \end{equation*}
\end{definition}
As before elements of $V_{0}$ are \emph{even}, elements of $V_{1}$ are \emph{odd}, and elements of $V_{1} \cup V_{2} \setminus \{0\}$ are \emph{homogeneous}.

\begin{definition}[dimension of a $\Z_{2}$-graded vector space]
  \label{def:dimensionofaz2gradedvectorspace}
  Let 
  \begin{equation*}
    V = V_{0} \oplus V_{1}.
  \end{equation*}
  be a $\Z_{2}$-graded vector space. The \defn{dimension} of $V$ is
  \begin{equation*}
    (\dim V_{0}| \dim V_{1}) \in (\N \cup \{\infty\})^{2}.
  \end{equation*}
\end{definition} 

\begin{definition}[finite-dimensional $\Z_{2}$-graded vector space]
  \label{def:finitedimensionalz2gradedvectorspace}
  We say that a $\Z_{2}$-graded vector space $V$ is \defn{finite dimensional} if $V_{1}$ and $V_{2}$ are.
\end{definition}

\begin{definition}[$\Z_{2}$-graded vector space morphism]
  \label{def:z2gradedvectorspacemorphism}
  Let $V$ and $W$ be $\Z_{2}$-graded vector spaces. A \defn{morphism} from $V$ to $W$ (also called a \defn{$\Z_{2}$-graded linear map}) is a linear map $V \to W$ which preserves the grading, i.e. which maps $V_{0} \to W_{0}$ and $V_{1} \to W_{1}$.
\end{definition}

\begin{definition}[direct sum of $\Z_{2}$-graded vector spaces]
  \label{def:directsumofz2gradedvectorspaces}
  Let $V$ and $W$ be $\Z_{2}$-graded vector spaces. Their \defn{direct sum}, denoted $V \oplus W$, is the $\Z_{2}$-graded vector space with grading
  \begin{equation*}
    (V \oplus W)_{i} = V_{i} \oplus W_{i},\qquad i = 1, 2.
  \end{equation*}
\end{definition} 
\begin{definition}[tensor product of $\Z_{2}$-graded vector spaces]
  \label{def:tensorproduttofz2gradedvectorspaces}
  Let $V$ and $W$ be $\Z_{2}$-graded vector spaces. Their \defn{tensor product} denoted $V \otimes W$, is the tensor product of the underlying ungraded vector spaces, with the grading
  \begin{equation*}
    (V \otimes W)_{0} = (V_{0} \otimes W_{0})\oplus(V_{1} \otimes V_{1}),\qquad (V \otimes W)_{1} = (V_{0} \otimes W_{1})\oplus(V_{1} \otimes V_{0}).
  \end{equation*}
\end{definition}

\section{Superalgebras}

\begin{definition}[superalgebra]
  \label{def:superalgebra}
  A superalgebra $A$ is a $\Z/2$-graded vector space $A = A_{0} \oplus A_{1}$ together with a bilinear map
  \begin{equation*}
    \cdot\colon A \times A \to A,
  \end{equation*}
  which respects the grading in the sense that $A_{i} \cdot A_{j} \subseteq A_{i+j\ \mathrm{mod}(2)}$.
\end{definition}

\begin{definition}[supercommutative superalgebra]
  \label{def:supercommutativesuperalgebra}
  A \defn{supercommutative superalgebra} is a superalgebra which obeys the Koszul sign rule, i.e.  $\widetilde{a \cdot b} = \tilde{a} \cdot \tilde{b}$ for elements $a$, $b$ of pure degree.
\end{definition}

\begin{definition}[superalgebra homomorphism]
  \label{def:superalgebrahomomorphism}
  Let $A$ and $B$ be superalgebras. A \defn{superalgebra homomorphism} $f\colon A \to B$ is a homomorphism of the underlying algebras which respects the grading. That is, 
  \begin{equation*}
    f = f_{0} \oplus f_{1},
  \end{equation*}
  where $f_{0}$ and $f_{1}$ are algebra homomorphisms.
\end{definition}

\begin{definition}[parity involution]
  \label{def:parityinvolution}
  Let $A = A_{0} \oplus A_{1}$ be a supercommutative superalgebra. Then there is an algebra automorphism $P\colon A\to A$ called the \defn{parity involution} which acts on elements of pure degree via
  \begin{equation*}
    P(a) = (-1)^{\tilde{a}}a.
  \end{equation*}
\end{definition}

\begin{thebibliography}{9}
  \bibitem{michelson-lawson} M.-L. Michelson and H.B. Lawson.
    \textit{Spin Geometry}.
    Princeton University Press, Princeton, NJ, 1989.

  \bibitem{vakil-rising-sea} R. Vakil,
    \textit{The Rising Sea}.
    \url{http://www.math216.wordpress.com/}

  \bibitem{sontz-principal-bundles-classical} S. B. Sontz.
    \textit{Principal Bundles: The Classical Case}.
    Springer International Publishing, Switzerland, 2015.

  \bibitem{hungerford-algebra} T. Hungerford.
    \textit{Algebra}.
    Springer-Verlag New York, NY, 1974.

  \bibitem{o'farril-spin-geometry} J. Figuroa-O'Farril. 
    \textit{PG Course on Spin Geometry}.
    \url{https://empg.maths.ed.ac.uk/Activities/Spin/}

  \bibitem{quantum-fields-and-strings} P. Deligne et al.
    \textit{Quantum Fields and Strings: a Course for Mathematicians}.
    American Mathematical Society, 1999.

  \bibitem{susy-for-mathematicians} V. S. Varadarajan.
    \textit{Supersymmetry for Mathematicans: An Introduction}.
    American Mathematical Society, 2004.

  \bibitem{catsters} The Catsters.
    \url{https://www.youtube.com/channel/UC5Y9H2KDRHZZTWZJtlH4VbA}

  \bibitem{connes-marcolli-ncg} A. Connes and M. Marcolli.
    \textit{Noncommutative Geometry, Quantum Fields and Motives}.
    \url{http://www.alainconnes.org/docs/bookwebfinal.pdf}

  \bibitem{aluffi-algebra-chapter-0} P. Aluffi.
    \textit{Algebra: Chapter 0}.
    American Mathematical Society, 2009.

  \bibitem{nlab-deligne-theorem} 
    \textit{Nlab---Deligne's Theorem on Tensor Categories}.
    \url{https://ncatlab.org/nlab/show/Deligne's+theorem+on+tensor+categories}

  \bibitem{baez-this-weeks-finds-137} J. Baez.
    \textit{This Week's Finds in Mathematical Physics (Week 137)}.
    \url{http://math.ucr.edu/home/baez/week137.html}

  \bibitem{baez-definitions-everyone-should-know} J. Baez.
    \textit{Some Definitions Everyone Should Know}.
    \url{http://math.ucr.edu/home/baez/qg-fall2004/definitions.pdf}

  \bibitem{unapolagetic-mathematician-mac-lanes-theorem} 
    \textit{The Unapologetic Mathematician: Mac Lane's Coherence Theorem}.
    \url{https://unapologetic.wordpress.com/2007/06/29/mac-lanes-coherence-theorem/}

  \bibitem{gleason-montgomery-zippin} D. Montgomery and L. Zippin. 
    \textit{Topological Transformation Groups}.
    University of Chicago Press, Chicago, 1955.

  \bibitem{DMOS} P. Deligne, J.S. Milne, A. Ogus, and K. Shih.
    \textit{Hodge Cycles, Motives, and Shimura Varieties}.
    Springer Verlag, 1982

  \bibitem{maclane-categories} S. Mac Lane.
    \textit{Categories for the Working Mathematician}.
    Springer-Verlag New York, New York, 1998.

  \bibitem{baez-higher-categories} J. Baez. 
    \textit{An introduction to $n$-Categories}.
    \url{https://arxiv.org/pdf/q-alg/9705009.pdf}

  \bibitem{baez-categories-usenet} 
    \url{https://groups.google.com/forum/#!topic/sci.math/7LqPFfmWGOA}

  \bibitem{nlab} 
    \url{https://www.ncatlab.org/}

  \bibitem{wikipedia-product} 
    \textit{Wikipedia - Product (Category Theory)}.
    \url{https://en.wikipedia.org/wiki/Product\_(category\_theory)}

  \bibitem{EGNO-tensor-categories} P. Etingof, S. Gelaki, D. Nikshych, and V. Ostrik.
    \textit{Tensor Categories}.
    American Mathematical Society, 2015.

  \bibitem{awodey-intro-to-categories} S. Awodey and A. Bauer.
    \textit{Lecture Notes: Introduction to Categorical Logic}

  \bibitem{awodey-category-theory-foundations-videos} S. Awodey.
    \textit{Category Theory Foundations}.
    \url{https://www.youtube.com/watch?v=BF6kHD1DAeU&index=1&list=PLGCr8P\_YncjVjwAxrifKgcQYtbZ3zuPlb}

  \bibitem{awodey-category-theory} S. Awodey.
    \textit{Category Theory}.
    Oxford University Press, 2006.

  \bibitem{haag-local-quantum-physics} R. Haag.
    \textit{Local Quantum Physics: Fields, Particles, Algebras}.
    Springer-Verlag Berlin Heidelberg New York, 1996

  \bibitem{sexl-urbantke-relativity-groups-particles} R. Sexl and H. Urbantke.
    \textit{Relativity, Groups, Particles: Special Relativity and Relativistic Symmetry in Field and Particle Physics}.
    Springer-Verlag Wein, 1992

  \bibitem{wess-bagger-susy-sugra} J. Wess and J. Bagger.
    \textit{Supersymmetry and Supergravity}.
    Princeton University Press, Princeton NJ, 1992

  \bibitem{muller-kirsen-wiedmann-intro-susy} H J W M{\"u}ller-Kirsen and Armin Wiedemann
    \textit{Introduction to Supersymmetry (Second edition)}.
    World Scientific, 2010.

  \bibitem{deligne-categories-tensorielle} Pierre Deligne.
    \textit{Cat{\'e}gories Tensorielle},
    Moscow Math. Journal 2 (2002) no. 2, 227-228
    \url{https://www.math.ias.edu/files/deligne/Tensorielles.pdf}

  \bibitem{KMS-natural-operations-differential-geometry} I. Kol\'{a}\v{r}, P. Michor, and J. Slov\'{a}k.
    \textit{Natural Operations in Differential Geometry}.
    Springer-Verlag, Berlin Heidelberg, 1993.

  \bibitem{hartshorne-algebraic-geometry} R. Hartshorne.
    \textit{Algebraic Geometry}.
    Springer-Verlag, New York, 1977.

  \bibitem{annoying-precision-meditation} Q. Yuan.
    \textit{Annoying Precision: A neditation on semiadditive categories}.
    \url{https://qchu.wordpress.com/2012/09/14/a-meditation-on-semiadditive-categories/}

  \bibitem{nestruev-smooth-manifolds-observables} J. Nestruev.
    \textit{Smooth Monifolds and Observables}.
    Springer-Verlag, New York, 2002.

  \bibitem{baez-lauda-prehistory} J. Baez and A. Lauda. 
    \textit{A prehistory of $n$-categorical physics}.
    \url{https://arxiv.org/pdf/0908.2469.pdf}

  \bibitem{neumaier} A. Neumaier.
    \textit{Elementary particles as irreducible representations}.
    \url{https://www.physicsoverflow.org/21960}.

  \bibitem{physicsforums-why-deligne} U. Schreiber.
    \textit{Why Supersymmetry? Because of Deligne's theorem}
    \url{https://www.physicsforums.com/insights/supersymmetry-delignes-theorem/}

  \bibitem{mackey-induced-representations} G. W. Mackey.
    \textit{Induced representations}.
    W.A. Benjamin, Inc., and Editore Boringhieri, New York, 1968.

  \bibitem{nlab-additive-category} nLab: Additive category.
    \url{https://ncatlab.org/nlab/show/additive+category#ProductsAreBiproducts}

  \bibitem{freyd-abelian-categories} P. Freyd.
    \textit{Abelian Categories}
    Harper and Row, New York, 1964.

  \bibitem{milne-affine-group-schemes} J. Milne,
    \textit{Basic Theory of Affine Group Schemes},
    2012. 
    Available at \url{www.jmilne.org/math/}

  \bibitem{wigner-little-group} E. Wigner.
    \textit{On Unitary Representations of the Inhomogeneous Lorentz Group.}
    Annals of Mathematics. Second Series, Vol. 40, No. 1 (Jan., 1939), pp. 149-204

  \bibitem{jpmds-representation-theory-symmetric-groups} J. P. M. dos Santos.
    \textit{Representation theory of symmetric groups.}
    \url{https://www.math.tecnico.ulisboa.pt/~ggranja/joaopedro.pdf}

  \bibitem{manin-gauge-fields} U. Manin.
    \textit{Gauge Fields and Complex Geometry},
    Springer-Verlag Berlin Heidelberg New York, 1988.

  \bibitem{dissipation-in-lagrangian-mechanics} Valter Moretti (\url{https://physics.stackexchange.com/users/35354/valter-moretti}).
    \textit{Euler-Lagrange equations and friction forces}, 
    URL (version: 2014-02-02): \url{https://physics.stackexchange.com/q/96470}

  \bibitem{milneliealgebrasalgebraicgroupsliegroups} J.S. Milne.
    \textit{Lie Algebras, Algebraic Groups, and Lie Groups.}
    Available at \url{www.jmilne.org/math/}

  \bibitem{braidstatisticsinlocalqft} J. Fr\"{o}hlich and F. Gabbiani. 
    \textit{Braid statistics in Local Quantum Theory}.
    Rev. Math. Phys. 02, 251 (1990). 

  \bibitem{binneyphysicsofqm} J. Binney and D. Skinner.
    \textit{The Physics of Quantum Mechanics.}
    Available at \url{https://www-thphys.physics.ox.ac.uk/people/JamesBinney/qb.pdf}

  \bibitem{feynmanpathintegral} R. Feynman.
    \textit{Spacetime Approach to Non-Relativistic Quantum Mechanics.}
    Rev. Mod. Phys. 20, 367 – Published 1 April 1948

  \bibitem{berezinsecondquantization} F. A. Berezin. 
    \textit{The Method of Second Quantization.}
    Nauka, Moscow, 1965. Tranlation: Academic Press, New York, 1966. (Second edition, expanded: M. K. Polivanov, ed., Nauka, Moscow, 1986.)

  \bibitem{coleman-mandula} S. Coleman and J. Mandula. 
    \textit{All Possible Symmetries of the S Matrix}. 
    Physical Review, 159(5), 1967, pp. 1251–1256.

  \bibitem{haag-lopuszanski-sohnius} R. Haag, M. Sohnius, and J. T. {\L}opusza\'{n}ski.
    \textit{All possible generators of supersymmetries of the S-matrix}. 
    Nuclear Physics B, 88: 257–274 (1975)
\end{thebibliography}
\end{document}
